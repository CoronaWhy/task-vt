{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReadMe\n",
    "\n",
    "This notebook contains a pipeline for text data normalization and feature engineering for Stance Detection between two bodies of text, such as a title and abstract.  The pipeline was originally composed and executed for the [Fake News Challenge](http://www.fakenewschallenge.org/) whos dataset is roughly 70,000 observations.  This pipeline achieved roughly 98% and 96% accuracy for training and testing sets, respectively on the FNC data. The cells below normalize the *vt_contra_v9_covid19_metadata_200425.csv* data, completes feature engineering and then uses the saved FNC model weights to predict stances for the contradiction data {'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3}.\n",
    "\n",
    "Feature engineering was done using google compute engine as colab does not offer enough ram.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZU6WjSZeIkT"
   },
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n",
    "#pip install dtale\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OutuE2qcHvF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.float_format = '{:, .2f}'.format\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "from numpy import savez_compressed\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#from scipy.misc import comb, logsumexp\n",
    "from sklearn.manifold import TSNE #a tool to visualize high dimensional data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # dimensionality reduction using truncated SVD (AKA LSA)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.collocations import *\n",
    "import string #python module\n",
    "import re # python regex module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2444,
     "status": "ok",
     "timestamp": 1589245325266,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "XakrAQa2cWmC",
    "outputId": "745604f5-ca71-4be3-a265-8d5b886fe246"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "      <th>title_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12519</td>\n",
       "      <td>uk8rfroj</td>\n",
       "      <td>b5d303cbcfe6be92d733ec593118b388db77452e</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Complete Genome Sequence of a 2019 Novel Coronavirus (SARS-CoV-2) Strain Isolated in Nepal</td>\n",
       "      <td>10.1128/mra.00169-20</td>\n",
       "      <td>PMC7067954</td>\n",
       "      <td>32165386.0</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>A complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) strain isolated from an oropharyngeal swab specimen of a Nepalese patient with coronavirus disease 2019 (COVID-19), who had returned to Nepal after traveling to Wuhan, China.</td>\n",
       "      <td>3/12/2020</td>\n",
       "      <td>Sah, Ranjit; Rodriguez-Morales, Alfonso J.; Jha, Runa; Chu, Daniel K. W.; Gu, Haogao; Peiris, Malik; Bastola, Anup; Lal, Bibek Kumar; Ojha, Hemant Chanda; Rabaan, Ali A.; Zambrano, Lysien I.; Costello, Anthony; Morita, Kouichi; Pandey, Basu Dev; Poon, Leo L. M.</td>\n",
       "      <td>Microbiol Resour Announc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>comm_use_subset</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7067954/</td>\n",
       "      <td>complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  cord_uid                                       sha source_x  \\\n",
       "0       12519  uk8rfroj  b5d303cbcfe6be92d733ec593118b388db77452e      PMC   \n",
       "\n",
       "                                                                                        title  \\\n",
       "0  Complete Genome Sequence of a 2019 Novel Coronavirus (SARS-CoV-2) Strain Isolated in Nepal   \n",
       "\n",
       "                    doi       pmcid   pubmed_id license  \\\n",
       "0  10.1128/mra.00169-20  PMC7067954  32165386.0   cc-by   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    abstract  \\\n",
       "0  A complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) strain isolated from an oropharyngeal swab specimen of a Nepalese patient with coronavirus disease 2019 (COVID-19), who had returned to Nepal after traveling to Wuhan, China.   \n",
       "\n",
       "  publish_time  \\\n",
       "0    3/12/2020   \n",
       "\n",
       "                                                                                                                                                                                                                                                                 authors  \\\n",
       "0  Sah, Ranjit; Rodriguez-Morales, Alfonso J.; Jha, Runa; Chu, Daniel K. W.; Gu, Haogao; Peiris, Malik; Bastola, Anup; Lal, Bibek Kumar; Ojha, Hemant Chanda; Rabaan, Ali A.; Zambrano, Lysien I.; Costello, Anthony; Morita, Kouichi; Pandey, Basu Dev; Poon, Leo L. M.   \n",
       "\n",
       "                    journal  Microsoft Academic Paper ID WHO #Covidence  \\\n",
       "0  Microbiol Resour Announc                          NaN            NaN   \n",
       "\n",
       "   has_pdf_parse  has_pmc_xml_parse   full_text_file  \\\n",
       "0           True               True  comm_use_subset   \n",
       "\n",
       "                                                     url  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7067954/   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                         title_abstract  \n",
       "0  complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df_0 = pd.read_csv(\"vt_contra_v9_covid19_metadata_200425.csv\")\n",
    "df_0.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "488UtCCicb7U"
   },
   "outputs": [],
   "source": [
    "# isolate germane featrues\n",
    "df1 = copy.deepcopy(df_0[['cord_uid','title', 'abstract']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2424,
     "status": "ok",
     "timestamp": 1589245325267,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "VdgD806Ncyuc",
    "outputId": "03b64373-66a3-404f-fbe2-90825a07a1d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cord_uid's: 3956\n",
      "Number of unique title's: 3911\n",
      "Number of unique abstract's: 3941\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique cord_uid's: %s\" % df1.cord_uid.nunique())\n",
    "print(\"Number of unique title's: %s\" % df1.title.nunique())\n",
    "print(\"Number of unique abstract's: %s\" % df1.abstract.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2izmloBYc4_E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYfxb1foc9ls"
   },
   "source": [
    "# Preprocessing -- Normalize Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2739,
     "status": "ok",
     "timestamp": 1589245325596,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "u49fyZxIdB4T",
    "outputId": "c148c47c-9f8a-40fc-9a6e-5df8a2352b04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>abstract_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk8rfroj</td>\n",
       "      <td>complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal</td>\n",
       "      <td>a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.</td>\n",
       "      <td>[complete, genome, sequence, of, 2019, novel, coronavirus, sars, cov, strain, isolated, in, nepal]</td>\n",
       "      <td>[complete, genome, sequence, was, obtained, for, severe, acute, respiratory, syndrome, coronavirus, sars, cov, strain, isolated, from, an, oropharyngeal, swab, specimen, of, nepalese, patient, with, coronavirus, disease, 2019, covid, 19, who, had, returned, to, nepal, after, traveling, to, wuhan, china]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ivwn4nhl</td>\n",
       "      <td>first cases of coronavirus disease 2019 (covid-19) in the who european region, 24 january to 21 february 2020</td>\n",
       "      <td>in the who european region, covid-19 surveillance was implemented 27 january 2020. we detail the first european cases. as at 21 february, nine european countries reported 47 cases. among 38 cases studied, 21 were linked to two clusters in germany and france, 14 were infected in china. median case age was 42 years; 25 were male. late detection of the clusters’ index cases delayed isolation of further local cases. as at 5 march, there were 4,250 cases.</td>\n",
       "      <td>[first, cases, of, coronavirus, disease, 2019, covid, 19, in, the, who, european, region, 24, january, to, 21, february, 2020]</td>\n",
       "      <td>[in, the, who, european, region, covid, 19, surveillance, was, implemented, 27, january, 2020, we, detail, the, first, european, cases, as, at, 21, february, nine, european, countries, reported, 47, cases, among, 38, cases, studied, 21, were, linked, to, two, clusters, in, germany, and, france, 14, were, infected, in, china, median, case, age, was, 42, years, 25, were, male, late, detection, of, the, clusters, index, cases, delayed, isolation, of, further, local, cases, as, at, march, there,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4yuw7jo3</td>\n",
       "      <td>network-based drug repurposing for novel coronavirus 2019-ncov/sars-cov-2</td>\n",
       "      <td>human coronaviruses (hcovs), including severe acute respiratory syndrome coronavirus (sars-cov) and 2019 novel coronavirus (2019-ncov, also known as sars-cov-2), lead global epidemics with high morbidity and mortality. however, there are currently no effective drugs targeting 2019-ncov/sars-cov-2. drug repurposing, representing as an effective drug discovery strategy from existing drugs, could shorten the time and reduce the cost compared to de novo drug discovery. in this study, we present ...</td>\n",
       "      <td>[network, based, drug, repurposing, for, novel, coronavirus, 2019, ncov, sars, cov]</td>\n",
       "      <td>[human, coronaviruses, hcovs, including, severe, acute, respiratory, syndrome, coronavirus, sars, cov, and, 2019, novel, coronavirus, 2019, ncov, also, known, as, sars, cov, lead, global, epidemics, with, high, morbidity, and, mortality, however, there, are, currently, no, effective, drugs, targeting, 2019, ncov, sars, cov, drug, repurposing, representing, as, an, effective, drug, discovery, strategy, from, existing, drugs, could, shorten, the, time, and, reduce, the, cost, compared, to, de,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid  \\\n",
       "0  uk8rfroj   \n",
       "1  ivwn4nhl   \n",
       "2  4yuw7jo3   \n",
       "\n",
       "                                                                                                           title  \\\n",
       "0                     complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal   \n",
       "1  first cases of coronavirus disease 2019 (covid-19) in the who european region, 24 january to 21 february 2020   \n",
       "2                                      network-based drug repurposing for novel coronavirus 2019-ncov/sars-cov-2   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstract  \\\n",
       "0                                                                                                                                                                                                                            a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.   \n",
       "1                                               in the who european region, covid-19 surveillance was implemented 27 january 2020. we detail the first european cases. as at 21 february, nine european countries reported 47 cases. among 38 cases studied, 21 were linked to two clusters in germany and france, 14 were infected in china. median case age was 42 years; 25 were male. late detection of the clusters’ index cases delayed isolation of further local cases. as at 5 march, there were 4,250 cases.   \n",
       "2  human coronaviruses (hcovs), including severe acute respiratory syndrome coronavirus (sars-cov) and 2019 novel coronavirus (2019-ncov, also known as sars-cov-2), lead global epidemics with high morbidity and mortality. however, there are currently no effective drugs targeting 2019-ncov/sars-cov-2. drug repurposing, representing as an effective drug discovery strategy from existing drugs, could shorten the time and reduce the cost compared to de novo drug discovery. in this study, we present ...   \n",
       "\n",
       "                                                                                                                     title_tokens  \\\n",
       "0                              [complete, genome, sequence, of, 2019, novel, coronavirus, sars, cov, strain, isolated, in, nepal]   \n",
       "1  [first, cases, of, coronavirus, disease, 2019, covid, 19, in, the, who, european, region, 24, january, to, 21, february, 2020]   \n",
       "2                                             [network, based, drug, repurposing, for, novel, coronavirus, 2019, ncov, sars, cov]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract_tokens  \n",
       "0                                                                                                                                                                                                     [complete, genome, sequence, was, obtained, for, severe, acute, respiratory, syndrome, coronavirus, sars, cov, strain, isolated, from, an, oropharyngeal, swab, specimen, of, nepalese, patient, with, coronavirus, disease, 2019, covid, 19, who, had, returned, to, nepal, after, traveling, to, wuhan, china]  \n",
       "1  [in, the, who, european, region, covid, 19, surveillance, was, implemented, 27, january, 2020, we, detail, the, first, european, cases, as, at, 21, february, nine, european, countries, reported, 47, cases, among, 38, cases, studied, 21, were, linked, to, two, clusters, in, germany, and, france, 14, were, infected, in, china, median, case, age, was, 42, years, 25, were, male, late, detection, of, the, clusters, index, cases, delayed, isolation, of, further, local, cases, as, at, march, there,...  \n",
       "2  [human, coronaviruses, hcovs, including, severe, acute, respiratory, syndrome, coronavirus, sars, cov, and, 2019, novel, coronavirus, 2019, ncov, also, known, as, sars, cov, lead, global, epidemics, with, high, morbidity, and, mortality, however, there, are, currently, no, effective, drugs, targeting, 2019, ncov, sars, cov, drug, repurposing, representing, as, an, effective, drug, discovery, strategy, from, existing, drugs, could, shorten, the, time, and, reduce, the, cost, compared, to, de,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase all text\n",
    "df_2 = copy.deepcopy(df1)\n",
    "df_2['title'] = df_2['title'].str.lower()\n",
    "df_2['abstract'] = df_2['abstract'].str.lower()\n",
    "#df_2['title_abstract'] = df_2['title_abstract'].str.lower()\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer (r\"(?u)\\b\\w\\w+\\b\")\n",
    "df_2['title_tokens'] = df_2['title'].map(tokenizer.tokenize)\n",
    "df_2['abstract_tokens'] = df_2['abstract'].map(tokenizer.tokenize)\n",
    "#df_2['title_abstract_tokens'] = df_2['abstract'].map(tokenizer.tokenize)\n",
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1589245325861,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "Tft5HKredDCU",
    "outputId": "9595bc8d-ff98-48c1-9d92-483cf4d2174e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # a sentance tokenizer\n",
    "nltk.download('gutenberg') # a text corpora and lexical resources\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruaNwi4xdQHi"
   },
   "outputs": [],
   "source": [
    "# instantiate list of stop words and other characters/punctuation to remove\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``',\"_\"]\n",
    "\n",
    "# remove stop words / keep everything except stopwords_list\n",
    "df_2['title_tokens'] = df_2['title_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "df_2['abstract_tokens'] = df_2['abstract_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "#df_2['title_abstract_tokens'] = df_2['title_abstract_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12173,
     "status": "ok",
     "timestamp": 1589245335056,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "HcIOizDrdSgi",
    "outputId": "685a7e83-e846-4a3f-c968-0e78b37861db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>abstract_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uk8rfroj</td>\n",
       "      <td>complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal</td>\n",
       "      <td>a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.</td>\n",
       "      <td>[complet, genom, sequenc, 2019, novel, coronavirus, sar, cov, strain, isol, nepal]</td>\n",
       "      <td>[complet, genom, sequenc, obtain, sever, acut, respiratori, syndrom, coronavirus, sar, cov, strain, isol, oropharyng, swab, specimen, nepales, patient, coronavirus, diseas, 2019, covid, 19, return, nepal, travel, wuhan, china]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid  \\\n",
       "0  uk8rfroj   \n",
       "\n",
       "                                                                                        title  \\\n",
       "0  complete genome sequence of a 2019 novel coronavirus (sars-cov-2) strain isolated in nepal   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                    abstract  \\\n",
       "0  a complete genome sequence was obtained for a severe acute respiratory syndrome coronavirus 2 (sars-cov-2) strain isolated from an oropharyngeal swab specimen of a nepalese patient with coronavirus disease 2019 (covid-19), who had returned to nepal after traveling to wuhan, china.   \n",
       "\n",
       "                                                                         title_tokens  \\\n",
       "0  [complet, genom, sequenc, 2019, novel, coronavirus, sar, cov, strain, isol, nepal]   \n",
       "\n",
       "                                                                                                                                                                                                                      abstract_tokens  \n",
       "0  [complet, genom, sequenc, obtain, sever, acut, respiratori, syndrom, coronavirus, sar, cov, strain, isol, oropharyng, swab, specimen, nepales, patient, coronavirus, diseas, 2019, covid, 19, return, nepal, travel, wuhan, china]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alias stemmer method\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# stem Headline_tokens and articleBody_tokens\n",
    "df_2['title_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.title_tokens], axis=1)\n",
    "df_2['abstract_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.abstract_tokens], axis=1)\n",
    "#df_2['title_abstract_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.abstract_tokens], axis=1)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6v3k8kkPdWiR"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9uVxqF2ebEo"
   },
   "source": [
    "## Basic Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CT2lnPsqdYha"
   },
   "outputs": [],
   "source": [
    "# https://github.com/Cisco-Talos/fnc-1/blob/master/tree_model/ngram.py\n",
    "\n",
    "# create functions to build n_grams\n",
    "def getUnigram(words):\n",
    "    #assert type(words) == []\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1, skip+2):\n",
    "                if i + k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i+k]]))\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "        return lst\n",
    "                    \n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    #assert type(words) == []\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1, skip+2):\n",
    "                for k2 in range(1, skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i+k1], words[i+k1+k2]]))\n",
    "        return lst\n",
    "    else:\n",
    "        #set as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "        return lst\n",
    "    \n",
    "def getFourgram(words, join_string):\n",
    "\n",
    "    #assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 3:\n",
    "        lst = []\n",
    "        for i in xrange(L-3):\n",
    "            lst.append( join_string.join([words[i], words[i+1], words[i+2], words[i+3]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getTrigram(words, join_string)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def getBiterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny', 'boy']\n",
    "        Output: a list of biterm, e.g., ['I_am', 'I_Denny', 'I_boy', 'am_Denny', 'am_boy', 'Denny_boy']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                lst.append( join_string.join([words[i], words[j]]) )\n",
    "        return lst\n",
    "    \n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "    return lst\n",
    "    \n",
    "def getTriterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of triterm, e.g., ['I_am_Denny', 'I_Denny_am', 'am_I_Denny',\n",
    "        'am_Denny_I', 'Denny_I_am', 'Denny_am_I']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in xrange(L-2):\n",
    "            for j in xrange(i+1,L-1):\n",
    "                for k in xrange(j+1,L):\n",
    "                    lst.append( join_string.join([words[i], words[j], words[k]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as biterm\n",
    "        lst = getBiterm(words, join_string)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZXY7J6Ddc0f"
   },
   "outputs": [],
   "source": [
    "# generate unigram\n",
    "df_2[\"title_unigram\"] = df_2[\"title_tokens\"].map(lambda x: getUnigram(x))\n",
    "df_2[\"abstract_unigram\"] = df_2[\"abstract_tokens\"].map(lambda x: getUnigram(x))\n",
    "#df_2[\"title_abstract_unigram\"] = df_2[\"title_abstract_tokens\"].map(lambda x: getUnigram(x))\n",
    "\n",
    "# generate bigram\n",
    "join_str = \"_\"\n",
    "df_2[\"title_bigram\"] = df_2[\"title_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "df_2[\"abstract_bigram\"] = df_2[\"abstract_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "#df_2[\"title_abstract_bigram\"] = df_2[\"title_abstract_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "        \n",
    "# generate trigram\n",
    "join_str = \"_\"\n",
    "df_2[\"title_trigram\"] = df_2[\"title_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "df_2[\"abstract_trigram\"] = df_2[\"abstract_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "#df_2[\"title_abstract_trigram\"] = df_2[\"title_abstract_unigram\"].map(lambda x: getTrigram(x, join_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_Vu4C-Udf48"
   },
   "outputs": [],
   "source": [
    "# calc percent of text in given Headline or articleBody that is unique ( unique grams / ttl grams)\n",
    "\n",
    "''' \n",
    "    count ttl # of n-gram\n",
    "    count ttl # of unique n-gram\n",
    "    divide ttl # uniqe by ttl #\n",
    "    \n",
    "'''\n",
    "\n",
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "feat_names = [\"title\", \"abstract\"]\n",
    "\n",
    "for feat_name in feat_names:\n",
    "    for gram in grams:\n",
    "        df_2[\"count_of_%s_%s\" % (feat_name, gram)] = list(df_2.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        df_2[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "              list(df_2.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        df_2[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "            df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)] / df_2[\"count_of_%s_%s\"%(feat_name,gram)]\n",
    "            #map(try_divide, df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)], df_2[\"count_of_%s_%s\"%(feat_name,gram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh3VAO3ndh-o"
   },
   "outputs": [],
   "source": [
    "# overlapping n-grams count\n",
    "\n",
    "for gram in grams:\n",
    "    # count grams appearing in Headline that are also inside its coresponding articleBody\n",
    "    df_2[\"count_of_title_%s_in_abstract\" % gram] = \\\n",
    "        list(df_2.apply(lambda x: sum([1. for w in x[\"title_\" + gram] if w in set(x[\"abstract_\" + gram])]), axis=1))\n",
    "    \n",
    "    # return the ratio of overlapping grams to ttl Headline grams\n",
    "    df_2[\"ratio_of_title_%s_in_abstract\" % gram] = \\\n",
    "        df_2[\"count_of_title_%s_in_abstract\" % gram] / df_2[\"count_of_title_%s\" % gram]\n",
    "        #map(try_divide, df[\"count_of_Headline_%s_in_articleBody\" % gram], df[\"count_of_Headline_%s\" % gram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKeqgLfydkRn"
   },
   "outputs": [],
   "source": [
    "# count number of sentences in title, abstract\n",
    "for feat_name in feat_names:\n",
    "    df_2['len_sent_%s' % feat_name] = df_2[feat_name].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18425,
     "status": "ok",
     "timestamp": 1589245341413,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "Han8LLrTdmC7",
    "outputId": "517b3705-1d9c-4e9a-9b63-b424d3d339b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_of_title_unigram',\n",
       " 'count_of_unique_title_unigram',\n",
       " 'ratio_of_unique_title_unigram',\n",
       " 'count_of_title_bigram',\n",
       " 'count_of_unique_title_bigram',\n",
       " 'ratio_of_unique_title_bigram',\n",
       " 'count_of_title_trigram',\n",
       " 'count_of_unique_title_trigram',\n",
       " 'ratio_of_unique_title_trigram',\n",
       " 'count_of_abstract_unigram',\n",
       " 'count_of_unique_abstract_unigram',\n",
       " 'ratio_of_unique_abstract_unigram',\n",
       " 'count_of_abstract_bigram',\n",
       " 'count_of_unique_abstract_bigram',\n",
       " 'ratio_of_unique_abstract_bigram',\n",
       " 'count_of_abstract_trigram',\n",
       " 'count_of_unique_abstract_trigram',\n",
       " 'ratio_of_unique_abstract_trigram',\n",
       " 'count_of_title_unigram_in_abstract',\n",
       " 'ratio_of_title_unigram_in_abstract',\n",
       " 'count_of_title_bigram_in_abstract',\n",
       " 'ratio_of_title_bigram_in_abstract',\n",
       " 'count_of_title_trigram_in_abstract',\n",
       " 'ratio_of_title_trigram_in_abstract',\n",
       " 'len_sent_title',\n",
       " 'len_sent_abstract']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # save basic count features to disk for later use\n",
    "\n",
    "feat_names_bcf = [ n for n in df_2.columns \\\n",
    "                if \"count\" in n \\\n",
    "                or \"ratio\" in n \\\n",
    "                or \"len_sent\" in n]\n",
    "\n",
    "\n",
    "feat_names_bcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3957, 26)\n"
     ]
    }
   ],
   "source": [
    "xBasicCounts = df_2[feat_names_bcf].values\n",
    "print(type(xBasicCounts))\n",
    "print(xBasicCounts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basic_count_features.pkl\", \"wb\") as outfile:\n",
    "    #pickle.dump(feat_names, outfile, -1)\n",
    "    pickle.dump(xBasicCounts, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UTOKrX7drk2"
   },
   "source": [
    "## Latent Symantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT7IHT-yhd7E"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6BHAsOLdt3s"
   },
   "outputs": [],
   "source": [
    "def cat_text(x):\n",
    "    res = '%s %s' % (' '.join(x['title_unigram']), ' '.join(x['abstract_unigram']))\n",
    "    return res\n",
    "\n",
    "# concatenate title and abstract so we can fit a tfidf vectorizer that will learn the combined vocabulary\n",
    "df_2['all_text'] = list(df_2.apply(cat_text, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFe60YdxduSU"
   },
   "outputs": [],
   "source": [
    "# fit a TfidfVectorizer on the concatenated strings (fit learns the vocabulary and idf)\n",
    "\n",
    "#vec = TfidfVectorizer(ngram_range = (1, 3), max_df= 0.8, min_df= 2)\n",
    "vec = TfidfVectorizer(ngram_range = (1, 3))\n",
    "vec.fit(df_2['all_text'])\n",
    "vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25900,
     "status": "ok",
     "timestamp": 1589245348927,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "JIsdlu1qnpM9",
    "outputId": "a37601ae-cd3e-45cd-c230-f8cdcec6a42c"
   },
   "outputs": [],
   "source": [
    "# transform title unigrams into tf-idf vector using the learned vocabulary\n",
    "vec_Title = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df= 2, vocabulary=vocabulary)\n",
    "Title_tfidf = vec_Title.fit_transform(df_2['title_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"Title_tfidf.shape:\" + str(Title_tfidf.shape))\n",
    "\n",
    "# transform abstract unigrams using the learned vocabulary\n",
    "vec_abstract = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)\n",
    "abstract_tfidf = vec_abstract.fit_transform(df_2['abstract_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"abstract_tfidf.shape:\" +  str(abstract_tfidf.shape))\n",
    "\n",
    "# save title tfidf for later use\n",
    "outfilename_title_tfidf = \"title_tfidf.pkl\"\n",
    "with open (outfilename_title_tfidf, 'wb') as outfile:\n",
    "    pickle.dump(Title_tfidf, outfile, -1)\n",
    "    \n",
    "# save abstract tfidf for later use\n",
    "outfilename_abstract_tfidf = \"abstract_tfidf.pkl\"\n",
    "with open(outfilename_abstract_tfidf, \"wb\") as outfile:\n",
    "    pickle.dump(abstract_tfidf, outfile, -1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovVMLGD_d3cD"
   },
   "outputs": [],
   "source": [
    "'''scikit-learn has a cosine_similarity function though, we must consider the input shape of our data and the desired output shape. \n",
    "   We need to take in extremely large 2-D arrays and end up with a 2-D array of one feature. To do this, we first convert each input \n",
    "   into a Coordinate Format matrix before computing cosine_similarity , calculate the row-wise cosine_similarity and finally coerce it \n",
    "   from a 1-D to 2-D array.'''\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1)\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print (x)\n",
    "        print (y)\n",
    "        d = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37867,
     "status": "ok",
     "timestamp": 1589245360932,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "OvOtondId5cr",
    "outputId": "f03aaf69-93ae-494f-a1eb-3c85efb3c33c"
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarity between Headline and articleBody\n",
    "\n",
    "simTfidf_train = np.asarray(list(map(cosine_sim,Title_tfidf, abstract_tfidf)))[:, np.newaxis]\n",
    "\n",
    "print(simTfidf_train.shape)\n",
    "\n",
    "# save for later use\n",
    "outfilename_simtfidf_train = \"sim_tfidf.pkl\"\n",
    "with open(outfilename_simtfidf_train, \"wb\") as outfile:\n",
    "    pickle.dump(simTfidf_train, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eum1TxoeWkPF"
   },
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG6aGr1DPNF9"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "x_title_abstract_tfidf = vstack((Title_tfidf, abstract_tfidf)).toarray() # toarray() converts the csr_matrix objects to numpy arrays\n",
    "svd = TruncatedSVD(n_components=100, n_iter=15, random_state = 42)\n",
    "\n",
    "print(x_title_abstract_tfidf.shape)\n",
    "\n",
    "# fit to the combined train-test set \n",
    "svd.fit(x_title_abstract_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08pzKg4WY8nx"
   },
   "outputs": [],
   "source": [
    "# transform title tfidf features using svd\n",
    "x_title_Svd = svd.transform(Title_tfidf)\n",
    "print ('x_title_Svd.shape:')\n",
    "print (x_title_Svd.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"title_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(TitleSvdTrain, outfile, -1)\n",
    "    \n",
    "    \n",
    "# transform abstract tfidf features using svd\n",
    "x_abstract_Svd = svd.transform(abstract_tfidf)\n",
    "print ('x_abstract_Svd.shape:')\n",
    "print (x_abstract_Svd.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"abstract_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(x_abstract_Svd, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity for each record\n",
    "\n",
    "simSvd_train = np.asarray(list(map(cosine_sim, x_title_Svd, x_abstract_Svd)))[:, np.newaxis]\n",
    "print ('sim_svd_train shape:')\n",
    "print (simSvd_train.shape)\n",
    "\n",
    "# save for later use\n",
    "\n",
    "with open(\"sim_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(simSvd_train, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[\"title_unigram_vec\"] = df_2[\"title_tokens\"]\n",
    "df_2[\"abstract_unigram_vec\"] = df_2[\"abstract_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_unigram_array = df_2[\"title_unigram_vec\"].values\n",
    "print(\"df_2 title_unigram_vec type: %s\" % type(df_2[\"title_unigram_vec\"]))\n",
    "print(\"title_unigram_array type: %s\" % type(title_unigram_array))\n",
    "print()\n",
    "\n",
    "\n",
    "title_vec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), title_unigram_array)))\n",
    "title_vec_norm = normalize(title_vec)\n",
    "print(\"title_vec type: %s\" % type(title_vec))\n",
    "print(\"title_vec shape:\" +  str(title_vec.shape))\n",
    "print()\n",
    "print(\"title_vec_norm type: %s\" % type(title_vec_norm))\n",
    "print(\"title_vec_norm shape:\" + str(title_vec_norm.shape))\n",
    "\n",
    "#save word embeddings\n",
    "with open(\"title_w2v.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(title_vec_norm, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_unigram_array = df_2[\"abstract_unigram_vec\"].values\n",
    "print(\"df_2 abstract_unigram_vec type: %s\" % type(df_2[\"abstract_unigram_vec\"]))\n",
    "print(\"abstract_unigram_array type: %s\" % type(abstract_unigram_array))\n",
    "print()\n",
    "\n",
    "abstract_vec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), abstract_unigram_array)))\n",
    "abstract_vec_norm = normalize(abstract_vec)\n",
    "\n",
    "print(\"abstract_vec type: %s\" % type(abstract_vec))\n",
    "print(\"abstract_vec:\" +  str(abstract_vec.shape))\n",
    "print()\n",
    "print(\"abstract_vec_norm type: %s\" % type(abstract_vec_norm))\n",
    "print(\"abstract_vec_norm shape:\" + str(abstract_vec_norm.shape))\n",
    "\n",
    "with open(\"abstract_w2v.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(abstract_vec_norm, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between title & abstract word2vec features\n",
    "simVec_w2v = np.asarray(list(map(cosine_sim, title_vec_norm, abstract_vec_norm)))[:, np.newaxis]\n",
    "print(type(simVec_w2v))\n",
    "print(simVec_w2v.shape)\n",
    "print(\"simVec_w2v num dimensions:\" + str(simVec_w2v.ndim))\n",
    "print(simVec_w2v[0:2])\n",
    "\n",
    "with open(\"sim_w2v.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(simVec_w2v, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate polarity score of each sentance in a Headline observation and return the average\n",
    "\n",
    "sid = SentimentIntensityAnalyzer() # https://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence) # https://www.nltk.org/howto/sentiment.html\n",
    "        result.append(ss)\n",
    "    return pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[\"title_senti\"] = df_2['title'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "df_2 = pd.concat([df_2, df_2['title_senti'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "df_2.rename(columns={'compound':'T_compound', 'neg':'T_neg', 'neu':'T_neu', 'pos':'T_pos'}, inplace=True)\n",
    "\n",
    "Title_Senti = df_2[['T_compound','T_neg','T_neu','T_pos']].values\n",
    "print ('Title_Senti shape:' + str(Title_Senti.shape))\n",
    "print()\n",
    "\n",
    "# save title sentiment\n",
    "with open(\"title_sentiment.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(Title_Senti, outfile, -1)\n",
    "    \n",
    "\n",
    "\n",
    "df_2[\"abstract_senti\"] = df_2['abstract'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "df_2 = pd.concat([df_2, df_2['abstract_senti'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "df_2.rename(columns={'compound':'A_compound', 'neg':'A_neg', 'neu':'A_neu', 'pos':'A_pos'}, inplace=True)\n",
    "\n",
    "Abstract_Senti = df_2[['A_compound','A_neg','A_neu','A_pos']].values\n",
    "print ('Abstract_Senti shape:' + str(Abstract_Senti.shape))\n",
    "\n",
    "# save abstract sentiment\n",
    "with open(\"abstract_sentiment.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(Abstract_Senti, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine engineered features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"CORD_data/basic_count_features.pkl\", \"rb\") as infile:\n",
    "    #feat_names = pickle.load(infile)\n",
    "    basic_count_feats = pickle.load(infile)\n",
    "    \n",
    "with open (\"CORD_data/title_tfidf.pkl\", \"rb\") as infile_:\n",
    "    title_tfidf = pickle.load(infile_)\n",
    "    title_tfidf = title_tfidf.toarray()\n",
    "    \n",
    "with open (\"CORD_data/abstract_tfidf.pkl\", \"rb\") as outfile:\n",
    "    abstract_tfidf = pickle.load(outfile)\n",
    "    abstract_tfidf = abstract_tfidf.toarray()\n",
    "\n",
    "with open (\"CORD_data/sim_tfidf.pkl\", \"rb\") as outfile_:\n",
    "    sim_tfidf = pickle.load(outfile_)\n",
    "    \n",
    "with open (\"CORD_data/title_svd.pkl\", \"rb\") as svd_title:\n",
    "    title_svd = pickle.load(svd_title)\n",
    "    \n",
    "with open (\"CORD_data/abstract_svd.pkl\", \"rb\") as svd_abstract:\n",
    "    abstract_svd = pickle.load(svd_abstract)\n",
    "    \n",
    "with open (\"CORD_data/sim_svd.pkl\", \"rb\") as sim_svd:\n",
    "    sim_svd = pickle.load(sim_svd)\n",
    "    \n",
    "with open (\"CORD_data/title_w2v.pkl\", \"rb\") as Tw:\n",
    "    title_w2v = pickle.load(Tw)\n",
    "    \n",
    "with open (\"CORD_data/abstract_w2v.pkl\", \"rb\") as Aw:\n",
    "    abstract_w2v = pickle.load(Aw)\n",
    "    \n",
    "with open (\"CORD_data/sim_w2v.pkl\", \"rb\") as Sw:\n",
    "    sim_w2v = pickle.load(Sw)\n",
    "    \n",
    "with open (\"CORD_data/title_sentiment.pkl\", \"rb\") as Ts:\n",
    "    title_sentiment = pickle.load(Ts)\n",
    "    \n",
    "with open (\"CORD_data/abstract_sentiment.pkl\", \"rb\") as As:\n",
    "    abstract_sentiment = pickle.load(As)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [basic_count_feats, sim_tfidf, \n",
    "           title_svd, abstract_svd, sim_svd, \n",
    "           title_w2v, abstract_w2v, sim_w2v, \n",
    "           title_sentiment, abstract_sentiment]\n",
    "\n",
    "for vec in vectors:\n",
    "    print(vec.ndim)\n",
    "    print(vec.shape)\n",
    "    print(type(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_data = np. hstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3957, 837)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cord_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"engineered_cord_data.pkl\", \"wb\") as all_data:\n",
    "    pickle.dump(cord_data, all_data, protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_FNC_model = pickle.load(open(\"/content/drive/My Drive/CoronaWhy/mini_task_CONTRADICTION/data_files/FNC data for CORD/XGB_fit_fnc_features_for_cord.pkl\", \"rb\"))\n",
    "\n",
    "loaded_contra_features = pickle.load(open(\"/content/drive/My Drive/CoronaWhy/mini_task_CONTRADICTION/data_files/CONTRADICTION_data/engineered_cord_data.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use XGB predict on contradiction features\n",
    "predictions = loaded_FNC_model.predict(loaded_contra_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(predictions, columns = ['predictions'])\n",
    "\n",
    "contra_txt_df = pd.read_csv(\"/content/drive/My Drive/CoronaWhy/mini_task_CONTRADICTION/data_files/vt_contra_v9_covid19_metadata_200425.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt_and_preds = pd.concat([contra_txt_df, df_preds], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds1.head(1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjT1Ia/SRxjIzfxRX8XVX9",
   "machine_shape": "hm",
   "mount_file_id": "1IDod64rbBcCPclXd2CiIdvHocLIIB1Vv",
   "name": "Task_VT_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
