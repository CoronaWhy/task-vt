{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset: drug_individual_claims_filtered(1).csv_\n",
    "### using mancon for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZU6WjSZeIkT"
   },
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n",
    "#pip install dtale\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OutuE2qcHvF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mngav\\AppData\\Local\\Continuum\\anaconda3\\envs\\learn-env\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.float_format = '{:, .2f}'.format\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "from numpy import savez_compressed\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#from scipy.misc import comb, logsumexp\n",
    "from sklearn.manifold import TSNE #a tool to visualize high dimensional data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # dimensionality reduction using truncated SVD (AKA LSA)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.collocations import *\n",
    "import string #python module\n",
    "import re # python regex module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2444,
     "status": "ok",
     "timestamp": 1589245325266,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "XakrAQa2cWmC",
    "outputId": "745604f5-ca71-4be3-a265-8d5b886fe246"
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "train_df = pd.read_csv(\"manconcorpus_sent_pairs_200516.tsv\", delimiter = \"\\t\", encoding = \"utf-8\")\n",
    "test_df = pd.read_excel(\"drug_individual_claims_similarity_annotated_v05.19.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "y_train.replace({'neutral': '0', 'entailment': '1', 'contradiction':'2'}, inplace=True)\n",
    "y_train.to_csv(\"y_train.csv\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17911,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15217\n",
       "1     1966\n",
       "2      728\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28643 entries, 0 to 28642\n",
      "Data columns (total 9 columns):\n",
      "paper1_cord_uid     28643 non-null object\n",
      "paper2_cord_uid     28643 non-null object\n",
      "text1               28643 non-null object\n",
      "text2               28643 non-null object\n",
      "similarity_score    28643 non-null float64\n",
      "drugs1              28643 non-null object\n",
      "drugs2              28643 non-null object\n",
      "annotation          107 non-null object\n",
      "evaluated_for       104 non-null object\n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "488UtCCicb7U"
   },
   "outputs": [],
   "source": [
    "# isolate germane featrues in test_df\n",
    "test_df = copy.deepcopy(test_df[['paper1_cord_uid','paper2_cord_uid','text1','text2']])\n",
    "\n",
    "# add a guid column to cord df so we can eventually stack all the data and preprocess together\n",
    "test_df.insert(0, 'guid', range(17911, 17911 + len(test_df)))\n",
    "\n",
    "# rename cord text columns to match mancon column names\n",
    "test_df.rename(columns= {\"text1\":\"text_a\", \"text2\":\"text_b\"}, inplace=True)\n",
    "\n",
    "# drop paper1_cord_uid and paper2_cord_uid\n",
    "test_df.drop(columns = ['paper1_cord_uid', 'paper2_cord_uid'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test data\n",
    "frames = [train_df, test_df]\n",
    "data = pd.concat(frames, sort = False)\n",
    "data = data[['guid', 'text_a', 'text_b', 'label']]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtale\n",
    "d = dtale.show(data, ignore_duplicate=True)\n",
    "d\n",
    "#d.kill(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYfxb1foc9ls"
   },
   "source": [
    "# Preprocessing -- Normalize Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2739,
     "status": "ok",
     "timestamp": 1589245325596,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "u49fyZxIdB4T",
    "outputId": "c148c47c-9f8a-40fc-9a6e-5df8a2352b04"
   },
   "outputs": [],
   "source": [
    "# lowercase all text\n",
    "#df_2['text1'] = df_2['text1'].str.lower()\n",
    "#df_2['text2'] = df_2['text2'].str.lower()\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer (r\"(?u)\\b\\w\\w+\\b\")\n",
    "#data['text1_tokens'] = data['text1'].map(tokenizer.tokenize)\n",
    "#data['text2_tokens'] = data['text2'].map(tokenizer.tokenize)\n",
    "data['text_a_tokens'] = data['text_a'].map(tokenizer.tokenize)\n",
    "data['text_b_tokens'] = data['text_b'].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1589245325861,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "Tft5HKredDCU",
    "outputId": "9595bc8d-ff98-48c1-9d92-483cf4d2174e"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt') # a sentance tokenizer\n",
    "nltk.download('gutenberg') # a text corpora and lexical resources\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruaNwi4xdQHi"
   },
   "outputs": [],
   "source": [
    "# instantiate list of stop words and other characters/punctuation to remove\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``',\"_\"]\n",
    "\n",
    "# remove stop words / keep everything except stopwords_list\n",
    "\n",
    "#data['text1_tokens'] = data['text1_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "#data['text2_tokens'] = data['text2_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "\n",
    "data['text_a_tokens'] = data['text_a_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "data['text_b_tokens'] = data['text_b_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12173,
     "status": "ok",
     "timestamp": 1589245335056,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "HcIOizDrdSgi",
    "outputId": "685a7e83-e846-4a3f-c968-0e78b37861db"
   },
   "outputs": [],
   "source": [
    "# alias stemmer method\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# stem Headline_tokens and articleBody_tokens\n",
    "\n",
    "#data['text1_tokens'] = data.apply(lambda row: [stemmer.stem(item) for item in row.text1_tokens], axis=1)\n",
    "#data['text2_tokens'] = data.apply(lambda row: [stemmer.stem(item) for item in row.text2_tokens], axis=1)\n",
    "\n",
    "data['text_a_tokens'] = data.apply(lambda row: [stemmer.stem(item) for item in row.text_a_tokens], axis=1)\n",
    "data['text_b_tokens'] = data.apply(lambda row: [stemmer.stem(item) for item in row.text_b_tokens], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6v3k8kkPdWiR"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "D9uVxqF2ebEo"
   },
   "source": [
    "## Basic Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "CT2lnPsqdYha"
   },
   "outputs": [],
   "source": [
    "# https://github.com/Cisco-Talos/fnc-1/blob/master/tree_model/ngram.py\n",
    "\n",
    "# create functions to build n_grams\n",
    "def getUnigram(words):\n",
    "    #assert type(words) == []\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1, skip+2):\n",
    "                if i + k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i+k]]))\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "        return lst\n",
    "                    \n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    #assert type(words) == []\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1, skip+2):\n",
    "                for k2 in range(1, skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i+k1], words[i+k1+k2]]))\n",
    "        return lst\n",
    "    else:\n",
    "        #set as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "        return lst\n",
    "    \n",
    "def getFourgram(words, join_string):\n",
    "\n",
    "    #assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 3:\n",
    "        lst = []\n",
    "        for i in xrange(L-3):\n",
    "            lst.append( join_string.join([words[i], words[i+1], words[i+2], words[i+3]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getTrigram(words, join_string)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def getBiterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny', 'boy']\n",
    "        Output: a list of biterm, e.g., ['I_am', 'I_Denny', 'I_boy', 'am_Denny', 'am_boy', 'Denny_boy']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                lst.append( join_string.join([words[i], words[j]]) )\n",
    "        return lst\n",
    "    \n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "    return lst\n",
    "    \n",
    "def getTriterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of triterm, e.g., ['I_am_Denny', 'I_Denny_am', 'am_I_Denny',\n",
    "        'am_Denny_I', 'Denny_I_am', 'Denny_am_I']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in xrange(L-2):\n",
    "            for j in xrange(i+1,L-1):\n",
    "                for k in xrange(j+1,L):\n",
    "                    lst.append( join_string.join([words[i], words[j], words[k]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as biterm\n",
    "        lst = getBiterm(words, join_string)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "pZXY7J6Ddc0f"
   },
   "outputs": [],
   "source": [
    "# generate unigram\n",
    "#data[\"text1_unigram\"] = data[\"text1_tokens\"].map(lambda x: getUnigram(x))\n",
    "#data[\"text2_unigram\"] = data[\"text2_tokens\"].map(lambda x: getUnigram(x))\n",
    "data[\"text_a_unigram\"] = data[\"text_a_tokens\"].map(lambda x: getUnigram(x))\n",
    "data[\"text_b_unigram\"] = data[\"text_b_tokens\"].map(lambda x: getUnigram(x))\n",
    "\n",
    "# generate bigram\n",
    "join_str = \"_\"\n",
    "#data[\"text1_bigram\"] = data[\"text1_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "#data[\"text2_bigram\"] = data[\"text2_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "data[\"text_a_bigram\"] = data[\"text_a_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "data[\"text_b_bigram\"] = data[\"text_b_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "        \n",
    "# generate trigram\n",
    "join_str = \"_\"\n",
    "#data[\"text1_trigram\"] = data[\"text1_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "#data[\"text2_trigram\"] = data[\"text2_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "data[\"text_a_trigram\"] = data[\"text_a_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "data[\"text_b_trigram\"] = data[\"text_b_unigram\"].map(lambda x: getTrigram(x, join_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "w_Vu4C-Udf48"
   },
   "outputs": [],
   "source": [
    "# calc percent of text in given Headline or articleBody that is unique ( unique grams / ttl grams)\n",
    "\n",
    "''' \n",
    "    count ttl # of n-gram\n",
    "    count ttl # of unique n-gram\n",
    "    divide ttl # uniqe by ttl #\n",
    "    \n",
    "'''\n",
    "\n",
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "feat_names = [ \"text_a\", \"text_b\"]\n",
    "\n",
    "for feat_name in feat_names:\n",
    "    for gram in grams:\n",
    "        data[\"count_of_%s_%s\" % (feat_name, gram)] = list(data.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        data[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "              list(data.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        data[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "            data[\"count_of_unique_%s_%s\"%(feat_name,gram)] / data[\"count_of_%s_%s\"%(feat_name,gram)]\n",
    "            #map(try_divide, df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)], df_2[\"count_of_%s_%s\"%(feat_name,gram)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Qh3VAO3ndh-o"
   },
   "outputs": [],
   "source": [
    "# overlapping n-grams count\n",
    "\n",
    "for gram in grams:\n",
    "    # count grams appearing in Headline that are also inside its coresponding articleBody\n",
    "    data[\"count_of_text_a_%s_in_text_b\" % gram] = \\\n",
    "        list(data.apply(lambda x: sum([1. for w in x[\"text_a_\" + gram] if w in set(x[\"text_b_\" + gram])]), axis=1))\n",
    "    \n",
    "    # return the ratio of overlapping grams to ttl Headline grams\n",
    "    data[\"ratio_of_text_a_%s_in_text_b\" % gram] = \\\n",
    "        data[\"count_of_text_a_%s_in_text_b\" % gram] / data[\"count_of_text_a_%s\" % gram]\n",
    "        #map(try_divide, df[\"count_of_Headline_%s_in_articleBody\" % gram], df[\"count_of_Headline_%s\" % gram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "lKeqgLfydkRn"
   },
   "outputs": [],
   "source": [
    "# count number of sentences in title, abstract\n",
    "for feat_name in feat_names:\n",
    "    data['len_sent_%s' % feat_name] = data[feat_name].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18425,
     "status": "ok",
     "timestamp": 1589245341413,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "hidden": true,
    "id": "Han8LLrTdmC7",
    "outputId": "517b3705-1d9c-4e9a-9b63-b424d3d339b6"
   },
   "outputs": [],
   "source": [
    " # save basic count features to disk for later use\n",
    "\n",
    "feat_names_bcf = [ n for n in data.columns \\\n",
    "                if \"count\" in n \\\n",
    "                or \"ratio\" in n \\\n",
    "                or \"len_sent\" in n]\n",
    "\n",
    "\n",
    "feat_names_bcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xBasicCounts = data[feat_names_bcf].values\n",
    "print(type(xBasicCounts))\n",
    "print(xBasicCounts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"data_with_ngrams_&_basic_Counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"basic_count_features.pkl\", \"wb\") as outfile:\n",
    "    #pickle.dump(feat_names, outfile, -1)\n",
    "    pickle.dump(xBasicCounts, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UTOKrX7drk2"
   },
   "source": [
    "## Latent Symantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT7IHT-yhd7E"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_with_ngrams_&_basic_Counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6BHAsOLdt3s"
   },
   "outputs": [],
   "source": [
    "def cat_text(x):\n",
    "    res = '%s %s' % (' '.join(x['text_a_unigram']), ' '.join(x['text_b_unigram']))\n",
    "    return res\n",
    "\n",
    "# concatenate title and abstract so we can fit a tfidf vectorizer that will learn the combined vocabulary\n",
    "data['all_text'] = list(data.apply(cat_text, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFe60YdxduSU"
   },
   "outputs": [],
   "source": [
    "# fit a TfidfVectorizer on the concatenated strings (fit learns the vocabulary and idf)\n",
    "\n",
    "#vec = TfidfVectorizer(ngram_range = (1, 3), max_df= 0.8, min_df= 2)\n",
    "vec = TfidfVectorizer(ngram_range = (1, 3))\n",
    "vec.fit(data['all_text'])\n",
    "vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25900,
     "status": "ok",
     "timestamp": 1589245348927,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "JIsdlu1qnpM9",
    "outputId": "a37601ae-cd3e-45cd-c230-f8cdcec6a42c"
   },
   "outputs": [],
   "source": [
    "# transform title unigrams into tf-idf vector using the learned vocabulary\n",
    "vec_text_a = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df= 2, vocabulary=vocabulary)\n",
    "text_a_tfidf = vec_text_a.fit_transform(data['text_a_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"text_a_tfidf.shape:\" + str(text_a_tfidf.shape))\n",
    "\n",
    "# transform abstract unigrams using the learned vocabulary\n",
    "vec_text_b = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)\n",
    "text_b_tfidf = vec_text_b.fit_transform(data['text_b_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"text_b_tfidf.shape:\" +  str(text_b_tfidf.shape))\n",
    "\n",
    "# save text1 tfidf for later use\n",
    "with open (\"text_a_tfidf.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(text_a_tfidf, outfile, -1)\n",
    "    \n",
    "# save text2 tfidf for later use\n",
    "with open(\"text_b_tfidf.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(text_b_tfidf, outfile, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovVMLGD_d3cD"
   },
   "outputs": [],
   "source": [
    "'''scikit-learn has a cosine_similarity function though, we must consider the input shape of our data and the desired output shape. \n",
    "   We need to take in extremely large 2-D arrays and end up with a 2-D array of one feature. To do this, we first convert each input \n",
    "   into a Coordinate Format matrix before computing cosine_similarity , calculate the row-wise cosine_similarity and finally coerce it \n",
    "   from a 1-D to 2-D array.'''\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1)\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print (x)\n",
    "        print (y)\n",
    "        d = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37867,
     "status": "ok",
     "timestamp": 1589245360932,
     "user": {
      "displayName": "Matan G",
      "photoUrl": "",
      "userId": "11084293173807468288"
     },
     "user_tz": 240
    },
    "id": "OvOtondId5cr",
    "outputId": "f03aaf69-93ae-494f-a1eb-3c85efb3c33c"
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarity between Headline and articleBody\n",
    "\n",
    "simTfidf = np.asarray(list(map(cosine_sim,text_a_tfidf, text_b_tfidf)))[:, np.newaxis]\n",
    "\n",
    "print(simTfidf.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"sim_tfidf.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(simTfidf, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eum1TxoeWkPF"
   },
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG6aGr1DPNF9"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack\n",
    "\n",
    "x_text_a_text_b_tfidf = vstack((text_a_tfidf, text_b_tfidf)).toarray() # toarray() converts the csr_matrix objects to numpy arrays\n",
    "svd = TruncatedSVD(n_components=100, n_iter=15, random_state = 42)\n",
    "\n",
    "print(x_text_a_text_b_tfidf.shape)\n",
    "\n",
    "# fit to the combined train-test set \n",
    "svd.fit(x_text_a_text_b_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08pzKg4WY8nx"
   },
   "outputs": [],
   "source": [
    "# transform title tfidf features using svd\n",
    "x_text_a_Svd = svd.transform(text_a_tfidf)\n",
    "print ('x_text_a_Svd.shape:')\n",
    "print (x_text_a_Svd.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"text_a_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(x_text_a_Svd, outfile, -1)\n",
    "    \n",
    "    \n",
    "# transform abstract tfidf features using svd\n",
    "x_text_b_Svd = svd.transform(text_b_tfidf)\n",
    "print ('x_text_b_Svd.shape:')\n",
    "print (x_text_b_Svd.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"text_b_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(x_text_b_Svd, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity for each record\n",
    "\n",
    "simSvd = np.asarray(list(map(cosine_sim, x_text_a_Svd, x_text_b_Svd)))[:, np.newaxis]\n",
    "print ('simSvd shape:')\n",
    "print (simSvd.shape)\n",
    "\n",
    "# save for later use\n",
    "with open(\"sim_svd.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(simSvd, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioWordVec Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_a_unigram_vec\"] = data[\"text_a_tokens\"]\n",
    "data[\"text_b_unigram_vec\"] = data[\"text_b_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('bio_embedding_intrinsic', \n",
    "                                                        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a_unigram_array = data[\"text_a_unigram_vec\"].values\n",
    "print(\"text_a_unigram_vec type: %s\" % type(data[\"text_a_unigram_vec\"]))\n",
    "print(\"text_a_unigram_array type: %s\" % type(text_a_unigram_array))\n",
    "print()\n",
    "\n",
    "\n",
    "text_a_vec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*200), text_a_unigram_array)))\n",
    "text_a_vec_norm = normalize(text_a_vec)\n",
    "print(\"text_a_vec type: %s\" % type(text_a_vec))\n",
    "print(\"text_a_vec shape:\" +  str(text_a_vec.shape))\n",
    "print()\n",
    "print(\"text_a_vec_norm type: %s\" % type(text_a_vec_norm))\n",
    "print(\"text_a_vec_norm shape:\" + str(text_a_vec_norm.shape))\n",
    "\n",
    "#save word embeddings\n",
    "with open(\"text_a_BioWordVec.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(text_a_vec_norm, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_b_unigram_array = data[\"text_b_unigram_vec\"].values\n",
    "print(\"text_b_unigram_vec type: %s\" % type(data[\"text_b_unigram_vec\"]))\n",
    "print(\"text_b_unigram_array type: %s\" % type(text_b_unigram_array))\n",
    "print()\n",
    "\n",
    "text_b_vec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*200), text_b_unigram_array)))\n",
    "text_b_vec_norm = normalize(text_b_vec)\n",
    "\n",
    "print(\"text_b_vec type: %s\" % type(text_b_vec))\n",
    "print(\"text_b_vec:\" +  str(text_b_vec.shape))\n",
    "print()\n",
    "print(\"text_b_vec_norm type: %s\" % type(text_b_vec_norm))\n",
    "print(\"text_b_vec_norm shape:\" + str(text_b_vec_norm.shape))\n",
    "\n",
    "with open(\"text_b_BioWordVec.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(text_b_vec_norm, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between title & abstract word2vec features\n",
    "simVec_BioWordVec = np.asarray(list(map(cosine_sim, text_a_vec_norm, text_b_vec_norm)))[:, np.newaxis]\n",
    "print(type(simVec_BioWordVec))\n",
    "print(simVec_BioWordVec.shape)\n",
    "print(\"simVec_BioWordVec num dimensions:\" + str(simVec_BioWordVec.ndim))\n",
    "print(simVec_BioWordVec[0:2])\n",
    "\n",
    "with open(\"sim_BioWordVec.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(simVec_BioWordVec, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate polarity score of each sentance in a Headline observation and return the average\n",
    "\n",
    "sid = SentimentIntensityAnalyzer() # https://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence) # https://www.nltk.org/howto/sentiment.html\n",
    "        result.append(ss)\n",
    "    return pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_a_senti\"] = data['text_a'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "data = pd.concat([data, data['text_a_senti'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "data.rename(columns={'compound':'T_a_compound', 'neg':'T_a_neg', 'neu':'T_a_neu', 'pos':'T_a_pos'}, inplace=True)\n",
    "\n",
    "text_a_Senti = data[['T_a_compound','T_a_neg','T_a_neu','T_a_pos']].values\n",
    "print ('text_a_Senti shape:' + str(text_a_Senti.shape))\n",
    "print()\n",
    "\n",
    "# save title sentiment\n",
    "with open(\"text_a_sentiment.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(text_a_Senti, outfile, -1)\n",
    "    \n",
    "\n",
    "\n",
    "data[\"text_b_senti\"] = data['text_b'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "data = pd.concat([data, data['text_b_senti'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "data.rename(columns={'compound':'t_b_compound', 'neg':'t_b_neg', 'neu':'t_b_neu', 'pos':'t_b_pos'}, inplace=True)\n",
    "\n",
    "text_b_Senti = data[['t_b_compound','t_b_neg','t_b_neu','t_b_pos']].values\n",
    "print ('text_b_Senti shape:' + str(text_b_Senti.shape))\n",
    "\n",
    "# save abstract sentiment\n",
    "with open(\"text_b_sentiment.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(text_b_Senti, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine engineered features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"basic_count_features.pkl\", \"rb\") as infile:\n",
    "    #feat_names = pickle.load(infile)\n",
    "    basic_count_feats = pickle.load(infile)\n",
    "    \n",
    "with open (\"text_a_tfidf.pkl\", \"rb\") as infile_:\n",
    "    text_a_tfidf = pickle.load(infile_)\n",
    "    text_a_tfidf = text_a_tfidf.toarray()\n",
    "    \n",
    "with open (\"text_b_tfidf.pkl\", \"rb\") as outfile:\n",
    "    text_b_tfidf = pickle.load(outfile)\n",
    "    text_b_tfidf = text_b_tfidf.toarray()\n",
    "\n",
    "with open (\"sim_tfidf.pkl\", \"rb\") as outfile_:\n",
    "    sim_tfidf = pickle.load(outfile_)\n",
    "    \n",
    "with open (\"text_a_svd.pkl\", \"rb\") as svd_title:\n",
    "    text_a_svd = pickle.load(svd_title)\n",
    "    \n",
    "with open (\"text_b_svd.pkl\", \"rb\") as svd_abstract:\n",
    "    text_b_svd = pickle.load(svd_abstract)\n",
    "    \n",
    "with open (\"sim_svd.pkl\", \"rb\") as sim_svd:\n",
    "    sim_svd = pickle.load(sim_svd)\n",
    "    \n",
    "with open (\"text_a_BioWordVec.pkl\", \"rb\") as Tw:\n",
    "    text_a_BioWordVec = pickle.load(Tw)\n",
    "    \n",
    "with open (\"text_b_BioWordVec.pkl\", \"rb\") as Aw:\n",
    "    text_b_BioWordVec = pickle.load(Aw)\n",
    "    \n",
    "with open (\"sim_BioWordVec.pkl\", \"rb\") as Sw:\n",
    "    sim_BioWordVec = pickle.load(Sw)\n",
    "    \n",
    "with open (\"text_a_sentiment.pkl\", \"rb\") as Ts:\n",
    "    text_a_sentiment = pickle.load(Ts)\n",
    "    \n",
    "with open (\"text_b_sentiment.pkl\", \"rb\") as As:\n",
    "    text_b_sentiment = pickle.load(As)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [basic_count_feats, sim_tfidf, \n",
    "           text_a_svd, text_b_svd, sim_svd, \n",
    "           text_a_BioWordVec, text_b_BioWordVec, sim_BioWordVec, \n",
    "           text_a_sentiment, text_b_sentiment]\n",
    "\n",
    "for vec in vectors:\n",
    "    print(vec.ndim)\n",
    "    print(vec.shape)\n",
    "    print(type(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contra_data = np. hstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contra_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"all_engineered_contra_data.pkl\", \"wb\") as all_data:\n",
    "    pickle.dump(contra_data, all_data, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjT1Ia/SRxjIzfxRX8XVX9",
   "machine_shape": "hm",
   "mount_file_id": "1IDod64rbBcCPclXd2CiIdvHocLIIB1Vv",
   "name": "Task_VT_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
