{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak-supervised learning models for relation extraction\n",
    "\n",
    "__Goal:__ Train weak-supervised models, get accuracy level\n",
    "\n",
    "__Method:__ Test of weak-supervised DL models:\n",
    "1. Train DL models on CORD-19 dataset\n",
    "2. Extract relations from papers that was used in test dataset\n",
    "3. Convert relations to BEL format\n",
    "4. Compare with relations from covid-19 dataset, calculate accuracy\n",
    "5. Run error analysis\n",
    "\n",
    "\n",
    "__Data:__ covid-19-kg dataset, [CORD-19 processed by CoronaWhy](https://console.cloud.google.com/storage/browser/coronawhy/NLPDatasets/)\n",
    "\n",
    "__Tools:__ [PyTorch](https://pytorch.org/), [OpenNRE](https://github.com/thunlp/OpenNRE), [Snorkel](https://www.snorkel.org/) [PyBEL](https://github.com/pybel/pybel)\n",
    "\n",
    "__Result:__ Trained weak-supervised models, accuracy of weak-supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 --version\n",
    "#!echo $PYTHONPATH\n",
    "# Update PYTHONPATH, by setting <USERNAME> below.  This is to ensure access to OpenNRE frameworks and models\n",
    "#!export PYTHONPATH=/home/<USERNAME>/local/lib/python:/home/<USERNAME>/OpenNRE:/usr/local/lib/python3.7/site-packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests pybel pandas requests indra pybel[jupyter] spacy pyyaml\n",
    "#python3 -m spacy download en-core-web-sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pybel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm # not sure why you need both\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from indra.processors import bel\n",
    "from indra.sources import bel\n",
    "from indra.util import batch_iter\n",
    "from indra.sources import indra_db_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "These are copied from other task-vt notebooks (Protein Co-Occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    raw_files = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        raw_files.append(file)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def get_all_files(dirname):\n",
    "    all_files = []\n",
    "    \n",
    "    filenames = os.listdir(dirname)\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        all_files.append(file)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_cat_vocab(cat):\n",
    "    df_cat = df[cat]\n",
    "    items = df_cat.dropna().tolist()\n",
    "\n",
    "    vocab_list = []\n",
    "\n",
    "    for element in items:\n",
    "        item = element.split(\",\")\n",
    "        for e in item:\n",
    "            vocab_list.append(e)\n",
    "    \n",
    "    c = collections.Counter()\n",
    "\n",
    "    for word in vocab_list:\n",
    "        c[word] += 1\n",
    "        \n",
    "    result_dic = dict(c)\n",
    "    \n",
    "    return result_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CORD-19 neural relation extraction model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Step 1 - Load and Pre-Process CORD-19 Annnotated Data\n",
    "This dataset is taken from: https://github.com/SciBiteLabs/CORD19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1342/1342 [00:03<00:00, 343.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#load all files\n",
    "# update rootpath to location of CORD19 data set\n",
    "root_path='/mount_disk/CORD19/annotated-CORD-19/1.4/CORD19'\n",
    "#path=os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "\n",
    "dirs = [\n",
    "    os.path.join(root_path, 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/')\n",
    "    #os.path.join(root_path, 'comm_use_subset/comm_use_subset/pdf_json/'),\n",
    "    #os.path.join(root_path, 'custom_license/custom_license/pdf_json/'),\n",
    "    #os.path.join(root_path, 'noncomm_use_subset/noncomm_use_subset/pdf_json/')\n",
    "]\n",
    "\n",
    "files_stack = []\n",
    "for dir_ in dirs:\n",
    "    files = get_all_files(dir_)\n",
    "    files_stack.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#build list of entities types\n",
    "# not sure if this blob of code is needed for neural relation extraction\n",
    "c = collections.Counter()\n",
    "\n",
    "cat_vocab = []\n",
    "\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        for block in file['body_text']:\n",
    "            dict_file = block['termite_hits'].keys()\n",
    "            for key in dict_file:\n",
    "                cat_vocab.append(key)\n",
    "\n",
    "for word in cat_vocab:\n",
    "    c[word] += 1\n",
    "   \n",
    "vocab_list = (set(list(c.elements())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>block_id</th>\n",
       "      <th>INDICATION</th>\n",
       "      <th>CVPROT</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>HPO</th>\n",
       "      <th>GENE</th>\n",
       "      <th>SPECIES</th>\n",
       "      <th>GOONTOL</th>\n",
       "      <th>SARSCOV</th>\n",
       "      <th>DRUG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96ef1767754a53f792951ba1752440ae94e90c60</td>\n",
       "      <td>abstract_0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Lymphocytic choriomeningitis virus,Viruses</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96ef1767754a53f792951ba1752440ae94e90c60</td>\n",
       "      <td>body_text_1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>interferon gamma</td>\n",
       "      <td>Viruses</td>\n",
       "      <td>peptide binding,MHC protein binding,T cell rec...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50a217a2dacfe1364383ec8c681f64f2fd76dbe7</td>\n",
       "      <td>abstract_0</td>\n",
       "      <td>Coronavirus Infections</td>\n",
       "      <td>None</td>\n",
       "      <td>Korea, Republic of,Italy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50a217a2dacfe1364383ec8c681f64f2fd76dbe7</td>\n",
       "      <td>abstract_1</td>\n",
       "      <td>Coronavirus Infections</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50a217a2dacfe1364383ec8c681f64f2fd76dbe7</td>\n",
       "      <td>abstract_2</td>\n",
       "      <td>Coronavirus Infections</td>\n",
       "      <td>None</td>\n",
       "      <td>Korea, Republic of,China,Italy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id     block_id  \\\n",
       "0  96ef1767754a53f792951ba1752440ae94e90c60   abstract_0   \n",
       "1  96ef1767754a53f792951ba1752440ae94e90c60  body_text_1   \n",
       "2  50a217a2dacfe1364383ec8c681f64f2fd76dbe7   abstract_0   \n",
       "3  50a217a2dacfe1364383ec8c681f64f2fd76dbe7   abstract_1   \n",
       "4  50a217a2dacfe1364383ec8c681f64f2fd76dbe7   abstract_2   \n",
       "\n",
       "               INDICATION CVPROT                         COUNTRY   HPO  \\\n",
       "0                    None   None                            None  None   \n",
       "1                    None   None                            None  None   \n",
       "2  Coronavirus Infections   None        Korea, Republic of,Italy  None   \n",
       "3  Coronavirus Infections   None                            None  None   \n",
       "4  Coronavirus Infections   None  Korea, Republic of,China,Italy  None   \n",
       "\n",
       "               GENE                                     SPECIES  \\\n",
       "0              None  Lymphocytic choriomeningitis virus,Viruses   \n",
       "1  interferon gamma                                     Viruses   \n",
       "2              None                                        None   \n",
       "3              None                                        None   \n",
       "4              None                                        None   \n",
       "\n",
       "                                             GOONTOL  \\\n",
       "0                                               None   \n",
       "1  peptide binding,MHC protein binding,T cell rec...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             SARSCOV  DRUG  \n",
       "0                                               None  None  \n",
       "1                                               None  None  \n",
       "2  Severe acute respiratory syndrome coronavirus ...  None  \n",
       "3  Severe acute respiratory syndrome coronavirus ...  None  \n",
       "4    Severe acute respiratory syndrome coronavirus 2  None  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build dataframe: entity mentions by blocks ignoring hint count\n",
    "features = []\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        paper_id = file['paper_id']\n",
    "        \n",
    "        i = 0\n",
    "        sections = ['abstract', 'body_text']\n",
    "        for section in sections:\n",
    "            for block in file[section]:\n",
    "\n",
    "                block_id = section + '_' + str(i)\n",
    "                \n",
    "                block_features = []\n",
    "                block_features.append(paper_id)\n",
    "                block_features.append(block_id)\n",
    "                \n",
    "                termite_hits = block['termite_hits']\n",
    "                \n",
    "                block_categories = termite_hits.keys()\n",
    "                block_categories = list(block_categories)\n",
    "                for cat in vocab_list:\n",
    "        \n",
    "                    if cat in block_categories:\n",
    "                        cat_entities = []\n",
    "                        for hit in termite_hits[cat]:\n",
    "                            entity = hit.get('name')\n",
    "                            if entity not in cat_entities:\n",
    "                                cat_entities.append(entity)\n",
    "                                \n",
    "                        cat_entities = \",\".join(cat_entities)\n",
    "\n",
    "                    else:\n",
    "                        cat_entities = None\n",
    "\n",
    "                    block_features.append(cat_entities)\n",
    "\n",
    "                features.append(block_features)\n",
    "                i += 1\n",
    "\n",
    "\n",
    "col_names = ['paper_id', 'block_id']\n",
    "for cat in vocab_list:\n",
    "    col_names.append(cat)\n",
    "df = pd.DataFrame(features, columns=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore block categories\n",
    "#block_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data that will be used for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file['body_text'][0]['text'] # ok - so each text is already in a block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Load Covid19kg - manually annotated kg to get labelling data\n",
    "\n",
    "This will be used for training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph pre-procesed by Charlie Hoyt: https://github.com/CoronaWhy/bel4corona/tree/master/data/covid19kg\n",
    "url = 'https://github.com/CoronaWhy/bel4corona/raw/master/data/covid19kg/covid19-fraunhofer-grounded.bel.nodelink.json'\n",
    "res = requests.get(url)\n",
    "pybel_graph = pybel.from_nodelink(res.json())\n",
    "\n",
    "# view graph in jupyter (not displaying)\n",
    "#jupyter.to_html(pybel_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pybel-tools\n",
    "import pybel_tools\n",
    "from pybel_tools import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict of key_val pairs \n",
    "edges=pybel_tools.summary.get_edge_relations(pybel_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore edges\n",
    "#for key,value in edges.items():\n",
    "#    print(key)\n",
    "#    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relation types\n",
    "relations_pybel=pybel.struct.summary.count_relations(pybel_graph)\n",
    "relations=[]\n",
    "for i in relations_pybel.keys():\n",
    "    relations.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed covid19 frauenhofer manual annotations\n",
    "We can actually use processed annotated data in the form of a dataframe for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/<USRENAME>/covid19_frauenhofer_annotations.csv' does not exist: b'/home/<USRENAME>/covid19_frauenhofer_annotations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-da15cedd5edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set the correct path location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpybel_pd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/<USRENAME>/covid19_frauenhofer_annotations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/<USRENAME>/covid19_frauenhofer_annotations.csv' does not exist: b'/home/<USRENAME>/covid19_frauenhofer_annotations.csv'"
     ]
    }
   ],
   "source": [
    "# set the correct path location\n",
    "pybel_pd=pd.read_csv('/home/<USRENAME>/covid19_frauenhofer_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>doi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>While blocking TPC2 activity by tetrandrine, a...</td>\n",
       "      <td>{'(+)-Tetrandrine': {'namespace': 'chebi', 'na...</td>\n",
       "      <td>negativeCorrelation</td>\n",
       "      <td>{'Tpcn2': {'namespace': 'mgi', 'name': 'Tpcn2'...</td>\n",
       "      <td>{'annotations': {}, 'citation': {'authors': ['...</td>\n",
       "      <td>32221306.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chemoinformatics searches yielded 15 approved ...</td>\n",
       "      <td>{'(S)-verapamil': {'namespace': 'chebi', 'name...</td>\n",
       "      <td>negativeCorrelation</td>\n",
       "      <td>{'hypertension': {'namespace': 'doid', 'name':...</td>\n",
       "      <td>{'annotations': {}, 'citation': {'db': 'DOI', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://doi.org/10.1101/2020.03.22.002386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence  \\\n",
       "0           0  While blocking TPC2 activity by tetrandrine, a...   \n",
       "1           1  Chemoinformatics searches yielded 15 approved ...   \n",
       "\n",
       "                                              source             relation  \\\n",
       "0  {'(+)-Tetrandrine': {'namespace': 'chebi', 'na...  negativeCorrelation   \n",
       "1  {'(S)-verapamil': {'namespace': 'chebi', 'name...  negativeCorrelation   \n",
       "\n",
       "                                              target  \\\n",
       "0  {'Tpcn2': {'namespace': 'mgi', 'name': 'Tpcn2'...   \n",
       "1  {'hypertension': {'namespace': 'doid', 'name':...   \n",
       "\n",
       "                                                link      pmc_id  \\\n",
       "0  {'annotations': {}, 'citation': {'authors': ['...  32221306.0   \n",
       "1  {'annotations': {}, 'citation': {'db': 'DOI', ...         NaN   \n",
       "\n",
       "                                      doi_id  \n",
       "0                                        NaN  \n",
       "1  https://doi.org/10.1101/2020.03.22.002386  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore head \n",
    "pybel_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 - Label CORD-19 data with snorkel for Weak Supervision\n",
    "Create label functions here to label data heuristically\n",
    "\n",
    "Can also look at distant supervision - that is using a knowledge base (e.g. covid19kg) to label new unseen examples in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labeling function.. \n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Train a Classifer with openNRE\n",
    "\n",
    "\n",
    "For training\n",
    "- need to have train,test, validate data (see examples in openNRE/pretrain/wiki80 for format)\n",
    "- need to use a LM for encoding e.g. BERT of glove \n",
    "- need to select model (e.g. CNN)\n",
    "\n",
    "See example of model training at:\n",
    "\n",
    "https://github.com/thunlp/OpenNRE/blob/master/example/train_wiki80_bert_softmax.py \n",
    "\n",
    "To do:\n",
    "(1) Get pybel dataset into format of openNRE training dataset (use wiki_train.txt as an example)\n",
    "\n",
    "(2) Decide which model (e.g. CNN) and sentence embedding LM (e.g. BERT) we will use to train the openNRE classifier. \n",
    "\n",
    "\n",
    "Note - start with simple model e.g. CNN, later, we can  try Few Shot Classifier with Meta-Learner, to deal with small amount of labelled data.\n",
    "\n",
    "https://www.aclweb.org/anthology/D18-1514.pdf\n",
    "\n",
    "https://github.com/ProKil/FewRel/blob/master/train_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [2020-06-13 12:24:27] transformers.file_utils - PyTorch version 1.3.0 available.\n"
     ]
    }
   ],
   "source": [
    "import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [2020-06-13 12:26:07] root - Initializing word embedding with word2vec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('father', 0.7500484585762024)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example - check example works\n",
    "model = opennre.get_model('wiki80_cnn_softmax')\n",
    "model.infer({'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-617b5176fd1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_train.txt'"
     ]
    }
   ],
   "source": [
    "# Exploration of OpenNRE\n",
    "# Explore wiki80 training data (to get an idea of training data format)\n",
    "# Load the file\n",
    "path='/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_train.txt'\n",
    "f = open(path)\n",
    "data = []\n",
    "for line in f.readlines():\n",
    "    line = line.rstrip()\n",
    "    if len(line) > 0:\n",
    "        data.append(eval(line))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'h', 't', 'relation']\n"
     ]
    }
   ],
   "source": [
    "# explore data\n",
    "# identify keys in the dictionary required for OpenNRE training data set \n",
    "type(data[0]) # data is a list of dicts \n",
    "keys=[]\n",
    "for key in data[0].keys():\n",
    "    keys.append(key)\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Explore data\n",
    "for value in data[0].values():\n",
    "    print(type(value))\n",
    "\n",
    "# token is a list of tokens\n",
    "# h,t is a dict, with keys name,id, pos\n",
    "# relation is a str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>h</th>\n",
       "      <th>t</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Merpati, flight, 106, departed, Jakarta, (, C...</td>\n",
       "      <td>{'name': 'tjq', 'id': 'Q1331049', 'pos': [16, ...</td>\n",
       "      <td>{'name': 'tanjung pandan', 'id': 'Q3056359', '...</td>\n",
       "      <td>place served by transport hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, name, was, at, one, point, changed, to, ...</td>\n",
       "      <td>{'name': 'east midlands airport', 'id': 'Q8977...</td>\n",
       "      <td>{'name': 'nottingham', 'id': 'Q41262', 'pos': ...</td>\n",
       "      <td>place served by transport hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[It, is, a, four, -, level, stack, interchange...</td>\n",
       "      <td>{'name': 'fort lauderdale-hollywood internatio...</td>\n",
       "      <td>{'name': 'fort lauderdale, florida', 'id': 'Q1...</td>\n",
       "      <td>place served by transport hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[It, is, the, main, alternate, of, Jinnah, Int...</td>\n",
       "      <td>{'name': 'jinnah international airport', 'id':...</td>\n",
       "      <td>{'name': 'karachi', 'id': 'Q8660', 'pos': [10,...</td>\n",
       "      <td>place served by transport hub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Nearby, airports, include, Akwa, Ibom, Airpor...</td>\n",
       "      <td>{'name': 'margaret ekpo international airport'...</td>\n",
       "      <td>{'name': 'calabar', 'id': 'Q844091', 'pos': [1...</td>\n",
       "      <td>place served by transport hub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token  \\\n",
       "0  [Merpati, flight, 106, departed, Jakarta, (, C...   \n",
       "1  [The, name, was, at, one, point, changed, to, ...   \n",
       "2  [It, is, a, four, -, level, stack, interchange...   \n",
       "3  [It, is, the, main, alternate, of, Jinnah, Int...   \n",
       "4  [Nearby, airports, include, Akwa, Ibom, Airpor...   \n",
       "\n",
       "                                                   h  \\\n",
       "0  {'name': 'tjq', 'id': 'Q1331049', 'pos': [16, ...   \n",
       "1  {'name': 'east midlands airport', 'id': 'Q8977...   \n",
       "2  {'name': 'fort lauderdale-hollywood internatio...   \n",
       "3  {'name': 'jinnah international airport', 'id':...   \n",
       "4  {'name': 'margaret ekpo international airport'...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  {'name': 'tanjung pandan', 'id': 'Q3056359', '...   \n",
       "1  {'name': 'nottingham', 'id': 'Q41262', 'pos': ...   \n",
       "2  {'name': 'fort lauderdale, florida', 'id': 'Q1...   \n",
       "3  {'name': 'karachi', 'id': 'Q8660', 'pos': [10,...   \n",
       "4  {'name': 'calabar', 'id': 'Q844091', 'pos': [1...   \n",
       "\n",
       "                        relation  \n",
       "0  place served by transport hub  \n",
       "1  place served by transport hub  \n",
       "2  place served by transport hub  \n",
       "3  place served by transport hub  \n",
       "4  place served by transport hub  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore wiki_train_data as training_data\n",
    "wiki_data=pd.DataFrame(data)\n",
    "wiki_data.columns\n",
    "wiki_data.shape\n",
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pybel dataframe of relations to correct training data format for OpenNRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>target</th>\n",
       "      <th>link</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>doi_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>While blocking TPC2 activity by tetrandrine, a...</td>\n",
       "      <td>{'(+)-Tetrandrine': {'namespace': 'chebi', 'na...</td>\n",
       "      <td>negativeCorrelation</td>\n",
       "      <td>{'Tpcn2': {'namespace': 'mgi', 'name': 'Tpcn2'...</td>\n",
       "      <td>{'annotations': {}, 'citation': {'authors': ['...</td>\n",
       "      <td>32221306.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chemoinformatics searches yielded 15 approved ...</td>\n",
       "      <td>{'(S)-verapamil': {'namespace': 'chebi', 'name...</td>\n",
       "      <td>negativeCorrelation</td>\n",
       "      <td>{'hypertension': {'namespace': 'doid', 'name':...</td>\n",
       "      <td>{'annotations': {}, 'citation': {'db': 'DOI', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://doi.org/10.1101/2020.03.22.002386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thyroid stimulating hormone and free triiodoth...</td>\n",
       "      <td>{\"3,3',5'-triiodothyronine\": {'namespace': 'ch...</td>\n",
       "      <td>negativeCorrelation</td>\n",
       "      <td>{'COVID-19': {'namespace': 'doid', 'name': 'CO...</td>\n",
       "      <td>{'annotations': {'mesh': {'D044967': True}}, '...</td>\n",
       "      <td>32217556.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Based on these results, we performed virtual d...</td>\n",
       "      <td>{\"4'-epidoxorubicin\": {'namespace': 'chebi', '...</td>\n",
       "      <td>decreases</td>\n",
       "      <td>{'3.4.22.69': {'namespace': 'eccode', 'name': ...</td>\n",
       "      <td>{'annotations': {}, 'citation': {'authors': ['...</td>\n",
       "      <td>32173287.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Doctors can also use a clinically approved bil...</td>\n",
       "      <td>{'4-methylumbelliferone': {'namespace': 'chebi...</td>\n",
       "      <td>decreases</td>\n",
       "      <td>{'HAS2': {'namespace': 'hgnc', 'name': 'HAS2',...</td>\n",
       "      <td>{'annotations': {'mesh': {'D008168': True}}, '...</td>\n",
       "      <td>32205856.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence  \\\n",
       "0           0  While blocking TPC2 activity by tetrandrine, a...   \n",
       "1           1  Chemoinformatics searches yielded 15 approved ...   \n",
       "2           2  Thyroid stimulating hormone and free triiodoth...   \n",
       "3           3  Based on these results, we performed virtual d...   \n",
       "4           4  Doctors can also use a clinically approved bil...   \n",
       "\n",
       "                                              source             relation  \\\n",
       "0  {'(+)-Tetrandrine': {'namespace': 'chebi', 'na...  negativeCorrelation   \n",
       "1  {'(S)-verapamil': {'namespace': 'chebi', 'name...  negativeCorrelation   \n",
       "2  {\"3,3',5'-triiodothyronine\": {'namespace': 'ch...  negativeCorrelation   \n",
       "3  {\"4'-epidoxorubicin\": {'namespace': 'chebi', '...            decreases   \n",
       "4  {'4-methylumbelliferone': {'namespace': 'chebi...            decreases   \n",
       "\n",
       "                                              target  \\\n",
       "0  {'Tpcn2': {'namespace': 'mgi', 'name': 'Tpcn2'...   \n",
       "1  {'hypertension': {'namespace': 'doid', 'name':...   \n",
       "2  {'COVID-19': {'namespace': 'doid', 'name': 'CO...   \n",
       "3  {'3.4.22.69': {'namespace': 'eccode', 'name': ...   \n",
       "4  {'HAS2': {'namespace': 'hgnc', 'name': 'HAS2',...   \n",
       "\n",
       "                                                link      pmc_id  \\\n",
       "0  {'annotations': {}, 'citation': {'authors': ['...  32221306.0   \n",
       "1  {'annotations': {}, 'citation': {'db': 'DOI', ...         NaN   \n",
       "2  {'annotations': {'mesh': {'D044967': True}}, '...  32217556.0   \n",
       "3  {'annotations': {}, 'citation': {'authors': ['...  32173287.0   \n",
       "4  {'annotations': {'mesh': {'D008168': True}}, '...  32205856.0   \n",
       "\n",
       "                                      doi_id  \n",
       "0                                        NaN  \n",
       "1  https://doi.org/10.1101/2020.03.22.002386  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pybel_pd.shape # 5236 training points\n",
    "pybel_pd.head() # sentence, source, target, relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'(S)-verapamil': {'namespace': 'chebi', 'name': '(S)-verapamil', 'identifier': '77736'}}\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence and source\n",
    "pybel_pd['sentence'][1]\n",
    "pybel_pd['source'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm # consider scispacy\n",
    "# use spacy to process sentence in pybel_pd - to tokenize sentence and get details such as source/target name, and position in text \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While 0\n",
      "blocking 6\n",
      "TPC2 15\n",
      "activity 20\n",
      "by 29\n",
      "tetrandrine 32\n",
      ", 43\n",
      "an 45\n",
      "inhibitor 48\n",
      "for 58\n",
      "TPC237 62\n",
      ", 68\n",
      "decreased 70\n",
      "entry 80\n",
      "of 86\n",
      "SARS 89\n",
      "- 93\n",
      "CoV-2 94\n",
      "S 100\n",
      "pseudovirions 102\n",
      "( 116\n",
      "Fig 117\n",
      ". 120\n",
      "3f 122\n",
      ") 124\n",
      ", 125\n",
      "treatment 127\n",
      "of 137\n",
      "cells 140\n",
      "with 146\n",
      "130 151\n",
      ", 154\n",
      "a 156\n",
      "TRPML1 158\n",
      "inhibitor 165\n",
      ", 174\n",
      "had 176\n",
      "no 180\n",
      "effect 183\n",
      "( 190\n",
      "Supplementary 191\n",
      "Fig 205\n",
      ". 208\n",
      "1 210\n",
      ") 211\n",
      ", 212\n",
      "indicating 214\n",
      "that 225\n",
      "TPC2 230\n",
      ", 234\n",
      "not 236\n",
      "TRPML1 240\n",
      ", 246\n",
      "is 248\n",
      "important 251\n",
      "for 261\n",
      "SARS 265\n",
      "- 269\n",
      "CoV-2 270\n",
      "entry 276\n",
      ". 281\n",
      "{'(+)-Tetrandrine': {'namespace': 'chebi', 'name': '(+)-Tetrandrine', 'identifier': '49'}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3f44cf5e7d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mhead_namespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'namespace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mhead_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mhead_identifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'identifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mkeys_head_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# keys are token, h, t, relation\n",
    "# token is a list of str tokens (spacy sep?)\n",
    "# h, t are both dicts with elements keys name,id, pos\n",
    "# relation is a str of type \"relation\"\n",
    "\n",
    "pairs=[]\n",
    "#for i in range(len(pybel_pd)):\n",
    "for i in range(3):\n",
    "\n",
    "    row=pybel_pd.iloc[i]\n",
    "    sentence=row['sentence']\n",
    "    sentence_spacy=nlp(sentence)\n",
    "    token_value=[tok.text for tok in sentence_spacy]\n",
    "    for token in sentence_spacy:\n",
    "        print(token.text, token.idx)\n",
    "        \n",
    "    pairs.append(token_value) # first element in pairs list\n",
    "    \n",
    "    # now get head\n",
    "    # seem to be in 'chebi' namespace\n",
    "    head_value_dict=row['source']\n",
    "    print(head_value_dict)\n",
    "    \n",
    "    #json_acceptable_string = head_value_dict.replace(\"'\", \"\\\" \")\n",
    "    #print(json_acceptable_string)\n",
    "    #d = json.loads(json_acceptable_string) # json not working \n",
    "    \n",
    "    d = yaml.load(head_value_dict,Loader=yaml.FullLoader)\n",
    "    for value_dict in d.values():\n",
    "        head_namespace=value_dict['namespace']\n",
    "        head_name=value_dict['name']\n",
    "        head_identifier=dict['identifier']\n",
    "     \n",
    "    keys_head_dict= ['name', 'id', 'pos']\n",
    "    #vals_head_dict=[head_name_value, head_identifier, ]\n",
    "\n",
    "    #print(head_value_dict)\n",
    "    #print(d)\n",
    "    #\n",
    "    head_value_dict['namespace']\n",
    "    #dlist = [{k: v} for k, v in zip(keys, pairs)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(+)-Tetrandrine\n",
      "While\n",
      "(+)-Tetrandrine\n",
      "blocking\n",
      "(+)-Tetrandrine\n",
      "TPC2\n",
      "(+)-Tetrandrine\n",
      "activity\n",
      "(+)-Tetrandrine\n",
      "by\n",
      "(+)-Tetrandrine\n",
      "tetrandrine\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "an\n",
      "(+)-Tetrandrine\n",
      "inhibitor\n",
      "(+)-Tetrandrine\n",
      "for\n",
      "(+)-Tetrandrine\n",
      "TPC237\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "decreased\n",
      "(+)-Tetrandrine\n",
      "entry\n",
      "(+)-Tetrandrine\n",
      "of\n",
      "(+)-Tetrandrine\n",
      "SARS\n",
      "(+)-Tetrandrine\n",
      "-\n",
      "(+)-Tetrandrine\n",
      "CoV-2\n",
      "(+)-Tetrandrine\n",
      "S\n",
      "(+)-Tetrandrine\n",
      "pseudovirions\n",
      "(+)-Tetrandrine\n",
      "(\n",
      "(+)-Tetrandrine\n",
      "Fig\n",
      "(+)-Tetrandrine\n",
      ".\n",
      "(+)-Tetrandrine\n",
      "3f\n",
      "(+)-Tetrandrine\n",
      ")\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "treatment\n",
      "(+)-Tetrandrine\n",
      "of\n",
      "(+)-Tetrandrine\n",
      "cells\n",
      "(+)-Tetrandrine\n",
      "with\n",
      "(+)-Tetrandrine\n",
      "130\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "a\n",
      "(+)-Tetrandrine\n",
      "TRPML1\n",
      "(+)-Tetrandrine\n",
      "inhibitor\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "had\n",
      "(+)-Tetrandrine\n",
      "no\n",
      "(+)-Tetrandrine\n",
      "effect\n",
      "(+)-Tetrandrine\n",
      "(\n",
      "(+)-Tetrandrine\n",
      "Supplementary\n",
      "(+)-Tetrandrine\n",
      "Fig\n",
      "(+)-Tetrandrine\n",
      ".\n",
      "(+)-Tetrandrine\n",
      "1\n",
      "(+)-Tetrandrine\n",
      ")\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "indicating\n",
      "(+)-Tetrandrine\n",
      "that\n",
      "(+)-Tetrandrine\n",
      "TPC2\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "not\n",
      "(+)-Tetrandrine\n",
      "TRPML1\n",
      "(+)-Tetrandrine\n",
      ",\n",
      "(+)-Tetrandrine\n",
      "is\n",
      "(+)-Tetrandrine\n",
      "important\n",
      "(+)-Tetrandrine\n",
      "for\n",
      "(+)-Tetrandrine\n",
      "SARS\n",
      "(+)-Tetrandrine\n",
      "-\n",
      "(+)-Tetrandrine\n",
      "CoV-2\n",
      "(+)-Tetrandrine\n",
      "entry\n",
      "(+)-Tetrandrine\n",
      ".\n",
      "(+)-Tetrandrine\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-944b70f74a8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# how to get tok.text if it matches sentence id.. \n",
    "print(head_name)\n",
    "sentence_spacy\n",
    "for tok in sentence_spacy:\n",
    "    print(tok.text)\n",
    "    print(head_name)\n",
    "    if tok.text == head_name:\n",
    "        a = tok.idx \n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of OpenNRE model training\n",
    "Our code will look something like this with the frameowork, model and training data customised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bag_size BAG_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/lani_lichtenstein/.local/share/jupyter/runtime/kernel-c4d2179c-7d7a-4863-83f8-b81292d6e989.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lani_lichtenstein/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://github.com/thunlp/OpenNRE/blob/master/example/train_nyt10_pcnn_att.py\n",
    "    \n",
    "import sys, json, \n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import opennre\n",
    "from opennre import encoder, model, framework\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bag_size', type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Some basic settings\n",
    "root_path = '.'\n",
    "if not os.path.exists('ckpt'):\n",
    "    os.mkdir('ckpt')\n",
    "ckpt = 'ckpt/nyt10_pcnn_att.pth.tar'\n",
    "\n",
    "# Check data\n",
    "opennre.download('nyt10', root_path=root_path)\n",
    "opennre.download('glove', root_path=root_path)\n",
    "rel2id = json.load(open(os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "wordi2d = json.load(open(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_word2id.json')))\n",
    "word2vec = np.load(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_mat.npy'))\n",
    "\n",
    "# Define the sentence encoder\n",
    "sentence_encoder = opennre.encoder.PCNNEncoder(\n",
    "    token2id=wordi2d,\n",
    "    max_length=120,\n",
    "    word_size=50,\n",
    "    position_size=5,\n",
    "    hidden_size=230,\n",
    "    blank_padding=True,\n",
    "    kernel_size=3,\n",
    "    padding_size=1,\n",
    "    word2vec=word2vec,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = opennre.model.BagAttention(sentence_encoder, len(rel2id), rel2id)\n",
    "\n",
    "# Define the whole training framework\n",
    "framework = opennre.framework.BagRE(\n",
    "    train_path='benchmark/nyt10/nyt10_train.txt',\n",
    "    val_path='benchmark/nyt10/nyt10_val.txt',\n",
    "    test_path='benchmark/nyt10/nyt10_test.txt',\n",
    "    model=model,\n",
    "    ckpt=ckpt,\n",
    "    batch_size=160,\n",
    "    max_epoch=60,\n",
    "    lr=0.5,\n",
    "    weight_decay=0,\n",
    "    opt='sgd',\n",
    "    bag_size=args.bag_size)\n",
    "\n",
    "# Train the model\n",
    "framework.train_model()\n",
    "\n",
    "# Test the model\n",
    "framework.load_state_dict(torch.load(ckpt)['state_dict'])\n",
    "result = framework.eval_model(framework.test_loader)\n",
    "\n",
    "# Print the result\n",
    "print('AUC on test set: {}'.format(result['auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
