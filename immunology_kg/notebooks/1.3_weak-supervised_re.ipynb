{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak-supervised learning models for relation extraction\n",
    "\n",
    "__Goal:__ Train weak-supervised models, get accuracy level\n",
    "\n",
    "__Method:__ Test of weak-supervised DL models:\n",
    "1. Train DL models on CORD-19 dataset\n",
    "2. Extract relations from papers that was used in test dataset\n",
    "3. Convert relations to BEL format\n",
    "4. Compare with relations from covid-19 dataset, calculate accuracy\n",
    "5. Run error analysis\n",
    "\n",
    "\n",
    "__Data:__ covid-19-kg dataset, [CORD-19 processed by CoronaWhy](https://console.cloud.google.com/storage/browser/coronawhy/NLPDatasets/)\n",
    "\n",
    "__Tools:__ [PyTorch](https://pytorch.org/), [OpenNRE](https://github.com/thunlp/OpenNRE), [Snorkel](https://www.snorkel.org/) [PyBEL](https://github.com/pybel/pybel)\n",
    "\n",
    "__Result:__ Trained weak-supervised models, accuracy of weak-supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 --version\n",
    "#!echo $PYTHONPATH\n",
    "# Update PYTHONPATH, by setting <USERNAME> below.  This is to ensure access to OpenNRE frameworks and models\n",
    "#!export PYTHONPATH= /home/<USERNAME>/local/lib/python:/home/<USERNAME>/OpenNRE:/usr/local/lib/python3.7/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests pybel pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pybel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm # not sure why you need both\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "These are copied from other task-vt notebooks (Protein Co-Occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    raw_files = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        raw_files.append(file)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def get_all_files(dirname):\n",
    "    all_files = []\n",
    "    \n",
    "    filenames = os.listdir(dirname)\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        all_files.append(file)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_cat_vocab(cat):\n",
    "    df_cat = df[cat]\n",
    "    items = df_cat.dropna().tolist()\n",
    "\n",
    "    vocab_list = []\n",
    "\n",
    "    for element in items:\n",
    "        item = element.split(\",\")\n",
    "        for e in item:\n",
    "            vocab_list.append(e)\n",
    "    \n",
    "    c = collections.Counter()\n",
    "\n",
    "    for word in vocab_list:\n",
    "        c[word] += 1\n",
    "        \n",
    "    result_dic = dict(c)\n",
    "    \n",
    "    return result_dic\n",
    "\n",
    "#https://www.kaggle.com/rtatman/co-occurrence-matrix-plot-in-python\n",
    "def df_co_occurrance(df, strain_group):\n",
    "  strains_df = df.copy()  \n",
    "  for i in strain_group:\n",
    "        eval_match = df.SARS_COV.str.contains(i)\n",
    "        strains_df[i] = eval_match\n",
    "  return strains_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CORD-19 neural relation extraction model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Step 1 - Load and Pre-Process CORD-19 Annnotated Data\n",
    "This dataset is taken from: https://github.com/SciBiteLabs/CORD19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all files\n",
    "# update rootpath to location of CORD19 data set\n",
    "root_path='/mount_disk/CORD19/annotated-CORD-19/1.4/CORD19'\n",
    "#path=os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "\n",
    "dirs = [\n",
    "    os.path.join(root_path, 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/'),\n",
    "    os.path.join(root_path, 'comm_use_subset/comm_use_subset/pdf_json/'),\n",
    "    os.path.join(root_path, 'custom_license/custom_license/pdf_json/'),\n",
    "    os.path.join(root_path, 'noncomm_use_subset/noncomm_use_subset/pdf_json/')\n",
    "]\n",
    "\n",
    "files_stack = []\n",
    "for dir_ in dirs:\n",
    "    files = get_all_files(dir_)\n",
    "    files_stack.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of entities types\n",
    "# not sure if this blob of code is needed for neural relation extraction\n",
    "c = collections.Counter()\n",
    "\n",
    "cat_vocab = []\n",
    "\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        for block in file['body_text']:\n",
    "            dict_file = block['termite_hits'].keys()\n",
    "            for key in dict_file:\n",
    "                cat_vocab.append(key)\n",
    "\n",
    "for word in cat_vocab:\n",
    "    c[word] += 1\n",
    "   \n",
    "vocab_list = (set(list(c.elements())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build dataframe: entity mentions by blocks ignoring hint count\n",
    "features = []\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        paper_id = file['paper_id']\n",
    "        \n",
    "        i = 0\n",
    "        sections = ['abstract', 'body_text']\n",
    "        for section in sections:\n",
    "            for block in file[section]:\n",
    "\n",
    "                block_id = section + '_' + str(i)\n",
    "                \n",
    "                block_features = []\n",
    "                block_features.append(paper_id)\n",
    "                block_features.append(block_id)\n",
    "                \n",
    "                termite_hits = block['termite_hits']\n",
    "                \n",
    "                block_categories = termite_hits.keys()\n",
    "                block_categories = list(block_categories)\n",
    "                for cat in vocab_list:\n",
    "        \n",
    "                    if cat in block_categories:\n",
    "                        cat_entities = []\n",
    "                        for hit in termite_hits[cat]:\n",
    "                            entity = hit.get('name')\n",
    "                            if entity not in cat_entities:\n",
    "                                cat_entities.append(entity)\n",
    "                                \n",
    "                        cat_entities = \",\".join(cat_entities)\n",
    "\n",
    "                    else:\n",
    "                        cat_entities = None\n",
    "\n",
    "                    block_features.append(cat_entities)\n",
    "\n",
    "                features.append(block_features)\n",
    "                i += 1\n",
    "\n",
    "\n",
    "col_names = ['paper_id', 'block_id']\n",
    "for cat in vocab_list:\n",
    "    col_names.append(cat)\n",
    "df = pd.DataFrame(features, columns=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data that will be used for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['body_text'][1]['text'] # ok - so each text is already in a block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Label CORD-19 data with snorkel for Weak Supervision\n",
    "Create label functions here to label data heuristically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 - Train a Classifer with openNRE\n",
    "\n",
    "For training\n",
    "- need to have train,test, validate data (see examples in openNRE/pretrain/wiki80 for format)\n",
    "- need to use a LM for encoding e.g. BERT of glove \n",
    "- need to set up the code.. (see example below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - check example works\n",
    "model = opennre.get_model('wiki80_cnn_softmax')\n",
    "model.infer({'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of OpenNRE\n",
    "# Explore wiki80 training data (to get an idea of training data format)\n",
    "# Load the file\n",
    "path='/home/lani_lichtenstein/.opennre/benchmark/wiki80/wiki80_train.txt'\n",
    "f = open(path)\n",
    "data = []\n",
    "for line in f.readlines():\n",
    "    line = line.rstrip()\n",
    "    if len(line) > 0:\n",
    "        data.append(eval(line))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data\n",
    "type(data[0]) # data is a list of dicts \n",
    "data[0]['t'] # explore tail value of first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data=pd.DataFrame(data)\n",
    "wiki_data.columns\n",
    "wiki_data.shape\n",
    "wiki_data.iloc[0]\n",
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - use Bert as model, on our training data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/thunlp/OpenNRE/blob/master/example/train_nyt10_pcnn_att.py\n",
    "    \n",
    "import sys, json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import opennre\n",
    "from opennre import encoder, model, framework\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bag_size', type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Some basic settings\n",
    "root_path = '.'\n",
    "if not os.path.exists('ckpt'):\n",
    "    os.mkdir('ckpt')\n",
    "ckpt = 'ckpt/nyt10_pcnn_att.pth.tar'\n",
    "\n",
    "# Check data\n",
    "opennre.download('nyt10', root_path=root_path)\n",
    "opennre.download('glove', root_path=root_path)\n",
    "rel2id = json.load(open(os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "wordi2d = json.load(open(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_word2id.json')))\n",
    "word2vec = np.load(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_mat.npy'))\n",
    "\n",
    "# Define the sentence encoder\n",
    "sentence_encoder = opennre.encoder.PCNNEncoder(\n",
    "    token2id=wordi2d,\n",
    "    max_length=120,\n",
    "    word_size=50,\n",
    "    position_size=5,\n",
    "    hidden_size=230,\n",
    "    blank_padding=True,\n",
    "    kernel_size=3,\n",
    "    padding_size=1,\n",
    "    word2vec=word2vec,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = opennre.model.BagAttention(sentence_encoder, len(rel2id), rel2id)\n",
    "\n",
    "# Define the whole training framework\n",
    "framework = opennre.framework.BagRE(\n",
    "    train_path='benchmark/nyt10/nyt10_train.txt',\n",
    "    val_path='benchmark/nyt10/nyt10_val.txt',\n",
    "    test_path='benchmark/nyt10/nyt10_test.txt',\n",
    "    model=model,\n",
    "    ckpt=ckpt,\n",
    "    batch_size=160,\n",
    "    max_epoch=60,\n",
    "    lr=0.5,\n",
    "    weight_decay=0,\n",
    "    opt='sgd',\n",
    "    bag_size=args.bag_size)\n",
    "\n",
    "# Train the model\n",
    "framework.train_model()\n",
    "\n",
    "# Test the model\n",
    "framework.load_state_dict(torch.load(ckpt)['state_dict'])\n",
    "result = framework.eval_model(framework.test_loader)\n",
    "\n",
    "# Print the result\n",
    "print('AUC on test set: {}'.format(result['auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with relations from covid19kg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph pre-procesed by Charlie Hoyt: https://github.com/CoronaWhy/bel4corona/tree/master/data/covid19kg\n",
    "url = 'https://github.com/CoronaWhy/bel4corona/raw/master/data/covid19kg/covid19-fraunhofer-grounded.bel.nodelink.json'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
