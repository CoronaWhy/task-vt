{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak-supervised learning models for relation extraction\n",
    "\n",
    "__Goal:__ Train weak-supervised models, get accuracy level\n",
    "\n",
    "__Method:__ Test of weak-supervised DL models:\n",
    "1. Train DL models on CORD-19 dataset\n",
    "2. Extract relations from papers that was used in test dataset\n",
    "3. Convert relations to BEL format\n",
    "4. Compare with relations from covid-19 dataset, calculate accuracy\n",
    "5. Run error analysis\n",
    "\n",
    "\n",
    "__Data:__ covid-19-kg dataset, [CORD-19 processed by CoronaWhy](https://console.cloud.google.com/storage/browser/coronawhy/NLPDatasets/)\n",
    "\n",
    "__Tools:__ [PyTorch](https://pytorch.org/), [OpenNRE](https://github.com/thunlp/OpenNRE), [Snorkel](https://www.snorkel.org/) [PyBEL](https://github.com/pybel/pybel)\n",
    "\n",
    "__Result:__ Trained weak-supervised models, accuracy of weak-supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 --version\n",
    "#!echo $PYTHONPATH\n",
    "# Update PYTHONPATH, by setting <USERNAME> below.  This is to ensure access to OpenNRE frameworks and models\n",
    "# !export PYTHONPATH=/home/<USERNAME>/local/lib/python:/home/<USERNAME>/OpenNRE:/usr/local/lib/python3.7/site-packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests pybel pandas requests indra pybel[jupyter] pybel-tools spacy pyyaml\n",
    "#python3 -m spacy download en-core-web-sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pybel\n",
    "from pybel_tools import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm # not sure why you need both\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from indra.processors import bel\n",
    "from indra.sources import bel\n",
    "from indra.util import batch_iter\n",
    "from indra.sources import indra_db_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "These are copied from other task-vt notebooks (Protein Co-Occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    raw_files = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        raw_files.append(file)\n",
    "    \n",
    "    return raw_files\n",
    "\n",
    "\n",
    "def get_all_files(dirname):\n",
    "    all_files = []\n",
    "    \n",
    "    filenames = os.listdir(dirname)\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        filename = dirname + filename\n",
    "        file = json.load(open(filename, 'rb'))\n",
    "        all_files.append(file)\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "def get_cat_vocab(cat):\n",
    "    df_cat = df[cat]\n",
    "    items = df_cat.dropna().tolist()\n",
    "\n",
    "    vocab_list = []\n",
    "\n",
    "    for element in items:\n",
    "        item = element.split(\",\")\n",
    "        for e in item:\n",
    "            vocab_list.append(e)\n",
    "    \n",
    "    c = collections.Counter()\n",
    "\n",
    "    for word in vocab_list:\n",
    "        c[word] += 1\n",
    "        \n",
    "    result_dic = dict(c)\n",
    "    \n",
    "    return result_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CORD-19 neural relation extraction model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Step 1 - Load and Pre-Process CORD-19 Annnotated Data\n",
    "This dataset is taken from: https://github.com/SciBiteLabs/CORD19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all files\n",
    "# update rootpath to location of CORD19 data set\n",
    "root_path='/mount_disk/CORD19/annotated-CORD-19/1.4/CORD19'\n",
    "#path=os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "\n",
    "# start with small dataset (biorxiv_medrxiv)\n",
    "# uncomment to include other datasets\n",
    "dirs = [\n",
    "    os.path.join(root_path, 'biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/')\n",
    "    #os.path.join(root_path, 'comm_use_subset/comm_use_subset/pdf_json/'),\n",
    "    #os.path.join(root_path, 'custom_license/custom_license/pdf_json/'),\n",
    "    #os.path.join(root_path, 'noncomm_use_subset/noncomm_use_subset/pdf_json/')\n",
    "]\n",
    "\n",
    "files_stack = []\n",
    "for dir_ in dirs:\n",
    "    files = get_all_files(dir_)\n",
    "    files_stack.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of entities types\n",
    "# not sure if this blob of code is needed for neural relation extraction\n",
    "c = collections.Counter()\n",
    "\n",
    "cat_vocab = []\n",
    "\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        for block in file['body_text']:\n",
    "            dict_file = block['termite_hits'].keys()\n",
    "            for key in dict_file:\n",
    "                cat_vocab.append(key)\n",
    "\n",
    "for word in cat_vocab:\n",
    "    c[word] += 1\n",
    "   \n",
    "vocab_list = (set(list(c.elements())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list # these are entity types in CORD-19 according to ? pipeline???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build dataframe: entity mentions by blocks ignoring hint count\n",
    "features = []\n",
    "for files in tqdm(files_stack):\n",
    "    for file in files:\n",
    "        paper_id = file['paper_id']\n",
    "        \n",
    "        i = 0\n",
    "        sections = ['abstract', 'body_text']\n",
    "        for section in sections:\n",
    "            for block in file[section]:\n",
    "\n",
    "                block_id = section + '_' + str(i)\n",
    "                \n",
    "                block_features = []\n",
    "                block_features.append(paper_id)\n",
    "                block_features.append(block_id)\n",
    "                \n",
    "                termite_hits = block['termite_hits']\n",
    "                \n",
    "                block_categories = termite_hits.keys()\n",
    "                block_categories = list(block_categories)\n",
    "                for cat in vocab_list:\n",
    "        \n",
    "                    if cat in block_categories:\n",
    "                        cat_entities = []\n",
    "                        for hit in termite_hits[cat]:\n",
    "                            entity = hit.get('name')\n",
    "                            if entity not in cat_entities:\n",
    "                                cat_entities.append(entity)\n",
    "                                \n",
    "                        cat_entities = \",\".join(cat_entities)\n",
    "\n",
    "                    else:\n",
    "                        cat_entities = None\n",
    "\n",
    "                    block_features.append(cat_entities)\n",
    "\n",
    "                features.append(block_features)\n",
    "                i += 1\n",
    "\n",
    "\n",
    "col_names = ['paper_id', 'block_id']\n",
    "for cat in vocab_list:\n",
    "    col_names.append(cat)\n",
    "df = pd.DataFrame(features, columns=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore text data (sentences) in CORD-19 \n",
    "\n",
    "These are the corpus sentences that will be used for entity and relation classification, and ultimately form the content of our immunology knowledge graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['body_text'][0]['text'] # ok - so each text is already in a block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Load Fraunhofer Covid19kg \n",
    "\n",
    "This is manually annotated kg by scientists at Fraunhofer Institue. This dataset will be used to generate training data which is used in our relation classifier.  Data is in BEL format, although biological namespaces used in this annoted graph are varied, e.g. chebi, MESH.  The 'pybel' library is used to manipulate the graph in python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load graph pre-procesed by Charlie Hoyt: https://github.com/CoronaWhy/bel4corona/tree/master/data/covid19kg\n",
    "url = 'https://github.com/CoronaWhy/bel4corona/raw/master/data/covid19kg/covid19-fraunhofer-grounded.bel.nodelink.json'\n",
    "res = requests.get(url)\n",
    "pybel_graph = pybel.from_nodelink(res.json())\n",
    "\n",
    "# view graph in jupyter (not displaying)\n",
    "#jupyter.to_html(pybel_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore edges\n",
    "# returns a dict of key_val pairs \n",
    "edges=pybel_tools.summary.get_edge_relations(pybel_graph)\n",
    "\n",
    "#for key,value in edges.items():\n",
    "#    print(key)\n",
    "#    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore annotations\n",
    "annotations=pybel_tools.summary.get_annotations(pybel_graph)\n",
    "\n",
    "# annotations # {'cl', 'doid', 'efo', 'mesh'} # are these sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore activities (?)\n",
    "activities=pybel_tools.summary.get_activities(pybel_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore entities \n",
    "# get the set of all entities appearing in a node\n",
    "pybel_entities=pybel.struct.summary.node_summary.iterate_entities(pybel_graph)\n",
    "#x=pybel_tools.summary.node_properties.Iterable()\n",
    "entities_list=[]\n",
    "for entity in pybel_entities:\n",
    "    entities_list.append(entity)\n",
    "\n",
    "# intilize a null list \n",
    "unique_ent_list = [] \n",
    "\n",
    "# traverse for all elements \n",
    "for x in entities_list: \n",
    "    # check if exists in unique_list or not \n",
    "    if x not in unique_ent_list: \n",
    "        unique_ent_list.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_ent_list) # 2236 unique entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list[0:5] # explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore nodes that are incorporate in edges \n",
    "from pybel.dsl import BaseConcept\n",
    "\n",
    "for u,v,data in pybel_graph.edges(data=True):\n",
    "    \n",
    "    node_entities=pybel.struct.summary.node_summary.iterate_node_entities(u)\n",
    "    \n",
    "    if isinstance(u, BaseConcept):\n",
    "        entity = u.entity\n",
    "        print(entity)\n",
    "        print(u.name)\n",
    "        print(u.obo)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore keys that are in each entity node in relation graph\n",
    "# We will want to extract this info into the knowledge graph\n",
    "for key in u.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of relation types\n",
    "relations_pybel=pybel.struct.summary.count_relations(pybel_graph)\n",
    "relations=[]\n",
    "for i in relations_pybel.keys():\n",
    "    relations.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations # 15 relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces_pybel=pybel.struct.summary.count_namespaces(pybel_graph)\n",
    "len(namespaces_pybel) # 17 unique namespaces\n",
    "namespaces_pybel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns nothing\n",
    "#naked_names=pybel.struct.summary.count_naked_names(pybel_graph)\n",
    "#naked_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed covid19 frauenhofer manual annotations\n",
    "\n",
    "WE can load the pybel graph AFTER it has been processed by our NER pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the correct path location\n",
    "# edit path to where your version of the csv file is\n",
    "#pybel_pd=pd.read_csv('/home/<USERNAME>/covid19_frauenhofer_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore head \n",
    "pybel_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 - Label CORD-19 data with snorkel for Weak Supervision\n",
    "Create label functions here to label data heuristically\n",
    "\n",
    "Can also look at distant supervision - that is using a knowledge base (e.g. covid19kg) to label new unseen examples in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labeling function.. \n",
    "# This is an EXAMPLE of what a labelling function might look like\n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Train a Classifer with openNRE\n",
    "\n",
    "\n",
    "For training\n",
    "- need to have train,test, validate data (see examples in openNRE/pretrain/wiki80 for format)\n",
    "- need to use a LM for encoding e.g. BERT of glove \n",
    "- need to select model (e.g. CNN)\n",
    "\n",
    "See example of model training at:\n",
    "\n",
    "https://github.com/thunlp/OpenNRE/blob/master/example/train_wiki80_bert_softmax.py \n",
    "\n",
    "To do:\n",
    "\n",
    "(1) Get pybel dataset into format of openNRE training dataset (use wiki_train.txt as an example)\n",
    "\n",
    "(2) Decide which model (e.g. CNN) and sentence embedding LM (e.g. BERT) we will use to train the openNRE classifier. \n",
    "\n",
    "\n",
    "Note - start with simple model e.g. CNN, later, we can  try Few Shot Classifier with Meta-Learner, to deal with small amount of labelled data.\n",
    "\n",
    "https://www.aclweb.org/anthology/D18-1514.pdf\n",
    "\n",
    "https://github.com/ProKil/FewRel/blob/master/train_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - check example works\n",
    "model = opennre.get_model('wiki80_cnn_softmax')\n",
    "model.infer({'text': 'He was the son of Máel Dúin mac Máele Fithrich, and grandson of the high king Áed Uaridnach (died 612).', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of OpenNRE\n",
    "# Explore wiki80 training data (to get an idea of training data format)\n",
    "# Load the file, editing the path to the wiki data\n",
    "path='/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_train.txt'\n",
    "f = open(path)\n",
    "data = []\n",
    "for line in f.readlines():\n",
    "    line = line.rstrip()\n",
    "    if len(line) > 0:\n",
    "        data.append(eval(line))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/<USERNAME>/.opennre/benchmark/wiki80/wiki80_rel2id.json') as f:\n",
    "    rel2id = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data\n",
    "# identify keys in the dictionary required for OpenNRE training data set \n",
    "type(data[0]) # data is a list of dicts \n",
    "keys=[]\n",
    "for key in data[0].keys():\n",
    "    keys.append(key)\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "for value in data[0].values():\n",
    "    print(type(value))\n",
    "\n",
    "# token is a list of tokens\n",
    "# h,t is a dict, with keys name,id, pos\n",
    "# relation is a str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore wiki_train_data as training_data\n",
    "wiki_data=pd.DataFrame(data)\n",
    "wiki_data.columns\n",
    "wiki_data.shape\n",
    "wiki_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data['h'][2] # entitys have a name, and an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key,val in rel2id.items():\n",
    "#    print(key)\n",
    "#    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pybel dataframe of relations to correct training data format for OpenNRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybel_pd.shape # 5236 training points\n",
    "pybel_pd.head() # sentence, source, target, relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and source\n",
    "pybel_pd['sentence'][1]\n",
    "pybel_pd['source'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_sm # consider scispacy\n",
    "# use spacy to process sentence in pybel_pd - to tokenize sentence and get details such as source/target name, and position in text \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEEDS TO BE COMPLETED ONCE ENTITY  GROUNDING IS COMPLETED \n",
    "\n",
    "# keys are token, h, t, relation\n",
    "# token is a list of str tokens (spacy sep?)\n",
    "# h, t are both dicts with elements keys name,id, pos\n",
    "# relation is a str of type \"relation\"\n",
    "\n",
    "pairs=[]\n",
    "#for i in range(len(pybel_pd)):\n",
    "for i in range(3):\n",
    "\n",
    "    row=pybel_pd.iloc[i]\n",
    "    sentence=row['sentence']\n",
    "    sentence_spacy=nlp(sentence)\n",
    "    token_value=[tok.text for tok in sentence_spacy]\n",
    "    for token in sentence_spacy:\n",
    "        print(token.text, token.idx)\n",
    "        \n",
    "    pairs.append(token_value) # first element in pairs list\n",
    "    \n",
    "    # now get head\n",
    "    # seem to be in 'chebi' namespace\n",
    "    head_value_dict=row['source']\n",
    "    print(head_value_dict)\n",
    "    \n",
    "    #json_acceptable_string = head_value_dict.replace(\"'\", \"\\\" \")\n",
    "    #print(json_acceptable_string)\n",
    "    #d = json.loads(json_acceptable_string) # json not working \n",
    "    \n",
    "    d = yaml.load(head_value_dict,Loader=yaml.FullLoader)\n",
    "    for value_dict in d.values():\n",
    "        head_namespace=value_dict['namespace']\n",
    "        head_name=value_dict['name']\n",
    "        head_identifier=dict['identifier']\n",
    "     \n",
    "    keys_head_dict= ['name', 'id', 'pos']\n",
    "    #vals_head_dict=[head_name_value, head_identifier, ]\n",
    "\n",
    "    #print(head_value_dict)\n",
    "    #print(d)\n",
    "    #\n",
    "    head_value_dict['namespace']\n",
    "    #dlist = [{k: v} for k, v in zip(keys, pairs)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get tok.text if it matches sentence id.. \n",
    "print(head_name)\n",
    "sentence_spacy\n",
    "for tok in sentence_spacy:\n",
    "    print(tok.text)\n",
    "    print(head_name)\n",
    "    if tok.text == head_name:\n",
    "        a = tok.idx \n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of OpenNRE model training\n",
    "Our code will look something like this with the frameowork, model and training data customised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/thunlp/OpenNRE/blob/master/example/train_nyt10_pcnn_att.py\n",
    "    \n",
    "import sys, json, \n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import opennre\n",
    "from opennre import encoder, model, framework\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--bag_size', type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Some basic settings\n",
    "root_path = '.'\n",
    "if not os.path.exists('ckpt'):\n",
    "    os.mkdir('ckpt')\n",
    "ckpt = 'ckpt/nyt10_pcnn_att.pth.tar'\n",
    "\n",
    "# Check data\n",
    "opennre.download('nyt10', root_path=root_path)\n",
    "opennre.download('glove', root_path=root_path)\n",
    "rel2id = json.load(open(os.path.join(root_path, 'benchmark/nyt10/nyt10_rel2id.json')))\n",
    "wordi2d = json.load(open(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_word2id.json')))\n",
    "word2vec = np.load(os.path.join(root_path, 'pretrain/glove/glove.6B.50d_mat.npy'))\n",
    "\n",
    "# Define the sentence encoder\n",
    "sentence_encoder = opennre.encoder.PCNNEncoder(\n",
    "    token2id=wordi2d,\n",
    "    max_length=120,\n",
    "    word_size=50,\n",
    "    position_size=5,\n",
    "    hidden_size=230,\n",
    "    blank_padding=True,\n",
    "    kernel_size=3,\n",
    "    padding_size=1,\n",
    "    word2vec=word2vec,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = opennre.model.BagAttention(sentence_encoder, len(rel2id), rel2id)\n",
    "\n",
    "# Define the whole training framework\n",
    "framework = opennre.framework.BagRE(\n",
    "    train_path='benchmark/nyt10/nyt10_train.txt',\n",
    "    val_path='benchmark/nyt10/nyt10_val.txt',\n",
    "    test_path='benchmark/nyt10/nyt10_test.txt',\n",
    "    model=model,\n",
    "    ckpt=ckpt,\n",
    "    batch_size=160,\n",
    "    max_epoch=60,\n",
    "    lr=0.5,\n",
    "    weight_decay=0,\n",
    "    opt='sgd',\n",
    "    bag_size=args.bag_size)\n",
    "\n",
    "# Train the model\n",
    "framework.train_model()\n",
    "\n",
    "# Test the model\n",
    "framework.load_state_dict(torch.load(ckpt)['state_dict'])\n",
    "result = framework.eval_model(framework.test_loader)\n",
    "\n",
    "# Print the result\n",
    "print('AUC on test set: {}'.format(result['auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sPert\n",
    "\n",
    "This is an alternative frameowork and python package for RC to OpenNRE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore example training data\n",
    "ade_path='/home/lani_lichtenstein/spert/data/datasets/ade/ade_split_0_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade_types = json.load(open(ade_path), object_pairs_hook=OrderedDict)  # entity + relation types\n",
    "len(ade_types) # 427 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ade_types is a list\n",
    "# these tell us what is required for the training data\n",
    "for key in ade_types:\n",
    "    print(key) # tokens, entities, relations, orig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore an example\n",
    "for item in ade_types[10].items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade_types[0]['entities'] # only two entitiy types, adverse-effect, drug (from )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
