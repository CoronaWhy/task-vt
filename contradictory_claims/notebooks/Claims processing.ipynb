{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SuresMal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from itertools import combinations\n",
    "\n",
    "# import en_core_sci_lg\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pandas as pd  # noqa: E402\n",
    "import spacy  # noqa: E402\n",
    "from nltk import sent_tokenize  # noqa: E402\n",
    "from numba import jit  # noqa: E402\n",
    "# import scispacy  # noqa: F401\n",
    "from scispacy.abbreviation import AbbreviationDetector  # noqa: E402\n",
    "from scispacy.umls_linking import UmlsEntityLinker  # noqa: E402\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # noqa: E402\n",
    "# from spacy.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_papers_on_claim_presence(claims_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Separate papers with at least 1 claim from those with no claims.\n",
    "\n",
    "    :param claims_df: pandas dataframe of publication text, with a flag indicating claim presence\n",
    "    :return: Separate dataframes for claim text and text for papers with no claims\n",
    "    \"\"\"\n",
    "    no_claims_cord_uid = set(claims_df.loc[claims_df.claim_flag == 0, 'cord_uid'])\\\n",
    "        - set(claims_df.loc[claims_df.claim_flag == 1, 'cord_uid'])\n",
    "    claims_data = claims_df.loc[claims_df.claim_flag == 1, :].copy().reset_index(drop=True)\n",
    "    no_claims_data = claims_df.loc[claims_df.cord_uid.isin(no_claims_cord_uid), :]\\\n",
    "                              .copy().reset_index(drop=True)\n",
    "\n",
    "    return claims_data, no_claims_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_section_text(input_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Tokenize section text to sentences.\n",
    "\n",
    "    :param input_data: pandas dataframe with publication text\n",
    "    :retunr: Dataframe with section text tokenized to sentences\n",
    "    \"\"\"\n",
    "    # Empty dictonary to store the tokenized text\n",
    "    text_dict = {}\n",
    "    # Dictionary iterator\n",
    "    k = 0\n",
    "\n",
    "    # Loop through the sections and tokenize text to sentences\n",
    "    for i, text in enumerate(input_data.text):\n",
    "        for sent in sent_tokenize(text):\n",
    "            text_dict[k] = {'cord_uid': input_data.cord_uid[i],\n",
    "                            'section': input_data.section[i],\n",
    "                            'text': input_data.text[i],\n",
    "                            'drug_terms_used': input_data.drug_terms_used[i],\n",
    "                            'claims': sent}\n",
    "            k = k + 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(text_dict, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nlp(virus_lex_path: str, scispacy_model_name: str = \"en_core_sci_lg\"):\n",
    "    \"\"\"\n",
    "    Initialize scispacy nlp object and virus terms to the vocabulary.\n",
    "\n",
    "    :param virus_lex_path: path to virus lexicon\n",
    "    :param scispacy_model_name: name of scispacy model to use for w2v vectors\n",
    "    :return: Scispacy nlp object\n",
    "    \"\"\"\n",
    "    # Load the scispacy large model\n",
    "    # nlp = en_core_sci_lg.load(disable='parser')\n",
    "    # I believe this should work, I wonder if it's not recommended for  memory reasons though in a v env like Travis...\n",
    "    nlp = spacy.load(scispacy_model_name, disable='parser')\n",
    "    # Enable umls entity detection and abbreviation detection\n",
    "    linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "    nlp.add_pipe(linker)\n",
    "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "    nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "    # Create a new vector to assign to the virus terms\n",
    "    new_vector = nlp(\"\"\"Positive-sense single‐stranded ribonucleic acid virus, subgenus \"\"\"\n",
    "                     \"\"\"sarbecovirus of the genus Betacoronavirus. \"\"\"\n",
    "                     \"\"\"Also known as severe acute respiratory syndrome coronavirus 2, \"\"\"\n",
    "                     \"\"\"also known by 2019 novel coronavirus. It is \"\"\"\n",
    "                     \"\"\"contagious in humans and is the cause of the ongoing pandemic of \"\"\"\n",
    "                     \"\"\"coronavirus disease. Coronavirus disease 2019 is a zoonotic infectious \"\"\"\n",
    "                     \"\"\"disease.\"\"\").vector\n",
    "\n",
    "    # Add virus terms to the model vocabulary and assign to them the new vector created above\n",
    "    # vocab = Vocab()\n",
    "    virus_words = pd.read_csv(virus_lex_path, header=None)\n",
    "    for virus_word in virus_words[0]:\n",
    "        nlp.vocab.set_vector(virus_word, new_vector)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_similar_claims(claims_data: pd.DataFrame, nlp):\n",
    "    \"\"\"\n",
    "    Pair similar claims.\n",
    "\n",
    "    :param claims_data: pandas dataframe with cord 19 claims\n",
    "    :param nlp: Scispacy nlp object\n",
    "    :return: Dataframe of paired claims\n",
    "    \"\"\"\n",
    "    print(\"Start pair func:\",datetime.datetime.now())\n",
    "    # Extract list of drug terms present across all claims\n",
    "    # Note: 'drug_terms_used' consists of drug terms present in the section in which the claim appears\n",
    "    drug_terms = []\n",
    "    for drugs in claims_data.drug_terms_used:\n",
    "        drug_terms = drug_terms + str(drugs).split(',')\n",
    "    drug_terms = list(set(drug_terms))\n",
    "    drug_terms.append('acei/arb')\n",
    "\n",
    "    # Filter to claims that contain drug terms\n",
    "    sentences_to_keep = [any(True for d in drug_terms if d in c) for c in claims_data.claims]\n",
    "    claims_data = claims_data[sentences_to_keep].reset_index(drop=True)\n",
    "    # Add a new column for storing the drug terms present in each claim\n",
    "    claims_data['drug_terms_mention'] = [[d for d in drug_terms if d in c] for c in claims_data.claims]\n",
    "    \n",
    "    print(\"Start pair claims:\",datetime.datetime.now())\n",
    "    drug_terms_mentions_flat = [d for d_list in claims_data['drug_terms_mention'] for d in d_list]\n",
    "    drug_terms_mentions_flat = list(set(drug_terms_mentions_flat))\n",
    "    \n",
    "    paper_pairs_filt = []\n",
    "    # Loop through drugs and filter to claims that mention the drug term\n",
    "    for d in drug_terms_mentions_flat:\n",
    "        claims_with_drug_index = [d in d_list for d_list in claims_data.drug_terms_mention]\n",
    "        claims_with_drug = claims_data[claims_with_drug_index]\n",
    "         # Pair all claims with the same drug mention\n",
    "        paper_pairs = list(combinations(claims_with_drug.index, 2))\n",
    "        # Filter to claim pairs that come from different papers\n",
    "        for i, j in paper_pairs:\n",
    "            if claims_with_drug.cord_uid[i] != claims_with_drug.cord_uid[j]:\n",
    "                #if any(d1 in claims_data.drug_terms_mention[i] for d1 in claims_data.drug_terms_mention[j]):\n",
    "                paper_pairs_filt.append((i, j))\n",
    "\n",
    "    print(\"Start vector calc:\",datetime.datetime.now())\n",
    "    # Calculate scispacy vector for each claim\n",
    "    claims_data['w2vVector'] = [nlp(c).vector.reshape(1, -1) for c in claims_data.claims]\n",
    "\n",
    "    # Empty dictonary to store the similar claim pairs\n",
    "    claim_pairs_dict = {}\n",
    "    # Dictionary iterator\n",
    "    k = 0\n",
    "\n",
    "    # Initialize just-in-time compiler for efficient parallel processing\n",
    "    jit(nopython=True, parallel=True)\n",
    "\n",
    "    print(\"Start similarity calc:\",datetime.datetime.now())\n",
    "    # For each pair of claims, calculate cosine similarity between the respective scispacy vectors\n",
    "    # and keep only those pairs with at least 50% similarity\n",
    "    for i, j in paper_pairs_filt:\n",
    "        cos_sim = cosine_similarity(claims_data.w2vVector[i], claims_data.w2vVector[j])[0][0]\n",
    "        #if cos_sim >= 0.5:\n",
    "        claim_pairs_dict[k] = {'paper1_cord_uid': claims_data.cord_uid[i],\n",
    "                               'paper2_cord_uid': claims_data.cord_uid[j],\n",
    "                               'text1': claims_data.claims[i],\n",
    "                               'text2': claims_data.claims[j],\n",
    "                               'similarity_score': cos_sim,\n",
    "                               'drugs1': claims_data.drug_terms_mention[i],\n",
    "                               'drugs2': claims_data.drug_terms_mention[j]}\n",
    "        k = k + 1\n",
    "    print(\"End pair func:\",datetime.datetime.now())\n",
    "\n",
    "    return pd.DataFrame.from_dict(claim_pairs_dict, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cord_metadata(input_data, metadata_path):\n",
    "    \"\"\"\n",
    "    Add paper publish time and title metadata to the given cord claim pairs.\n",
    "\n",
    "    :param input_data: pandas dataframe with cord claim pairs\n",
    "    :param metadata: path to cord metadata.csv\n",
    "    :return: Merged dataframe\n",
    "    \"\"\"\n",
    "    # Read metadata\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    metadata = metadata[['cord_uid', 'publish_time', 'title']]\n",
    "\n",
    "    # Add title and publish time for first claim's paper\n",
    "    input_data = pd.merge(input_data, metadata, how='inner',\n",
    "                          left_on='paper1_cord_uid',\n",
    "                          right_on='cord_uid')\n",
    "    cols_rename = {'title': 'title1', 'publish_time': 'publish_time1'}\n",
    "    input_data.drop(columns='cord_uid', inplace=True)\n",
    "    input_data.rename(columns=cols_rename, inplace=True)\n",
    "\n",
    "    # Add title and publish time for second claim's paper\n",
    "    input_data = pd.merge(input_data, metadata, how='inner',\n",
    "                          left_on='paper2_cord_uid',\n",
    "                          right_on='cord_uid')\n",
    "    cols_rename = {'title': 'title2', 'publish_time': 'publish_time2'}\n",
    "    input_data.drop(columns='cord_uid', inplace=True)\n",
    "    input_data.rename(columns=cols_rename, inplace=True)\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = pd.read_csv('C:\\\\Users\\\\SuresMal\\\\Documents\\\\Coronawhy\\\\Contradictory Claims\\\\data\\\\cord19\\\\2020-08-15\\\\processed\\\\claims_section_text_with_drug_mentions_150820.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_data, no_claims_data = split_papers_on_claim_presence(claims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_claims_data = tokenize_section_text(no_claims_data)\n",
    "claims_data = claims_data.append(no_claims_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_sci_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-585b13be312a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\SuresMal\\\\Documents\\\\Coronawhy\\\\Contradictory Claims\\\\data\\\\virus_words.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-ad3ba29442e7>\u001b[0m in \u001b[0;36minitialize_nlp\u001b[1;34m(virus_lex_path, scispacy_model_name)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# nlp = en_core_sci_lg.load(disable='parser')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# I believe this should work, I wonder if it's not recommended for  memory reasons though in a v env like Travis...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscispacy_model_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Enable umls entity detection and abbreviation detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlinker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUmlsEntityLinker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolve_abbreviations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_sci_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = initialize_nlp('C:\\\\Users\\\\SuresMal\\\\Documents\\\\Coronawhy\\\\Contradictory Claims\\\\data\\\\virus_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_paired_df = pair_similar_claims(claims_data, nlp)\n",
    "\n",
    "# Add paper publish time and title info\n",
    "claims_paired_df = add_cord_metadata(claims_paired_df, metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claims_paired_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
