{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import libraries","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport ast\nimport string\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport spacy\nimport scispacy\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\n\nfrom nltk import sent_tokenize\n\nfrom itertools import combinations\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom numba import jit # parallel processing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"nlp = spacy.load(\"/opt/conda/lib/python3.7/site-packages/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4\")\nlinker = UmlsEntityLinker(resolve_abbreviations=True)\nnlp.add_pipe(linker)\nabbreviation_pipe = AbbreviationDetector(nlp)\nnlp.add_pipe(abbreviation_pipe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp(\"test for the drug remdesivir for sars-cov-2 treatment and HCQ\").ents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp(\"test for the drug azithromycin, lopinavir, remdesivir for sars-cov-2 treatment and HCQ\").ents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define i/o paths","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_data_path = '/kaggle/input/claims-flag-sentence-cord-uid-updated/'\ninput_data_file = 'claims_flag_sentence_cord_uid_updated.csv'\n\nw2v_data_path= '/kaggle/input/claims-flag-each-cord-uid/'\nw2v_data_file = 'claims_flag_each_cord_uid.csv'\n\nmetadata_file_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_file = 'metadata.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read and process data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata = pd.read_csv(metadata_file_path+metadata_file)\nmetadata = metadata[['cord_uid','publish_time','title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#input_data = pd.read_csv(input_data_path+input_data_file)\n#input_processed = pd.read_csv(input_data_path+input_data_file)\ninput_data = pd.read_csv(input_data_path+input_data_file)\nw2v_data = pd.read_csv(w2v_data_path+w2v_data_file)\n\ninput_data.cord_uid = input_data.cord_uid.str.lower().str.strip()\ninput_data.claims = input_data.claims.str.lower().str.strip()\nw2v_data.cord_uid = w2v_data.cord_uid.str.lower().str.strip()\nw2v_data.sentence = w2v_data.sentence.str.lower().str.strip()\n\ninput_processed_all = pd.merge(input_data,w2v_data[['cord_uid','sentence','w2vVector']],\\\n                            left_on=['cord_uid','claims'],right_on=['cord_uid','sentence'],how='left')\ninput_processed_all = input_processed_all.drop(columns='sentence_y')\ninput_processed_all.rename(columns={'sentence_x':'sentence'},inplace=True)\ninput_processed = input_processed_all.loc[~(input_processed_all.w2vVector.isnull()),:].reset_index()\nclaims_unmapped = input_processed_all.loc[(input_processed_all.w2vVector.isnull())\\\n                                          & input_processed_all.claims!='[]',:].reset_index()\n\nprint('No of unique sentences in claims:',input_data.loc[input_data.claims!='[]','claims'].nunique())\nprint('No of unique sentences in claims after join:',input_processed.claims.nunique())\nprint('No of rows in claims:',len(input_data.loc[input_data.claims!='[]',:]))\nprint('No of rows in claims after join:',len(input_processed))\nprint('No of rows in claims unmapped:',len(claims_unmapped))\nprint('\\n')\nprint('No of unique papers in claims:',input_data.loc[input_data.claims!='[]','cord_uid'].nunique())\nprint('No of unique papers in claims after join:',input_processed.cord_uid.nunique())\n\nprint('\\nPapers with no claims:',input_data.loc[input_data.claims=='[]','cord_uid'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For papers with no claims, tokenize to sentences and map to w2v vectors\nno_claims = input_data.loc[input_data.claims=='[]',:].reset_index()\n\ndict = {}\nk = 0\n\nfor i,text in enumerate(no_claims.sentence):\n    for sent in sent_tokenize(text):\n        dict[i] = {'cord_uid':no_claims.cord_uid[i],\\\n                   'section':no_claims.section[i],\\\n                   'sentence':no_claims.sentence[i],\\\n                   'drug_terms_used':no_claims.drug_terms_used[i],\\\n                   'claims':sent}\n        k = k + 1\n        \nno_claims = pd.DataFrame.from_dict(dict, \"index\")\n\nno_claims_all = pd.merge(no_claims,w2v_data[['cord_uid','sentence','w2vVector']],\\\n                            left_on=['cord_uid','claims'],right_on=['cord_uid','sentence'],how='left')\nno_claims_all = no_claims_all.drop(columns='sentence_y')\nno_claims_all.rename(columns={'sentence_x':'sentence'},inplace=True)\nno_claims_mapped = no_claims_all.loc[~(no_claims_all.w2vVector.isnull()),:].reset_index()\nno_claims_unmapped = no_claims_all.loc[no_claims_all.w2vVector.isnull(),:].reset_index()\n\nprint('No. of rows mapped to vectors:', len(no_claims_mapped))\nprint('No. of rows unmapped to vectors:', len(no_claims_unmapped))\nprint('Total rows:',len(no_claims_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keep only sentences containing at least 3 words other than those defined below\n#This also removes any sentences that do not contain any words at all\n\n# rep = {\"text\": \"\", \"cite_spans\": \"\", \"ref_spans\": \"\", \"section\": \"\", \"Abstract\": \"\",\\\n#        \"bioRxiv preprint\": \"\", \"medRxiv preprint\": \"\", \"doi:\": \"\"}\n# rep = dict((re.escape(k), v) for k, v in rep.items())\n# pattern = re.compile(\"|\".join(rep.keys()))\n# sentences_temp = [pattern.sub(lambda m: rep[re.escape(m.group(0))], s) for s in input_data.sentence]\n# pattern = re.compile(\".*[A-Za-z].*\")\n# sentences_to_keep = [(bool(re.search(pattern,s))) & (len(s.split(' '))>2) for s in sentences_temp]\n# input_processed = input_data.loc[sentences_to_keep,:]\n# sentences_to_drop = [not i for i in sentences_to_keep]\n# input_excluded = input_data.loc[sentences_to_drop,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert w2vVector column from string type to  list\ninput_processed.w2vVector = [re.sub(',+', ',', ','.join(w.replace('\\n','').split(' '))) for w in input_processed.w2vVector]\ninput_processed.w2vVector = [re.sub('\\[,', '', w) for w in input_processed.w2vVector]\ninput_processed.w2vVector = [re.sub(',\\]', '', w) for w in input_processed.w2vVector]\ninput_processed.w2vVector = [re.sub('\\[', '', w) for w in input_processed.w2vVector]\ninput_processed.w2vVector = [re.sub('\\]', '', w) for w in input_processed.w2vVector]\ninput_processed.w2vVector = input_processed.w2vVector.apply(lambda s: list(ast.literal_eval(s)))\n\nno_claims_mapped.w2vVector = [re.sub(',+', ',', ','.join(w.replace('\\n','').split(' '))) for w in no_claims_mapped.w2vVector]\nno_claims_mapped.w2vVector = [re.sub('\\[,', '', w) for w in no_claims_mapped.w2vVector]\nno_claims_mapped.w2vVector = [re.sub(',\\]', '', w) for w in no_claims_mapped.w2vVector]\nno_claims_mapped.w2vVector = [re.sub('\\[', '', w) for w in no_claims_mapped.w2vVector]\nno_claims_mapped.w2vVector = [re.sub('\\]', '', w) for w in no_claims_mapped.w2vVector]\nno_claims_mapped.w2vVector = no_claims_mapped.w2vVector.apply(lambda s: list(ast.literal_eval(s)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_processed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_claims_mapped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drug_terms = []\nfor drugs in input_data.drug_terms_used:\n    drug_terms = drug_terms + drugs.split(',')\ndrug_terms = list(set(drug_terms))\ndrug_terms.append('acei/arb')\nlen(drug_terms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#input_processed.to_csv('cord_drug_titles_abstracts_conclusions.csv')\n#input_excluded.to_csv('cord_drug_titles_abstracts_conclusions_excluded.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title_data = input_processed.loc[input_processed.section=='title',:]\n# abstract_data = input_processed.loc[input_processed.section=='abstract',:]\n# title_abstract_data = input_processed.loc[(input_processed.section=='title') | (input_processed.section=='abstract'),:]\n# conclusion_data = input_processed.loc[(input_processed.section!='title') & (input_processed.section!='abstract'),:]\n# claims_data = input_processed.loc[input_processed.claim_flag==1,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Number of papers:', input_processed.cord_uid.nunique())\n# print('Number of papers with title:', title_data.cord_uid.nunique())\n# print('Number of papers with abstract:', abstract_data.cord_uid.nunique())\n# print('Number of papers with conclusion:', conclusion_data.cord_uid.nunique())\n\n# print('\\n')\n\n# print('Number of papers with core claims:', claims_data.cord_uid.nunique())\n# print('Number of papers with title in core claims:', claims_data.loc[claims_data.section=='title','cord_uid'].nunique())\n# print('Number of papers with abstract in core claims:', claims_data.loc[claims_data.section=='abstract','cord_uid'].nunique())\n# print('Number of papers with conclusion in core claims:', claims_data.loc[(claims_data.section!='title') & (claims_data.section!='abstract'),'cord_uid'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Number of unique sentences under titles:', title_data.sentence.nunique())\n# print('Number of unique sentence ids under titles:', title_data.sentence_id.nunique())\n# print('Number of unique sentences under abstracts:', abstract_data.sentence.nunique())\n# print('Number of unique sentence ids under abstracts:', abstract_data.sentence_id.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Number of unique sentences under claims:', claims_data.sentence.nunique())\n# print('Number of unique sentence ids under claims:', claims_data.sentence_id.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate cosine similarity between titles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Average w2v vectors of all sentences falling under a single cord_uid\n# title_data_final = pd.DataFrame(columns = ['cord_uid','sentence','w2vVector','drugs'])\n# for cord_uid in title_data.cord_uid.unique():\n#     title = \" \".join(title_data.loc[title_data.cord_uid==cord_uid,'sentence'])\n#     drugs = \",\".join(title_data.loc[title_data.cord_uid==cord_uid,'drug_terms_used'])\n#     drugs = \",\".join(list(set(drugs.split(','))))\n#     w2vVector = np.mean(list(title_data.loc[title_data.cord_uid==cord_uid,'w2vVector']), axis=0)\n#     title_data_final = title_data_final.append({'cord_uid':cord_uid,\\\n#                                                 'sentence': title,\\\n#                                                 'w2vVector': w2vVector,\\\n#                                                 'drugs': drugs},\\\n#                                               ignore_index=True)\n# len(title_data_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pattern = re.compile(\".*hydroxychloroquine.*\")\n# sentences_to_keep = [(bool(re.search(pattern,s.lower()))) for s in title_data_final.sentence]\n# drug_title_data = title_data_final.loc[sentences_to_keep,:].reset_index(drop=True)\n# len(drug_title_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','title1','title2','similarity_score'])\n# jit(nopython=True, parallel=True)\n# for i,paper1 in enumerate(drug_title_data.sentence):\n#     for j,paper2 in enumerate(drug_title_data.sentence):\n#         if i!=j:\n#             cos_sim = cosine_similarity(drug_title_data.w2vVector[i].reshape(1,-1),drug_title_data.w2vVector[j].reshape(1,-1))[0][0]\n#             title_similarity = title_similarity.append({'paper1_cord_uid':drug_title_data.cord_uid[i],\\\n#                                                         'paper2_cord_uid':drug_title_data.cord_uid[j],\\\n#                                                         'title1':paper1,\\\n#                                                         'title2':paper2,\\\n#                                                         'similarity_score':cos_sim},\\\n#                                                ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','title1','title2','drugs1','drugs2','similarity_score'])\n# jit(nopython=True, parallel=True)\n# title_pairs = list(combinations(title_data_final.index,2))\n# for i,j in title_pairs:\n#     drugs1 = title_data_final.drugs[i].split(',')\n#     drugs2 = title_data_final.drugs[j].split(',')\n#     if any(d1 in drugs2 for d1 in drugs1):\n#         cos_sim = cosine_similarity(title_data_final.w2vVector[i].reshape(1,-1),title_data_final.w2vVector[j].reshape(1,-1))[0][0]\n#         title_similarity = title_similarity.append({'paper1_cord_uid':title_data_final.cord_uid[i],\\\n#                                                     'paper2_cord_uid':title_data_final.cord_uid[j],\\\n#                                                     'title1':title_data_final.sentence[i],\\\n#                                                     'title2':title_data_final.sentence[j],\\\n#                                                     'similarity_score':cos_sim,\\\n#                                                     'drugs1':title_data_final.drugs[i],\\\n#                                                     'drugs2':title_data_final.drugs[j]},\\\n#                                            ignore_index=True)\n\n# title_similarity.to_csv('drug_title_similarity.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate cosine similarity between title+abstracts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Average w2v vectors of all sentences falling under a single cord_uid\n# title_abstract_data_final = pd.DataFrame(columns = ['cord_uid','sentence','w2vVector','drugs'])\n# for cord_uid in title_abstract_data.cord_uid.unique():\n#     sentences = \" \".join(title_abstract_data.loc[title_abstract_data.cord_uid==cord_uid,'sentence'])\n#     drugs = \",\".join(title_abstract_data.loc[title_abstract_data.cord_uid==cord_uid,'drug_terms_used'])\n#     drugs = \",\".join(list(set(drugs.split(','))))\n#     w2vVector = np.mean(list(title_abstract_data.loc[title_abstract_data.cord_uid==cord_uid,'w2vVector']), axis=0)\n#     title_abstract_data_final = title_abstract_data_final.append({'cord_uid':cord_uid,\\\n#                                                                 'sentence': sentences,\\\n#                                                                 'w2vVector': w2vVector,\\\n#                                                                  'drugs': drugs},\\\n#                                                                ignore_index=True)\n# len(title_abstract_data_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title_abstract_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','text1','text2','drugs1','drugs2','similarity_score'])\n# jit(nopython=True, parallel=True)\n# paper_pairs = list(combinations(title_abstract_data_final.index,2))\n# for i,j in paper_pairs:\n#     drugs1 = title_abstract_data_final.drugs[i].split(',')\n#     drugs2 = title_abstract_data_final.drugs[j].split(',')\n#     if any(d1 in drugs2 for d1 in drugs1):\n#         cos_sim = cosine_similarity(title_abstract_data_final.w2vVector[i].reshape(1,-1),title_abstract_data_final.w2vVector[j].reshape(1,-1))[0][0]\n#         title_abstract_similarity = title_abstract_similarity.append({'paper1_cord_uid':title_abstract_data_final.cord_uid[i],\\\n#                                                     'paper2_cord_uid':title_abstract_data_final.cord_uid[j],\\\n#                                                     'text1':title_abstract_data_final.sentence[i],\\\n#                                                     'text2':title_abstract_data_final.sentence[j],\\\n#                                                     'similarity_score':cos_sim,\\\n#                                                     'drugs1':title_abstract_data_final.drugs[i],\\\n#                                                     'drugs2':title_abstract_data_final.drugs[j]},\\\n#                                            ignore_index=True)\n\n# title_abstract_similarity.to_csv('drug_title_abstract_similarity.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate cosine similarity between core claims (avg w2v across core claims per cord_uid)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Average w2v vectors of all sentences falling under a single cord_uid\n# claims_data_final = pd.DataFrame(columns = ['cord_uid','sentence','w2vVector','drugs'])\n# for cord_uid in claims_data.cord_uid.unique():\n#     sentences = \" \".join(claims_data.loc[claims_data.cord_uid==cord_uid,'sentence'])\n#     drugs = \",\".join(claims_data.loc[claims_data.cord_uid==cord_uid,'drug_terms_used'])\n#     drugs = \",\".join(list(set(drugs.split(','))))\n#     w2vVector = np.mean(list(claims_data.loc[claims_data.cord_uid==cord_uid,'w2vVector']), axis=0)\n#     claims_data_final = claims_data_final.append({'cord_uid':cord_uid,\\\n#                                                                 'sentence': sentences,\\\n#                                                                 'w2vVector': w2vVector,\\\n#                                                                  'drugs': drugs},\\\n#                                                                ignore_index=True)\n# len(claims_data_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# claims_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','text1','text2','drugs1','drugs2','similarity_score'])\n# jit(nopython=True, parallel=True)\n# paper_pairs = list(combinations(claims_data_final.index,2))\n# for i,j in paper_pairs:\n#     drugs1 = claims_data_final.drugs[i].split(',')\n#     drugs2 = claims_data_final.drugs[j].split(',')\n#     if any(d1 in drugs2 for d1 in drugs1):\n#         cos_sim = cosine_similarity(claims_data_final.w2vVector[i].reshape(1,-1),claims_data_final.w2vVector[j].reshape(1,-1))[0][0]\n#         claims_similarity = claims_similarity.append({'paper1_cord_uid':claims_data_final.cord_uid[i],\\\n#                                                     'paper2_cord_uid':claims_data_final.cord_uid[j],\\\n#                                                     'text1':claims_data_final.sentence[i],\\\n#                                                     'text2':claims_data_final.sentence[j],\\\n#                                                     'similarity_score':cos_sim,\\\n#                                                     'drugs1':claims_data_final.drugs[i],\\\n#                                                     'drugs2':claims_data_final.drugs[j]},\\\n#                                            ignore_index=True)\n\n# claims_similarity.to_csv('drug_claims_similarity.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate cosine similarity between core claims (individual claims)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"claims_data = input_processed\nclaims_data = claims_data.append(no_claims_mapped).reset_index(drop=True)\n\n#Replace drug name short forms with full forms\ndict = {'hcq':'hydroxychloroquine','cq':'chloroquine','azt':'azithromycin','azi':'azithromycin', 'az':'azithromycin'}\nfor key,value in dict.items():\n    claims_data['claims'] = [t.lower().replace(key,value) for t in claims_data.claims]\n\n# claims_similarity = pd.DataFrame(columns=['paper1_cord_uid','paper2_cord_uid','text1','text2','drugs1','drugs2','similarity_score'])\ndict = {}\nk = 0\n\nprint(claims_data.cord_uid.nunique())\nprint(len(claims_data))\n\n\njit(nopython=True, parallel=True)\n\npaper_pairs = list(combinations(claims_data.index,2))\nfor i,j in paper_pairs:\n    drugs1 = claims_data.drug_terms_used[i].split(',')\n    drugs2 = claims_data.drug_terms_used[j].split(',')\n    if any(d1 in drugs2 for d1 in drugs1) and (claims_data.cord_uid[i]!=claims_data.cord_uid[j]):\n        cos_sim = cosine_similarity(np.array(claims_data.w2vVector[i]).reshape(1,-1),np.array(claims_data.w2vVector[j]).reshape(1,-1))[0][0]\n        dict[k] = {'paper1_cord_uid':claims_data.cord_uid[i],\\\n                    'paper2_cord_uid':claims_data.cord_uid[j],\\\n                    'text1':claims_data.claims[i],\\\n                    'text2':claims_data.claims[j],\\\n                    'similarity_score':cos_sim,\\\n                    'drugs1':claims_data.drug_terms_used[i],\\\n                    'drugs2':claims_data.drug_terms_used[j]}\n        k = k + 1\nclaims_similarity = pd.DataFrame.from_dict(dict, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Claims with no vectors\nclaims_data = claims_unmapped\nclaims_data = claims_data.append(no_claims_unmapped).reset_index(drop=True)\n\n#Replace drug name short forms with full forms\ndict = {'hcq':'hydroxychloroquine','cq':'chloroquine','azt':'azithromycin','azi':'azithromycin', 'az':'azithromycin'}\nfor key,value in dict.items():\n    claims_data['claims'] = [t.lower().replace(key,value) for t in claims_data.claims]\n\nprint(claims_data.cord_uid.nunique())\nprint(len(claims_data))\n\ndict = {}\nk = 0\n\njit(nopython=True, parallel=True)\n\npaper_pairs = list(combinations(claims_data.index,2))\nfor i,j in paper_pairs:\n    drugs1 = claims_data.drug_terms_used[i].split(',')\n    drugs2 = claims_data.drug_terms_used[j].split(',')\n    if any(d1 in drugs2 for d1 in drugs1) and (claims_data.cord_uid[i]!=claims_data.cord_uid[j]):\n        dict[k] = {'paper1_cord_uid':claims_data.cord_uid[i],\\\n                    'paper2_cord_uid':claims_data.cord_uid[j],\\\n                    'text1':claims_data.claims[i],\\\n                    'text2':claims_data.claims[j],\\\n                    'similarity_score':'NA',\\\n                    'drugs1':claims_data.drug_terms_used[i],\\\n                    'drugs2':claims_data.drug_terms_used[j]}\n        k = k + 1\nclaims_with_no_vectors = pd.DataFrame.from_dict(dict, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(claims_similarity))\nprint(len(claims_with_no_vectors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(claims_similarity.paper1_cord_uid.nunique())\nprint(claims_similarity.paper2_cord_uid.nunique())\nprint(claims_with_no_vectors.paper1_cord_uid.nunique())\nprint(claims_with_no_vectors.paper2_cord_uid.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"claims_all = claims_similarity.loc[claims_similarity.similarity_score >=0.5,:]\nclaims_all = claims_all.append(claims_with_no_vectors).reset_index(drop=True)\nprint('After filtering on similarity scores:',len(claims_all))\n\nk = 0\ndict = {}\n\nfor i in range(0,len(claims_all)):\n#     drugs1 = list(nlp(claims_all.loc[i,'text1']).ents)\n#     drugs2 = list(nlp(claims_all.loc[j,'text2']).ents)\n    drugs1 = [d for d in drug_terms if d in claims_all.text1[i]]\n    drugs2 = [d for d in drug_terms if d in claims_all.text2[i]]\n    if any(d1 in drugs2 for d1 in drugs1):\n        dict[k] = {'paper1_cord_uid':claims_all.paper1_cord_uid[i],\\\n                    'paper2_cord_uid':claims_all.paper2_cord_uid[i],\\\n                    'text1':claims_all.text1[i],\\\n                    'text2':claims_all.text2[i],\\\n                    'similarity_score':claims_all.similarity_score[i],\\\n                    'drugs1':drugs1,\\\n                    'drugs2':drugs2}\n        k = k + 1\nclaims_filtered = pd.DataFrame.from_dict(dict, \"index\")\n\nprint('After filtering on drug names:',len(claims_filtered))\nprint(claims_filtered.paper1_cord_uid.nunique())\nprint(claims_filtered.paper2_cord_uid.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"claims_filtered = pd.merge(claims_filtered,metadata,how='inner',\\\n                           left_on = 'paper1_cord_uid',\\\n                          right_on = 'cord_uid')\ncols_rename = {'title':'title1','publish_time':'publish_time1'}\nclaims_filtered.rename(columns = cols_rename,inplace=True)\n\nclaims_filtered = pd.merge(claims_filtered,metadata,how='inner',\\\n                           left_on = 'paper2_cord_uid',\\\n                          right_on = 'cord_uid')\ncols_rename = {'title':'title2','publish_time':'publish_time2'}\nclaims_filtered.rename(columns = cols_rename,inplace=True)\n\nprint(len(claims_filtered))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"claims_filtered = claims_filtered.drop(columns=['cord_uid_x','cord_uid_y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# claims_similarity.to_csv('drug_individual_claims_similarity.csv')\n# claims_with_no_vectors.to_csv('drug_individual_claims_no_vectors.csv')\nclaims_filtered.to_csv('drug_individual_claims_filtered.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}