{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims Extraction\n",
    "#### Using discourse and claim detection paper here: https://arxiv.org/abs/1907.00962\n",
    "#### github link of the model by author:  https://github.com/titipata/detecting-scientific-claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"detecting-scientific-claim\")\n",
    "import preprocess as pp\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.nn import ModuleList, Linear\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "from lxml import etree, html\n",
    "import urllib\n",
    "\n",
    "import flask\n",
    "from flask import Flask, request\n",
    "from gevent.pywsgi import WSGIServer\n",
    "\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.modules import Seq2VecEncoder, TimeDistributed, TextFieldEmbedder, ConditionalRandomField, FeedForward\n",
    "\n",
    "from discourse import read_json\n",
    "from discourse.dataset_readers import ClaimAnnotationReaderJSON, CrfPubmedRCTReader\n",
    "from discourse.predictors import DiscourseClassifierPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimCrfPredictor(Predictor):\n",
    "    \"\"\"\"\n",
    "    Predictor wrapper for the AcademicPaperClassifier\n",
    "    \"\"\"\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "#         print(json_dict)\n",
    "        sentences = json_dict['sentences']\n",
    "        instance = self._dataset_reader.text_to_instance(sents=sentences)\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/pubmed-rct/model_crf.tar.gz ## the  CRF model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = load_archive(\"./model_crf.tar.gz\") ## available at github\n",
    "predictor = Predictor.from_archive(archive, 'discourse_crf_predictor')\n",
    "archive_ = load_archive(\"./model_crf.tar.gz\")\n",
    "discourse_predictor = Predictor.from_archive(archive_, 'discourse_crf_predictor')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHT_PATH=\"https://s3-us-west-2.amazonaws.com/pubmed-rct/model_crf_tf.th\"\n",
    "WEIGHT_PATH=\"./model_crf_tf.th\"\n",
    "model = predictor._model\n",
    "for param in list(model.parameters()):\n",
    "    param.requires_grad = False ## not train weights\n",
    "EMBEDDING_DIM = 300\n",
    "num_classes, constraints, include_start_end_transitions = 2, None, False\n",
    "model.crf = ConditionalRandomField(num_classes, constraints, \n",
    "                                   include_start_end_transitions=include_start_end_transitions)\n",
    "model.label_projection_layer = TimeDistributed(Linear(2 * EMBEDDING_DIM, num_classes))\n",
    "model.load_state_dict(torch.load(cached_path(WEIGHT_PATH), map_location='cpu'))\n",
    "reader = CrfPubmedRCTReader()\n",
    "claim_predictor = ClaimCrfPredictor(model, dataset_reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "text_input=\"The present world is facing a devastating reality as drug abuse prevails in every corner of a society. The progress of a country is obstructed due to the excessive practice of taking drugs by the young generation. Like other countries, Bangladesh is also facing this dreadful situation. The multiple use of drug substances leads an individual to a sorrowful destination and for this reason, the natural behavior of human mind is disrupted. An addicted individual may regain his normal life by proper monitoring and treatment. The objective of this study is to analyze a mathematical model on the dynamics of drug abuse in the perspective of Bangladesh and reduce the harmful consequences with effective control policies using the idea of optimal control theory. The model has been solved analytically introducing a specific optimal goal. Numerical simulations have also been performed to review the behaviors of analytical findings. The analytical results have been verified with the numerical simulations. The analysis of this paper shows that it is possible to control drug addiction if there is less interaction among general people with the addicted individuals. Family based care, proper medical treatment, awareness and educational programs can be the most effective ways to reduce the adverse effects of drug addiction in a shortest possible time.\"\n",
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_input=df_agg[df_agg['section']=='abstract'].sentence.iloc[2]\n",
    "#\n",
    "article = {'title': '', 'abstract': text_input}\n",
    "\n",
    "abstract = article.get('abstract', '')\n",
    "sentences = sent_tokenize(abstract)\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The present world is facing a devastating reality as drug abuse prevails in every corner of a society. The progress of a country is obstructed due to the excessive practice of taking drugs by the young generation. Like other countries, Bangladesh is also facing this dreadful situation. The multiple use of drug substances leads an individual to a sorrowful destination and for this reason, the natural behavior of human mind is disrupted. An addicted individual may regain his normal life by proper monitoring and treatment. The objective of this study is to analyze a mathematical model on the dynamics of drug abuse in the perspective of Bangladesh and reduce the harmful consequences with effective control policies using the idea of optimal control theory. The model has been solved analytically introducing a specific optimal goal. Numerical simulations have also been performed to review the behaviors of analytical findings. The analytical results have been verified with the numerical simulations. The analysis of this paper shows that it is possible to control drug addiction if there is less interaction among general people with the addicted individuals. Family based care, proper medical treatment, awareness and educational programs can be the most effective ways to reduce the adverse effects of drug addiction in a shortest possible time.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "discourse_output = discourse_predictor.predict_json({'abstract': abstract})\n",
    "labels = discourse_output['labels']\n",
    "pred = claim_predictor.predict_json({'sentences': sentences})\n",
    "best_paths = model.crf.viterbi_tags(torch.FloatTensor(pred['logits']).unsqueeze(0), \n",
    "                                    torch.LongTensor(pred['mask']).unsqueeze(0))\n",
    "p_claims = 100 * np.array(best_paths[0][0])\n",
    "p_claims = list(p_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'sents': sentences,\n",
    "        'scores': p_claims,\n",
    "        'labels': labels,\n",
    "        'len': len,\n",
    "        'enumerate': enumerate,\n",
    "        'zip': zip}\n",
    "data.update(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"../Data/cord_titles_abstracts_conclusions.csv\")\n",
    "df=pd.read_csv(\"../Data/section_text_with_drug_mentions_ann_200620.csv\").drop('Unnamed: 0',axis=1)\n",
    "df=df.drop_duplicates([\"cord_uid\",\"text\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>section</th>\n",
       "      <th>text</th>\n",
       "      <th>drug_terms_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>019rcbpg</td>\n",
       "      <td>Potential biological mechanisms of SARS-CoV-2 ...</td>\n",
       "      <td>there are indications in the literature of a n...</td>\n",
       "      <td>ifn-gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01es0zv4</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>coronavirus disease 2019 has become a global p...</td>\n",
       "      <td>chloroquine,lopinavir,remdesivir,ritonavir,azi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01es0zv4</td>\n",
       "      <td>CONCLUSION</td>\n",
       "      <td>covid-19 is a pandemic with high morbidity and...</td>\n",
       "      <td>chloroquine,lopinavir,remdesivir,ritonavir,azi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01es0zv4</td>\n",
       "      <td>CONCLUSION:</td>\n",
       "      <td>covid-19 is a pandemic with high morbidity and...</td>\n",
       "      <td>chloroquine,lopinavir,remdesivir,ritonavir,azi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01lyavy2</td>\n",
       "      <td>Abstract</td>\n",
       "      <td>then, the really positive treatment could be t...</td>\n",
       "      <td>protein s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                            section  \\\n",
       "0  019rcbpg  Potential biological mechanisms of SARS-CoV-2 ...   \n",
       "1  01es0zv4                                           Abstract   \n",
       "2  01es0zv4                                         CONCLUSION   \n",
       "3  01es0zv4                                        CONCLUSION:   \n",
       "4  01lyavy2                                           Abstract   \n",
       "\n",
       "                                                text  \\\n",
       "0  there are indications in the literature of a n...   \n",
       "1  coronavirus disease 2019 has become a global p...   \n",
       "2  covid-19 is a pandemic with high morbidity and...   \n",
       "3  covid-19 is a pandemic with high morbidity and...   \n",
       "4  then, the really positive treatment could be t...   \n",
       "\n",
       "                                     drug_terms_used  \n",
       "0                                          ifn-gamma  \n",
       "1  chloroquine,lopinavir,remdesivir,ritonavir,azi...  \n",
       "2  chloroquine,lopinavir,remdesivir,ritonavir,azi...  \n",
       "3  chloroquine,lopinavir,remdesivir,ritonavir,azi...  \n",
       "4                                          protein s  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs=df.rename(columns={\"text\":\"sentences\"}).copy()\n",
    "\n",
    "df_abs['sentences'] = df_abs.sentences.apply(sent_tokenize)\n",
    "# labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cord_uid', 'section', 'sentences', 'drug_terms_used'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "## this would stop the output from getting printed\n",
    "df_abs['pred'] = df_abs.sentences.apply(lambda x:claim_predictor.predict_json({'sentences': x}))\n",
    "df_abs['best_paths'] = df_abs.pred.apply(lambda x: model.crf.viterbi_tags(torch.FloatTensor(x['logits']).unsqueeze(0), \n",
    "                                    torch.LongTensor(x['mask']).unsqueeze(0)))\n",
    "df_abs['p_claims']=df_abs['best_paths'].apply(lambda x:100*np.array(x[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_abs['claims']=df_abs.apply(lambda x: np.extract(x['p_claims'],x['sentences']),axis=1)\n",
    "# df_abs['claims_id']=df_abs.apply(lambda x:np.extract(x['p_claims'],x['sentence_bid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_abs.loc[df_abs.claims.str.len()==0,\"claims\"]=np.empty((len(df_abs.loc[df_abs.claims.str.len()==0]), 0)).tolist()\n",
    "df_claims=df_abs[~(df_abs.claims.str.len()==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated=df_claims[[\"cord_uid\",\"section\",\"claims\"]].explode(\"claims\") ## converting list to rows\n",
    "df_updated=df_updated.drop_duplicates().reset_index(drop=True)\n",
    "df_updated['claim_flag']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged=df.merge(df_updated,on=[\"cord_uid\",\"section\"],how=\"left\")\n",
    "df_merged['claim_flag']=df_merged['claim_flag'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../Output/claims_flag_each_cord_uid_200620.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
