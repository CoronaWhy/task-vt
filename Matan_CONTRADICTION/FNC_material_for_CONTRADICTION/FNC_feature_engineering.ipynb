{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Challenge\n",
    "\n",
    "**Data preprocessing and Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Fake News Challenge](http://www.fakenewschallenge.org/) was launched in 2017 as the first part of a series of ML challenges meant to build an AI pipeline.  This first part (FNC-1) is meant to perform **Stance Detection**; the idea being to look for consensus views and outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.float_format = '{:, .2f}'.format\n",
    "pd.set_option('display.max_colwidth',500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import save, load\n",
    "from numpy import savez_compressed\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "#from scipy.misc import comb, logsumexp\n",
    "from sklearn.manifold import TSNE #a tool to visualize high dimensional data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # dimensionality reduction using truncated SVD (AKA LSA)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.collocations import *\n",
    "import string #python module\n",
    "import re # python regex module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # a sentance tokenizer\n",
    "nltk.download('gutenberg') # a text corpora and lexical resources\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting familiar with the data\n",
    "\n",
    "Read data into pandas for visual inspection, look for nan values, get some basic counts, edit column and data types names as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body_ID        0\n",
      "articleBody    0\n",
      "dtype: int64\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1683 entries, 0 to 1682\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Body_ID      1683 non-null   int64 \n",
      " 1   articleBody  1683 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 26.4+ KB\n",
      "None\n",
      "\n",
      "TrainBodies_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \\r\\n\\r\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebola fears spread across America. Today, we get confirmation. As The Daily Caller reports, one passenger at Dulles International Airport outside Washington, D.C. is apparently not taking any chances. A female passenger dressed in a hazmat suit - complete with a full body gown, mask and gloves - was spotted Wednesday waiting for a flight at the airport.\\r\\n\\r\\n \\r\\n\\r\\n\\r\\n\\r\\nSource: The Daily Caller\\r\\n\\r\\nWe particularly liked the JCPenney bag - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder with cheese can last? Two Australians say they bought a few McDonald's burgers for friends back in 1995, when they were teens, and one of the friends never showed up. So the kid's burger went uneaten—and stayed that way, Australia's News Network reports. \"We’re pretty sure it’s the oldest burger in the world,\" says one of the men, Casey Dean. Holding onto the burger for their friend \"started off as a joke,\" he adds, but \"the months became years an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, ISIS supporters announced that the group’s youngest soldier has died in combat.\\r\\n\\r\\nTwitter accounts linked to the Islamic State of Iraq and Al-Sham claimed that the child soldier “got martyred” with his father while fighting for the terrorist group in Syria.\\r\\n\\r\\nPhotos posted on Twitter showed the smiling boy in military fatigues holding weapons that, at times, are almost as large as his body. British media reported that the child was rough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents were killed in clashes between soldiers and the Islamist militants in northeast Nigeria and five civilians were killed in fighting elsewhere in the region, a military source and residents said on Monday.\\r\\n\\r\\nA ceasefire agreement between Boko Haram and the Nigerian government was expected to lead to the liberation of more than 200 schoolgirls kidnapped by the militants six months ago, and talks were due to continue in neighbouring Chad on Monday...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body_ID  \\\n",
       "0        0   \n",
       "1        4   \n",
       "2        5   \n",
       "3        6   \n",
       "4        7   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           articleBody  \n",
       "0  A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \\r\\n\\r\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized ...  \n",
       "1  Last week we hinted at what was to come as Ebola fears spread across America. Today, we get confirmation. As The Daily Caller reports, one passenger at Dulles International Airport outside Washington, D.C. is apparently not taking any chances. A female passenger dressed in a hazmat suit - complete with a full body gown, mask and gloves - was spotted Wednesday waiting for a flight at the airport.\\r\\n\\r\\n \\r\\n\\r\\n\\r\\n\\r\\nSource: The Daily Caller\\r\\n\\r\\nWe particularly liked the JCPenney bag - ...  \n",
       "2  (NEWSER) – Wonder how long a Quarter Pounder with cheese can last? Two Australians say they bought a few McDonald's burgers for friends back in 1995, when they were teens, and one of the friends never showed up. So the kid's burger went uneaten—and stayed that way, Australia's News Network reports. \"We’re pretty sure it’s the oldest burger in the world,\" says one of the men, Casey Dean. Holding onto the burger for their friend \"started off as a joke,\" he adds, but \"the months became years an...  \n",
       "3  Posting photos of a gun-toting child online, ISIS supporters announced that the group’s youngest soldier has died in combat.\\r\\n\\r\\nTwitter accounts linked to the Islamic State of Iraq and Al-Sham claimed that the child soldier “got martyred” with his father while fighting for the terrorist group in Syria.\\r\\n\\r\\nPhotos posted on Twitter showed the smiling boy in military fatigues holding weapons that, at times, are almost as large as his body. British media reported that the child was rough...  \n",
       "4  At least 25 suspected Boko Haram insurgents were killed in clashes between soldiers and the Islamist militants in northeast Nigeria and five civilians were killed in fighting elsewhere in the region, a military source and residents said on Monday.\\r\\n\\r\\nA ceasefire agreement between Boko Haram and the Nigerian government was expected to lead to the liberation of more than 200 schoolgirls kidnapped by the militants six months ago, and talks were due to continue in neighbouring Chad on Monday...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alias csv file paths for the 4 datasets provided\n",
    "Train_Bodies = 'train_bodies.csv'\n",
    "Train_Stances = 'train_stances.csv'\n",
    "\n",
    "Test_Bodies = 'competition_test_bodies.csv'\n",
    "Test_Stances = 'competition_test_stances.csv'\n",
    "\n",
    "# read in bodies data to pandas and inspect\n",
    "TrainBodies_df = pd.read_csv(Train_Bodies)\n",
    "# rename Body ID column for uniformity and incase we use dot instad of bracket notation when working with the data\n",
    "TrainBodies_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "# inspect df: look for nan values, data types and view a small sample\n",
    "print(TrainBodies_df.isna().sum())\n",
    "print()\n",
    "print(TrainBodies_df.info())\n",
    "print()\n",
    "print(\"TrainBodies_df\")\n",
    "TrainBodies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline    0\n",
      "Body_ID     0\n",
      "Stance      0\n",
      "dtype: int64\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49972 entries, 0 to 49971\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Headline  49972 non-null  object\n",
      " 1   Body_ID   49972 non-null  int64 \n",
      " 2   Stance    49972 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      "There are 1683 unique Body_ID values in the Train Stances dataset\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodies' near Mexico town where 43 students disappeared after police clash</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza as Israel opens dams</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, actor reportedly felt he wasn't right for part</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV Streaming Service Launching in April</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and up into his chest</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              Headline  \\\n",
       "0  Police find mass graves with at least '15 bodies' near Mexico town where 43 students disappeared after police clash   \n",
       "1                                                    Hundreds of Palestinians flee floods in Gaza as Israel opens dams   \n",
       "2                          Christian Bale passes on role of Steve Jobs, actor reportedly felt he wasn't right for part   \n",
       "3                                   HBO and Apple in Talks for $15/Month Apple TV Streaming Service Launching in April   \n",
       "4                                                      Spider burrowed through tourist's stomach and up into his chest   \n",
       "\n",
       "   Body_ID     Stance  \n",
       "0      712  unrelated  \n",
       "1      158      agree  \n",
       "2      137  unrelated  \n",
       "3     1034  unrelated  \n",
       "4     1923   disagree  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read in Stances dataset and inspect\n",
    "TrainStances_df = pd.read_csv(Train_Stances)\n",
    "# rename Body ID column for uniformity and incase we use dot instad of bracket notation when working with the data\n",
    "TrainStances_df.rename(columns={'Body ID':'Body_ID'}, inplace=True)\n",
    "# inspect df: look for nan values, data types and view a small sample\n",
    "print(TrainStances_df.isna().sum())\n",
    "print()\n",
    "print(TrainStances_df.info())\n",
    "print()\n",
    "print(\"There are {} unique Body_ID values in the Train Stances dataset\".format(TrainStances_df['Body_ID'].nunique()))\n",
    "print()\n",
    "display(TrainStances_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Headlines: 1648\n",
      "Number of unique Body_IDs: 1683\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique Headlines: %s\" % TrainStances_df.Headline.nunique())\n",
    "print(\"Number of unique Body_IDs: %s\" % TrainStances_df.Body_ID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Body_ID        0\n",
      "articleBody    0\n",
      "dtype: int64\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904 entries, 0 to 903\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Body_ID      904 non-null    int64 \n",
      " 1   articleBody  904 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 14.2+ KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al-Sisi has denied Israeli reports stating that he offered to extend the Gaza Strip.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body_ID  \\\n",
       "0        1   \n",
       "\n",
       "                                                                            articleBody  \n",
       "0  Al-Sisi has denied Israeli reports stating that he offered to extend the Gaza Strip.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TestBodies_df = pd.read_csv(Test_Bodies)\n",
    "TestBodies_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "print()\n",
    "print(TestBodies_df.isna().sum())\n",
    "print()\n",
    "print(TestBodies_df.info())\n",
    "print()\n",
    "display(TestBodies_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline    0\n",
      "Body_ID     0\n",
      "Stance      0\n",
      "dtype: int64\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25413 entries, 0 to 25412\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Headline  25413 non-null  object\n",
      " 1   Body_ID   25413 non-null  int64 \n",
      " 2   Stance    25413 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 595.7+ KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window</td>\n",
       "      <td>2008</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     Headline  \\\n",
       "0  Ferguson riots: Pregnant woman loses eye after cops fire BEAN BAG round through car window   \n",
       "\n",
       "   Body_ID     Stance  \n",
       "0     2008  unrelated  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TestStances_df = pd.read_csv(Test_Stances)\n",
    "TestStances_df.rename(columns = {'Body ID':'Body_ID'}, inplace=True)\n",
    "print(TestStances_df.isna().sum())\n",
    "print()\n",
    "print(TestStances_df.info())\n",
    "print()\n",
    "display(TestStances_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00035122, 0.00027922, 0.00038102, 0.00034443, 0.00034727,\n",
       "        0.00042069, 0.00038213, 0.00049886, 0.00046566, 0.00047894]),\n",
       " array([   0. ,  253.2,  506.4,  759.6, 1012.8, 1266. , 1519.2, 1772.4,\n",
       "        2025.6, 2278.8, 2532. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAARJ0lEQVR4nO3dcaid9X3H8fdncWmha1FrlKC2Sbe7QdygSzPrWNs/JuuMZVzLJug/BhGyMAPbH4XdUgrtKMMWtg6pM3NMiGVMhK7zQlOchLKxMTvjatVUUq/OaWYw0YKtda1N+90f55ft9P5u7nlyc+M1J+8XHJ7n+T2/33Oe733I+eR5zjnPSVUhSdK4n1nrHZAkvfkYDpKkjuEgSeoYDpKkjuEgSeqct9Y7sBouuuii2rRp01rvhiSdVR555JGXqmrDUuumIhw2bdrEgQMH1no3JOmskuS/TrbOy0qSpI7hIEnqGA6SpI7hIEnqGA6SpM6gcEhyTZJDSRaSzC2xPklub+sfS7J10tgkFyZ5MMlTbXpBa9+U5H+SPNoee1ajUEnScBPDIck64A5gO7AFuDHJlkXdtgMz7bETuHPA2Dlgf1XNAPvb8glPV9V722PXSouTJK3MkDOHK4GFqnqmql4H7gVmF/WZBe6pkYeA85NsnDB2Ftjb5vcC151mLZKkVTIkHC4Fnh9bPtzahvRZbuwlVXUEoE0vHuu3Ock3kvxTkg8utVNJdiY5kOTAsWPHBpQhSRpqyDeks0Tb4l8IOlmfIWMXOwK8q6peTvI+4B+SXFFV3/2pjVTdBdwFsG3bNn+xSHoT2TT3lTV53mdv+8iaPO80GnLmcBi4fGz5MuCFgX2WG/tiu/REmx4FqKofVtXLbf4R4GngF4cUI0laHUPC4WFgJsnmJOuBG4D5RX3mgZvap5auAl5pl4qWGzsP7GjzO4D7AZJsaG9kk+Q9jN7kfmbFFUqSTtnEy0pVdTzJbuABYB1wd1UdTLKrrd8D7AOuBRaA14CblxvbNn0bcF+SW4DngOtb+4eAP0lyHPgxsKuqvrMq1UqSBhl0V9aq2scoAMbb9ozNF3Dr0LGt/WXg6iXavwR8ach+SZLODL8hLUnqGA6SpI7hIEnqTMUvwUnSWlqr73XAmftuh2cOkqSO4SBJ6hgOkqSO4SBJ6viGtKSpsZZvDE8bzxwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLU8WdCpTNsrX668tnbPrImz6vp4JmDJKljOEiSOoaDJKljOEiSOoaDJKkzKBySXJPkUJKFJHNLrE+S29v6x5JsnTQ2yYVJHkzyVJtesGib70ryapKPnU6BkqRTNzEckqwD7gC2A1uAG5NsWdRtOzDTHjuBOweMnQP2V9UMsL8tj/s88NUV1CRJOk1DzhyuBBaq6pmqeh24F5hd1GcWuKdGHgLOT7JxwthZYG+b3wtcd2JjSa4DngEOrrAuSdJpGBIOlwLPjy0fbm1D+iw39pKqOgLQphcDJHkb8MfAp5fbqSQ7kxxIcuDYsWMDypAkDTUkHLJEWw3sM2TsYp8GPl9Vry7XqaruqqptVbVtw4YNEzYpSToVQ26fcRi4fGz5MuCFgX3WLzP2xSQbq+pIuwR1tLW/H/i9JJ8Dzgd+kuQHVfWFIQVJkk7fkDOHh4GZJJuTrAduAOYX9ZkHbmqfWroKeKVdKlpu7Dywo83vAO4HqKoPVtWmqtoE/AXwpwaDJL2xJp45VNXxJLuBB4B1wN1VdTDJrrZ+D7APuBZYAF4Dbl5ubNv0bcB9SW4BngOuX9XKJEkrNuiurFW1j1EAjLftGZsv4NahY1v7y8DVE573U0P2T5K0uvyGtCSp4+85SFNqrX5HQtPBcDgHreWLhj9AI50dvKwkSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSeoYDpKkjuEgSep4byXW7l5D3mdI0puVZw6SpI5nDjonePtq6dQYDnpD+SItnR28rCRJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqTOoHBIck2SQ0kWkswtsT5Jbm/rH0uyddLYJBcmeTDJU216QWu/Msmj7fHNJB9djUIlScNNDIck64A7gO3AFuDGJFsWddsOzLTHTuDOAWPngP1VNQPsb8sATwDbquq9wDXAXyXxdyck6Q005MzhSmChqp6pqteBe4HZRX1mgXtq5CHg/CQbJ4ydBfa2+b3AdQBV9VpVHW/tbwVqhbVJklZoSDhcCjw/tny4tQ3ps9zYS6rqCECbXnyiU5L3JzkIPA7sGgsLxvrsTHIgyYFjx44NKEOSNNSQcMgSbYv/N3+yPkPG9h2qvl5VVwC/Bnw8yVuX6HNXVW2rqm0bNmyYtElJ0ikYEg6HgcvHli8DXhjYZ7mxL7ZLT7Tp0cVPXFVPAt8HfnnAfkqSVsmQcHgYmEmyOcl64AZgflGfeeCm9qmlq4BX2qWi5cbOAzva/A7gfoDW97w2/27gl4BnV1qgJOnUTfwUUFUdT7IbeABYB9xdVQeT7Grr9wD7gGuBBeA14OblxrZN3wbcl+QW4Dng+tb+AWAuyY+AnwB/UFUvrUq1kqRBBn1EtKr2MQqA8bY9Y/MF3Dp0bGt/Gbh6ifYvAl8csl+SpDPDb0hLkjqGgySpYzhIkjrelmINbZr7ylrvgiQtyTMHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVJnUDgkuSbJoSQLSeaWWJ8kt7f1jyXZOmlskguTPJjkqTa9oLX/VpJHkjzepr+5GoVKkoabGA5J1gF3ANuBLcCNSbYs6rYdmGmPncCdA8bOAfuragbY35YBXgJ+p6p+BdgBfHHF1UmSVmTImcOVwEJVPVNVrwP3ArOL+swC99TIQ8D5STZOGDsL7G3ze4HrAKrqG1X1Qms/CLw1yVtWWJ8kaQWGhMOlwPNjy4db25A+y429pKqOALTpxUs89+8C36iqHy5ekWRnkgNJDhw7dmxAGZKkoYaEQ5Zoq4F9hoxd+kmTK4DPAr+/1PqququqtlXVtg0bNgzZpCRpoCHhcBi4fGz5MuCFgX2WG/tiu/REmx490SnJZcCXgZuq6ukB+yhJWkVDwuFhYCbJ5iTrgRuA+UV95oGb2qeWrgJeaZeKlhs7z+gNZ9r0foAk5wNfAT5eVf96GrVJklbovEkdqup4kt3AA8A64O6qOphkV1u/B9gHXAssAK8BNy83tm36NuC+JLcAzwHXt/bdwC8An0zyydb24ar6vzMLSdKZNTEcAKpqH6MAGG/bMzZfwK1Dx7b2l4Grl2j/DPCZIfslSToz/Ia0JKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOoPCIck1SQ4lWUgyt8T6JLm9rX8sydZJY5NcmOTBJE+16QWt/Z1Jvpbk1SRfWI0iJUmnZmI4JFkH3AFsB7YANybZsqjbdmCmPXYCdw4YOwfsr6oZYH9bBvgB8EngYysvS5J0OoacOVwJLFTVM1X1OnAvMLuozyxwT408BJyfZOOEsbPA3ja/F7gOoKq+X1X/wigkJElrYEg4XAo8P7Z8uLUN6bPc2Euq6ghAm148fLchyc4kB5IcOHbs2KkMlSRNMCQcskRbDewzZOyKVNVdVbWtqrZt2LBhNTYpSWqGhMNh4PKx5cuAFwb2WW7si+3SE216dPhuS5LOpCHh8DAwk2RzkvXADcD8oj7zwE3tU0tXAa+0S0XLjZ0HdrT5HcD9p1mLJGmVnDepQ1UdT7IbeABYB9xdVQeT7Grr9wD7gGuBBeA14OblxrZN3wbcl+QW4Dng+hPPmeRZ4B3A+iTXAR+uqm+tQr2SpAEmhgNAVe1jFADjbXvG5gu4dejY1v4ycPVJxmwasl+SpDPDb0hLkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqDwiHJNUkOJVlIMrfE+iS5va1/LMnWSWOTXJjkwSRPtekFY+s+3vofSvLbp1ukJOnUTAyHJOuAO4DtwBbgxiRbFnXbDsy0x07gzgFj54D9VTUD7G/LtPU3AFcA1wB/2bYjSXqDDDlzuBJYqKpnqup14F5gdlGfWeCeGnkIOD/JxgljZ4G9bX4vcN1Y+71V9cOq+k9goW1HkvQGOW9An0uB58eWDwPvH9Dn0gljL6mqIwBVdSTJxWPbemiJbf2UJDsZnaUAvJrk0IBaTuYi4KXTGH+2sd7pZr3T7afqzWdPa1vvPtmKIeGQJdpqYJ8hY1fyfFTVXcBdE7Y1SJIDVbVtNbZ1NrDe6Wa90+2NqnfIZaXDwOVjy5cBLwzss9zYF9ulJ9r06Ck8nyTpDBoSDg8DM0k2J1nP6M3i+UV95oGb2qeWrgJeaZeMlhs7D+xo8zuA+8fab0jyliSbGb3J/e8rrE+StAITLytV1fEku4EHgHXA3VV1MMmutn4PsA+4ltGbx68BNy83tm36NuC+JLcAzwHXtzEHk9wHfAs4DtxaVT9erYJPYlUuT51FrHe6We90e0PqTdWktwAkSecavyEtSeoYDpKkzjkdDpNuC3K2SvJskseTPJrkQGubmtuVJLk7ydEkT4y1nXJ9Sd7X/k4L7fYvS32Mes2dpN5PJfnvdowfTXLt2Lqzvd7Lk3wtyZNJDib5w9Y+lcd4mXrX9hhX1Tn5YPQG+dPAe4D1wDeBLWu9X6tU27PARYvaPgfMtfk54LNtfkur/S3A5vY3WbfWNUyo70PAVuCJ06mP0afgfp3Rd2u+Cmxf69pOod5PAR9bou801LsR2Nrm3w58u9U1lcd4mXrX9Bify2cOQ24LMk2m5nYlVfXPwHcWNZ9Sfe27Ne+oqn+r0b+qe8bGvKmcpN6TmYZ6j1TVf7T57wFPMrpLwlQe42XqPZk3pN5zORxOdsuPaVDAPyZ5pN1mBBbdrgQYv13JNPwdTrW+S9v84vazye6M7oJ899gllqmqN8km4FeBr3MOHONF9cIaHuNzORxWcmuPs8VvVNVWRnfDvTXJh5bpO81/B1jdW7u8mdwJ/DzwXuAI8GetfWrqTfJzwJeAP6qq7y7XdYm2s67mJepd02N8LofD1N6mo6peaNOjwJcZXSaa9tuVnGp9h9v84vazQlW9WFU/rqqfAH/N/18KnIp6k/wsoxfKv62qv2/NU3uMl6p3rY/xuRwOQ24LctZJ8rYkbz8xD3wYeILpv13JKdXXLkt8L8lV7RMdN42NedM78SLZfJTRMYYpqLft398AT1bVn4+tmspjfLJ61/wYr/U79Wv5YHTLj28zerf/E2u9P6tU03sYfZLhm8DBE3UB72T0o0pPtemFY2M+0f4Gh3gTfppjiRr/jtFp9o8Y/W/plpXUB2xr/+CeBr5Au2PAm+1xknq/CDwOPNZeLDZOUb0fYHQ55DHg0fa4dlqP8TL1rukx9vYZkqTOuXxZSZJ0EoaDJKljOEiSOoaDJKljOEiSOoaDJKljOEiSOv8LVe6PkGwGfB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# is there anything to note about the distribution of body_ID?\n",
    "# instantiate a figure with axes object\n",
    "fig, ax = plt.subplots()\n",
    "#alias data we will view\n",
    "x = TrainStances_df['Body_ID']\n",
    "# add to axis object within the figure\n",
    "ax.hist(x, density=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preparing text data for machine learning (in this instance) means:\n",
    " - label encode targets\n",
    " - lower case all text so the same word spelled with a capital does not result in a distinction/duplicate\n",
    " - tokenizing the corpus as preparation for feature engineering\n",
    " - remove stop words\n",
    " - stem corpus\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape:(49972, 4)\n",
      "df_test shape:(25413, 4)\n"
     ]
    }
   ],
   "source": [
    "# join TrainBodies_df and TrainStances_df\n",
    "\n",
    "df_train = TrainStances_df.merge(TrainBodies_df, how = 'left', on = 'Body_ID', validate= 'm:1')\n",
    "df_test = TestStances_df.merge(TestBodies_df, how = 'left', on = 'Body_ID', validate= 'm:1')\n",
    "\n",
    "print(\"df_train shape:\" + str(df_train.shape))\n",
    "print(\"df_test shape:\" + str(df_test.shape))\n",
    "#(not using f strings here since GCE (Google Compute Engine) py3 kernal does not support them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75385, 4)\n"
     ]
    }
   ],
   "source": [
    "# stack train and test sets\n",
    "objs = [df_train, df_test]\n",
    "data = pd.concat(objs, axis = 0, join='outer')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared after police clash</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-founder steve wozniak in sony’s steve jobs biopic.\\r\\n\\r\\ndanny boyle is directing the untitled film, based on walter isaacson's book and adapted by aaron sorkin, which is one of the most anticipated biopics in recent years.\\r\\n\\r\\nnegotiations have not yet begun, and it’s not even clear if rogen has an official offer, but the producers — scott rudin, guymon casady and mark gordon — have set their sig...</td>\n",
       "      <td>[police, find, mass, graves, with, at, least, 15, bodies, near, mexico, town, where, 43, students, disappeared, after, police, clash]</td>\n",
       "      <td>[danny, boyle, is, directing, the, untitled, film, seth, rogen, is, being, eyed, to, play, apple, co, founder, steve, wozniak, in, sony, steve, jobs, biopic, danny, boyle, is, directing, the, untitled, film, based, on, walter, isaacson, book, and, adapted, by, aaron, sorkin, which, is, one, of, the, most, anticipated, biopics, in, recent, years, negotiations, have, not, yet, begun, and, it, not, even, clear, if, rogen, has, an, official, offer, but, the, producers, scott, rudin, guymon, casa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hundreds of palestinians flee floods in gaza as israel opens dams</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>hundreds of palestinians were evacuated from their homes sunday morning after israeli authorities opened a number of dams near the border, flooding the gaza valley in the wake of a recent severe winter storm.\\r\\n\\r\\nthe gaza ministry of interior said in a statement that civil defense services and teams from the ministry of public works had evacuated more than 80 families from both sides of the gaza valley (wadi gaza) after their homes flooded as water levels reached more than three meters.\\r...</td>\n",
       "      <td>[hundreds, of, palestinians, flee, floods, in, gaza, as, israel, opens, dams]</td>\n",
       "      <td>[hundreds, of, palestinians, were, evacuated, from, their, homes, sunday, morning, after, israeli, authorities, opened, number, of, dams, near, the, border, flooding, the, gaza, valley, in, the, wake, of, recent, severe, winter, storm, the, gaza, ministry, of, interior, said, in, statement, that, civil, defense, services, and, teams, from, the, ministry, of, public, works, had, evacuated, more, than, 80, families, from, both, sides, of, the, gaza, valley, wadi, gaza, after, their, homes, flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>christian bale passes on role of steve jobs, actor reportedly felt he wasn't right for part</td>\n",
       "      <td>137</td>\n",
       "      <td>3</td>\n",
       "      <td>30-year-old moscow resident was hospitalized with wounds very intimate nature. as it became known lifenews, in the hands of doctors, the man complained that his casual acquaintance opoila in the sauna, and then gently held his castration operation. and actions criminals were executed with surgical precision - woman sewed all the smallest blood vessels.\\r\\n\\r\\n\\r\\n\\r\\n- i met a girl at the bar, and then we went to the sauna for a taxi. i remember that i had a beer, and more do not remember, -...</td>\n",
       "      <td>[christian, bale, passes, on, role, of, steve, jobs, actor, reportedly, felt, he, wasn, right, for, part]</td>\n",
       "      <td>[30, year, old, moscow, resident, was, hospitalized, with, wounds, very, intimate, nature, as, it, became, known, lifenews, in, the, hands, of, doctors, the, man, complained, that, his, casual, acquaintance, opoila, in, the, sauna, and, then, gently, held, his, castration, operation, and, actions, criminals, were, executed, with, surgical, precision, woman, sewed, all, the, smallest, blood, vessels, met, girl, at, the, bar, and, then, we, went, to, the, sauna, for, taxi, remember, that, had,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared after police clash   \n",
       "1                                                    hundreds of palestinians flee floods in gaza as israel opens dams   \n",
       "2                          christian bale passes on role of steve jobs, actor reportedly felt he wasn't right for part   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "1      158       0   \n",
       "2      137       3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-founder steve wozniak in sony’s steve jobs biopic.\\r\\n\\r\\ndanny boyle is directing the untitled film, based on walter isaacson's book and adapted by aaron sorkin, which is one of the most anticipated biopics in recent years.\\r\\n\\r\\nnegotiations have not yet begun, and it’s not even clear if rogen has an official offer, but the producers — scott rudin, guymon casady and mark gordon — have set their sig...   \n",
       "1  hundreds of palestinians were evacuated from their homes sunday morning after israeli authorities opened a number of dams near the border, flooding the gaza valley in the wake of a recent severe winter storm.\\r\\n\\r\\nthe gaza ministry of interior said in a statement that civil defense services and teams from the ministry of public works had evacuated more than 80 families from both sides of the gaza valley (wadi gaza) after their homes flooded as water levels reached more than three meters.\\r...   \n",
       "2  30-year-old moscow resident was hospitalized with wounds very intimate nature. as it became known lifenews, in the hands of doctors, the man complained that his casual acquaintance opoila in the sauna, and then gently held his castration operation. and actions criminals were executed with surgical precision - woman sewed all the smallest blood vessels.\\r\\n\\r\\n\\r\\n\\r\\n- i met a girl at the bar, and then we went to the sauna for a taxi. i remember that i had a beer, and more do not remember, -...   \n",
       "\n",
       "                                                                                                                         Headline_tokens  \\\n",
       "0  [police, find, mass, graves, with, at, least, 15, bodies, near, mexico, town, where, 43, students, disappeared, after, police, clash]   \n",
       "1                                                          [hundreds, of, palestinians, flee, floods, in, gaza, as, israel, opens, dams]   \n",
       "2                              [christian, bale, passes, on, role, of, steve, jobs, actor, reportedly, felt, he, wasn, right, for, part]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    articleBody_tokens  \n",
       "0  [danny, boyle, is, directing, the, untitled, film, seth, rogen, is, being, eyed, to, play, apple, co, founder, steve, wozniak, in, sony, steve, jobs, biopic, danny, boyle, is, directing, the, untitled, film, based, on, walter, isaacson, book, and, adapted, by, aaron, sorkin, which, is, one, of, the, most, anticipated, biopics, in, recent, years, negotiations, have, not, yet, begun, and, it, not, even, clear, if, rogen, has, an, official, offer, but, the, producers, scott, rudin, guymon, casa...  \n",
       "1  [hundreds, of, palestinians, were, evacuated, from, their, homes, sunday, morning, after, israeli, authorities, opened, number, of, dams, near, the, border, flooding, the, gaza, valley, in, the, wake, of, recent, severe, winter, storm, the, gaza, ministry, of, interior, said, in, statement, that, civil, defense, services, and, teams, from, the, ministry, of, public, works, had, evacuated, more, than, 80, families, from, both, sides, of, the, gaza, valley, wadi, gaza, after, their, homes, flo...  \n",
       "2  [30, year, old, moscow, resident, was, hospitalized, with, wounds, very, intimate, nature, as, it, became, known, lifenews, in, the, hands, of, doctors, the, man, complained, that, his, casual, acquaintance, opoila, in, the, sauna, and, then, gently, held, his, castration, operation, and, actions, criminals, were, executed, with, surgical, precision, woman, sewed, all, the, smallest, blood, vessels, met, girl, at, the, bar, and, then, we, went, to, the, sauna, for, taxi, remember, that, had,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace target lables with numeric target values, ie, label encode\n",
    "df_1 = copy.deepcopy(data)\n",
    "df_1.Stance.replace({'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3}, inplace=True)\n",
    "\n",
    "# lowercase all text\n",
    "df_2 = copy.deepcopy(df_1)\n",
    "df_2['Headline'] = df_2['Headline'].str.lower()\n",
    "df_2['articleBody'] = df_2['articleBody'].str.lower()\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer (r\"(?u)\\b\\w\\w+\\b\")\n",
    "df_2['Headline_tokens'] = df_2['Headline'].map(tokenizer.tokenize)\n",
    "df_2['articleBody_tokens'] = df_2['articleBody'].map(tokenizer.tokenize)\n",
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# instantiate list of stop words and other characters/punctuation to remove\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``',\"_\"]\n",
    "\n",
    "# remove stop words / keep everything except stopwords_list\n",
    "df_2['Headline_tokens'] = df_2['Headline_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])\n",
    "df_2['articleBody_tokens'] = df_2['articleBody_tokens'].apply(lambda x: [item for item in x if item not in stopwords_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The highest scoring team used stemming instead of lemmatization so I decided to use stemming since it's less complicated and will save time.  Lemmatization may be included in future work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared after police clash</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-founder steve wozniak in sony’s steve jobs biopic.\\r\\n\\r\\ndanny boyle is directing the untitled film, based on walter isaacson's book and adapted by aaron sorkin, which is one of the most anticipated biopics in recent years.\\r\\n\\r\\nnegotiations have not yet begun, and it’s not even clear if rogen has an official offer, but the producers — scott rudin, guymon casady and mark gordon — have set their sig...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, clash]</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, soni, steve, job, biopic, danni, boyl, direct, untitl, film, base, walter, isaacson, book, adapt, aaron, sorkin, one, anticip, biopic, recent, year, negoti, yet, begun, even, clear, rogen, offici, offer, produc, scott, rudin, guymon, casadi, mark, gordon, set, sight, talent, talk, cours, may, naught, christian, bale, actor, play, job, still, midst, close, deal, sourc, say, dealmak, process, sensit,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared after police clash   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-founder steve wozniak in sony’s steve jobs biopic.\\r\\n\\r\\ndanny boyle is directing the untitled film, based on walter isaacson's book and adapted by aaron sorkin, which is one of the most anticipated biopics in recent years.\\r\\n\\r\\nnegotiations have not yet begun, and it’s not even clear if rogen has an official offer, but the producers — scott rudin, guymon casady and mark gordon — have set their sig...   \n",
       "\n",
       "                                                                                         Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, clash]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    articleBody_tokens  \n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, soni, steve, job, biopic, danni, boyl, direct, untitl, film, base, walter, isaacson, book, adapt, aaron, sorkin, one, anticip, biopic, recent, year, negoti, yet, begun, even, clear, rogen, offici, offer, produc, scott, rudin, guymon, casadi, mark, gordon, set, sight, talent, talk, cours, may, naught, christian, bale, actor, play, job, still, midst, close, deal, sourc, say, dealmak, process, sensit,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alias stemmer method\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# stem Headline_tokens and articleBody_tokens\n",
    "df_2['Headline_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.Headline_tokens], axis=1)\n",
    "df_2['articleBody_tokens'] = df_2.apply(lambda row: [stemmer.stem(item) for item in row.articleBody_tokens], axis=1)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We have cleaned the unstructured data and can now work on molding it into training features that will become a painting of nuances that math can evaluate and extract meaning from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Basic Count Features\n",
    "\n",
    "A common approach in NLP is to look at word frequency distributions though, we will substitute those here for a set of ratios the top scoring team employed.\n",
    "\n",
    "- count how many times an n_gram appears in Headlline or articleBody, how many unique grams there are and the ratio between the two counts\n",
    "- calc how many grams that appear in Headline are also in articleBody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Generate n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The funcitons below return a list of the given n-grams instead of an iterable object that `nltk.BigramCollocationFinder` returns.  Having the grams in a list will allow us to generate counts as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/Cisco-Talos/fnc-1/blob/master/tree_model/ngram.py\n",
    "\n",
    "# create functions to build n_grams\n",
    "def getUnigram(words):\n",
    "    #assert type(words) == []\n",
    "    return words\n",
    "\n",
    "def getBigram(words, join_string, skip=0):\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for k in range(1, skip+2):\n",
    "                if i + k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i+k]]))\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "        return lst\n",
    "                    \n",
    "def getTrigram(words, join_string, skip=0):\n",
    "    #assert type(words) == []\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "            for k1 in range(1, skip+2):\n",
    "                for k2 in range(1, skip+2):\n",
    "                    if i+k1 < L and i+k1+k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i+k1], words[i+k1+k2]]))\n",
    "        return lst\n",
    "    else:\n",
    "        #set as bigram\n",
    "        lst = getBigram(words, join_string, skip)\n",
    "        return lst\n",
    "    \n",
    "def getFourgram(words, join_string):\n",
    "\n",
    "    #assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 3:\n",
    "        lst = []\n",
    "        for i in xrange(L-3):\n",
    "            lst.append( join_string.join([words[i], words[i+1], words[i+2], words[i+3]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = getTrigram(words, join_string)\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def getBiterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny', 'boy']\n",
    "        Output: a list of biterm, e.g., ['I_am', 'I_Denny', 'I_boy', 'am_Denny', 'am_boy', 'Denny_boy']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1,L):\n",
    "                lst.append( join_string.join([words[i], words[j]]) )\n",
    "        return lst\n",
    "    \n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = getUnigram(words)\n",
    "    return lst\n",
    "    \n",
    "def getTriterm(words, join_string):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., ['I', 'am', 'Denny']\n",
    "        Output: a list of triterm, e.g., ['I_am_Denny', 'I_Denny_am', 'am_I_Denny',\n",
    "        'am_Denny_I', 'Denny_I_am', 'Denny_am_I']\n",
    "        I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "   # assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in xrange(L-2):\n",
    "            for j in xrange(i+1,L-1):\n",
    "                for k in xrange(j+1,L):\n",
    "                    lst.append( join_string.join([words[i], words[j], words[k]]) )\n",
    "        return lst\n",
    "    else:\n",
    "        # set it as biterm\n",
    "        lst = getBiterm(words, join_string)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p>&nbsp;</p>\n",
    "We could create grams of greater length though, none of the top scoring teams went beyond trigrams. \n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate unigram\n",
    "df_2[\"Headline_unigram\"] = df_2[\"Headline_tokens\"].map(lambda x: getUnigram(x))\n",
    "df_2[\"articleBody_unigram\"] = df_2[\"articleBody_tokens\"].map(lambda x: getUnigram(x))\n",
    "\n",
    "# generate bigram\n",
    "join_str = \"_\"\n",
    "df_2[\"Headline_bigram\"] = df_2[\"Headline_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "df_2[\"articleBody_bigram\"] = df_2[\"articleBody_unigram\"].map(lambda x: getBigram(x, join_str))\n",
    "        \n",
    "# generate trigram\n",
    "join_str = \"_\"\n",
    "df_2[\"Headline_trigram\"] = df_2[\"Headline_unigram\"].map(lambda x: getTrigram(x, join_str))\n",
    "df_2[\"articleBody_trigram\"] = df_2[\"articleBody_unigram\"].map(lambda x: getTrigram(x, join_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_leas...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untit...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body_ID  Stance  \\\n",
       "0  police find mass graves with at least '15 bodi...      712       3   \n",
       "\n",
       "                                         articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n...   \n",
       "\n",
       "                                     Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                  articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                    Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                 articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                     Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_leas...   \n",
       "\n",
       "                                  articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untit...   \n",
       "\n",
       "                                    Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_...   \n",
       "\n",
       "                                 articleBody_trigram  \n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',50)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calc percent of text in given Headline or articleBody that is unique ( unique grams / ttl grams)\n",
    "\n",
    "''' \n",
    "    count ttl # of n-gram\n",
    "    count ttl # of unique n-gram\n",
    "    divide ttl # uniqe by ttl #\n",
    "    \n",
    "'''\n",
    "\n",
    "grams = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "feat_names = [\"Headline\", \"articleBody\"]\n",
    "\n",
    "for feat_name in feat_names:\n",
    "    for gram in grams:\n",
    "        df_2[\"count_of_%s_%s\" % (feat_name, gram)] = list(df_2.apply(lambda x: len(x[feat_name + \"_\" + gram]), axis=1))\n",
    "        df_2[\"count_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "              list(df_2.apply(lambda x: len(set(x[feat_name + \"_\" + gram])), axis=1))\n",
    "        df_2[\"ratio_of_unique_%s_%s\" % (feat_name, gram)] = \\\n",
    "            df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)] / df_2[\"count_of_%s_%s\"%(feat_name,gram)]\n",
    "            #map(try_divide, df_2[\"count_of_unique_%s_%s\"%(feat_name,gram)], df_2[\"count_of_%s_%s\"%(feat_name,gram)])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# overlapping n-grams count\n",
    "\n",
    "for gram in grams:\n",
    "    # count grams appearing in Headline that are also inside its coresponding articleBody\n",
    "    df_2[\"count_of_Headline_%s_in_articleBody\" % gram] = \\\n",
    "        list(df_2.apply(lambda x: sum([1. for w in x[\"Headline_\" + gram] if w in set(x[\"articleBody_\" + gram])]), axis=1))\n",
    "    \n",
    "    # return the ratio of overlapping grams to total grams to ttl Headline grams\n",
    "    df_2[\"ratio_of_Headline_%s_in_articleBody\" % gram] = \\\n",
    "        df_2[\"count_of_Headline_%s_in_articleBody\" % gram] / df_2[\"count_of_Headline_%s\" % gram]\n",
    "        #map(try_divide, df[\"count_of_Headline_%s_in_articleBody\" % gram], df[\"count_of_Headline_%s\" % gram])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_leas...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untit...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body_ID  Stance  \\\n",
       "0  police find mass graves with at least '15 bodi...      712       3   \n",
       "\n",
       "                                         articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n...   \n",
       "\n",
       "                                     Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                  articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                    Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                 articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                     Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_leas...   \n",
       "\n",
       "                                  articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untit...   \n",
       "\n",
       "                                    Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_...   \n",
       "\n",
       "                                 articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  \n",
       "0                                       0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# count number of sentences in headline and body\n",
    "for feat_name in feat_names:\n",
    "    df_2['len_sent_%s' % feat_name] = df_2[feat_name].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, ne...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, roge...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_leas...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untit...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body_ID  Stance  \\\n",
       "0  police find mass graves with at least '15 bodi...      712       3   \n",
       "\n",
       "                                         articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n...   \n",
       "\n",
       "                                     Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                  articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                    Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, ne...   \n",
       "\n",
       "                                 articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, roge...   \n",
       "\n",
       "                                     Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_leas...   \n",
       "\n",
       "                                  articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untit...   \n",
       "\n",
       "                                    Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_...   \n",
       "\n",
       "                                 articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \n",
       "0                     9  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',50)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save basic count features to disk for later use\n",
    "\n",
    "feat_names_bcf = [ n for n in df_2.columns \\\n",
    "                if \"count\" in n \\\n",
    "                or \"ratio\" in n \\\n",
    "                or \"len_sent\" in n]\n",
    "\n",
    "xBasicCountsTrain = df_2[feat_names_bcf].values\n",
    "outfilename_bcf_train = \"train.basic.pkl\"\n",
    "with open(outfilename_bcf_train, \"wb\") as outfile:\n",
    "    pickle.dump(feat_names, outfile, -1)\n",
    "    pickle.dump(xBasicCountsTrain, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency scores words on a spectrum of more vs less rare, i.e., important, within the corpus.  The more rare words contain more information or insight than heavily used words. To that end, we will join the Headline and articleBody sets to train the tf-idf vectorizer on the entire corpus and then fit to each aforementioned set individualy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cat_text(x):\n",
    "    res = '%s %s' % (' '.join(x['Headline_unigram']), ' '.join(x['articleBody_unigram']))\n",
    "    return res\n",
    "\n",
    "# concatenate Headline and Body so we can fit a tfidf vectorizer that will learn the combined vocabulary\n",
    "df_2['all_text'] = list(df_2.apply(cat_text, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fit a TfidfVectorizer on the concatenated strings (fit learns the vocabulary and idf)\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range = (1, 3), max_df= 0.8, min_df= 2)\n",
    "vec.fit(df_2['all_text'])\n",
    "vocabulary = vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xHeadlineTfidf.shape:(75385, 762837)\n",
      "xBodyTfidf.shape:(75385, 762837)\n"
     ]
    }
   ],
   "source": [
    "# transform Headline unigrams into tf-idf vector using the learned vocabulary\n",
    "vecH = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df= 2, vocabulary=vocabulary)\n",
    "xHeadlineTfidf = vecH.fit_transform(df_2['Headline_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"xHeadlineTfidf.shape:\" + str(xHeadlineTfidf.shape))\n",
    "\n",
    "# save for later use\n",
    "outfilename_htfidf_train = \"train.headline.tfidf.pkl\"\n",
    "with open (outfilename_htfidf_train, 'wb') as outfile:\n",
    "    pickle.dump(xHeadlineTfidf, outfile, -1)\n",
    "\n",
    "# transform articleBody unigrams using the learned vocabulary\n",
    "vecB = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)\n",
    "xBodyTfidf = vecB.fit_transform(df_2['articleBody_unigram'].map(lambda x: ' '.join(x)))\n",
    "print (\"xBodyTfidf.shape:\" +  str(xBodyTfidf.shape))\n",
    "\n",
    "outfilename_btfidf_train = \"train.body.tfidf.pkl\"\n",
    "with open(outfilename_btfidf_train, \"wb\") as outfile:\n",
    "    pickle.dump(xBodyTfidf, outfile, -1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "scikit-learn has a `cosine_similarity` function though, we must consider the input shape of our data and the desired output shape.  We need to take in extremely large 2-D arrays and end up with a 2-D array of **one** feature.  To do this, we first convert each input into a Coordinate Format matrix before computing `cosine_similarity` , calculate the row-wise cosine_similarity and finally coerce it from a 1-D to 2-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    try:\n",
    "        if type(x) is np.ndarray: x = x.reshape(1, -1)\n",
    "        if type(y) is np.ndarray: y = y.reshape(1, -1)\n",
    "        d = cosine_similarity(x, y)\n",
    "        d = d[0][0]\n",
    "    except:\n",
    "        print (x)\n",
    "        print (y)\n",
    "        d = 0.\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75385, 1)\n"
     ]
    }
   ],
   "source": [
    "# calculate cosine similarity between Headline and articleBody\n",
    "\n",
    "#load_xHeadlineTfidf = pickle.load(open(\"train.headline.tfidf.pkl\", 'rb'))\n",
    "#load_bodyTfidf = pickle.load(open(\"train.body.tfidf.pkl\", 'rb'))\n",
    "\n",
    "simTfidf_train = np.asarray(list(map(cosine_sim, xHeadlineTfidf, xBodyTfidf)))[:, np.newaxis]\n",
    "\n",
    "print(simTfidf_train.shape)\n",
    "\n",
    "# save for later use\n",
    "outfilename_simtfidf_train = \"train.sim.tfidf.pkl\"\n",
    "with open(outfilename_simtfidf_train, \"wb\") as outfile:\n",
    "    pickle.dump(simTfidf_train, outfile, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Applying Singular Value Decomposition (SVD) to the tf-idf features to reduce dimensionality while preserving similarity structure and find latent topics.  Take tf-idf features and apply SVD.  Then calculate cosine similarities between the SVD features of Headline and articleBody.  This similarity metric is telling of whether the body and headline are related or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75385, 762837)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "(75385, 762837)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(xHeadlineTfidf.shape)\n",
    "print(type(xHeadlineTfidf))\n",
    "print()\n",
    "print(xBodyTfidf.shape)\n",
    "print(type(xBodyTfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xHeadlineTfidf.shape:\n",
      "(75385, 762837)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import vstack\n",
    "xHBTfidf = vstack((xHeadlineTfidf, xBodyTfidf)).toarray() # toarray() converts the csr_matrix objects to numpy arrays\n",
    "svd = TruncatedSVD(n_components=100, n_iter=15, random_state = 42)\n",
    "\n",
    "# fit to the combined train-test set (or t\n",
    "svd.fit(xHBTfidf) \n",
    "print ('xHeadlineTfidf.shape:')\n",
    "print (xHeadlineTfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xHeadlineSvd.shape:\n",
      "(75385, 100)\n"
     ]
    }
   ],
   "source": [
    "# transform headline tfidf features using svd\n",
    "xHeadlineSvd = svd.transform(xHeadlineTfidf)\n",
    "print ('xHeadlineSvd.shape:')\n",
    "print (xHeadlineSvd.shape)\n",
    "\n",
    "# save for later use\n",
    "xHeadlineSvdTrain = xHeadlineSvd\n",
    "outfilename_hsvd_train = \"train.headline.svd.pkl\"\n",
    "with open(outfilename_hsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(xHeadlineSvdTrain, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xBodySvd.shape:\n",
      "(75385, 100)\n"
     ]
    }
   ],
   "source": [
    "# transform articleBody tfidf features using svd\n",
    "xBodySvd = svd.transform(xBodyTfidf)\n",
    "print ('xBodySvd.shape:')\n",
    "print (xBodySvd.shape)\n",
    "\n",
    "# save for later use\n",
    "xBodySvdTrain = xBodySvd\n",
    "outfilename_bsvd_train = \"train.body.svd.pkl\"\n",
    "with open(outfilename_bsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(xBodySvdTrain, outfile, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_svd_train shape:\n",
      "(75385, 1)\n"
     ]
    }
   ],
   "source": [
    "# calculate cosine similarity for each record\n",
    "\n",
    "simSvd_train = np.asarray(list(map(cosine_sim, xHeadlineSvd, xBodySvd)))[:, np.newaxis]\n",
    "print ('sim_svd_train shape:')\n",
    "print (simSvd_train.shape)\n",
    "\n",
    "# save for later use\n",
    "outfilename_simsvd_train = \"train.sim.svd.pkl\"\n",
    "with open(outfilename_simsvd_train, \"wb\") as outfile:\n",
    "    pickle.dump(simSvd_train, outfile, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_2['Headline_unigram_vec'] = df_2['Headline_tokens']\n",
    "df_2['articleBody_unigram_vec'] = df_2['articleBody_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Headline_unigram_vec</th>\n",
       "      <th>articleBody_unigram_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...   \n",
       "\n",
       "                                                                                       Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                    articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                   articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                       Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...   \n",
       "\n",
       "                                                                                    articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...   \n",
       "\n",
       "                                                                                      Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...   \n",
       "\n",
       "                                                                                   articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \\\n",
       "0                     9   \n",
       "\n",
       "                                                                                              all_text  \\\n",
       "0  polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...   \n",
       "\n",
       "                                                                                  Headline_unigram_vec  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                               articleBody_unigram_vec  \n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_2 Headline_unigram_vec type: <class 'pandas.core.series.Series'>\n",
      "df_2 Headline_unigram_array type: <class 'str'>\n",
      "\n",
      "headline vec type: <class 'numpy.ndarray'>\n",
      "headline vec shape:(75385, 300)\n",
      "\n",
      "headlineVec_norm vec type: <class 'numpy.ndarray'>\n",
      "headlineVec_norm vec shape:(75385, 300)\n"
     ]
    }
   ],
   "source": [
    "Headline_unigram_array = df_2['Headline_unigram_vec'].values\n",
    "print(\"df_2 Headline_unigram_vec type: %s\" % type(df_2['Headline_unigram_vec']))\n",
    "print(\"df_2 Headline_unigram_array type: %s\" % type('Headline_unigram_array'))\n",
    "print()\n",
    "\n",
    "headlineVec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), Headline_unigram_array)))\n",
    "headlineVec_norm = normalize(headlineVec)\n",
    "print(\"headline vec type: %s\" % type(headlineVec))\n",
    "print(\"headline vec shape:\" +  str(headlineVec.shape))\n",
    "print()\n",
    "print(\"headlineVec_norm vec type: %s\" % type(headlineVec_norm))\n",
    "print(\"headlineVec_norm vec shape:\" + str(headlineVec_norm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline word2vec features of training set saved in train.headline.word2vec.pkl\n"
     ]
    }
   ],
   "source": [
    "headlineVecTrain = headlineVec_norm\n",
    "outfilename_hvec_train = \"train.headline.word2vec.pkl\"\n",
    "with open(outfilename_hvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(headlineVecTrain, outfile, -1)\n",
    "print ('headline word2vec features of training set saved in %s' % outfilename_hvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_2 articleBody_unigram_vec type: <class 'pandas.core.series.Series'>\n",
      "df_2 Body_unigram_array type: <class 'str'>\n",
      "\n",
      "BodyVec type: <class 'numpy.ndarray'>\n",
      "BodyVec shape:(75385, 300)\n",
      "\n",
      "bodyVec_norm type: <class 'numpy.ndarray'>\n",
      "bodyVec_norm shape:(75385, 300)\n"
     ]
    }
   ],
   "source": [
    "Body_unigram_array = df_2['articleBody_unigram_vec'].values\n",
    "print(\"df_2 articleBody_unigram_vec type: %s\" % type(df_2['articleBody_unigram_vec']))\n",
    "print(\"df_2 Body_unigram_array type: %s\" % type('Body_unigram_array'))\n",
    "print()\n",
    "\n",
    "BodyVec = np.array(list(map(lambda x: reduce(np.add, [model[y] for y in x if y in model], [0.]*300), Body_unigram_array)))\n",
    "#bodyVec = np.array(bodyVec)\n",
    "BodyVec_norm = normalize(BodyVec)\n",
    "\n",
    "print(\"BodyVec type: %s\" % type(BodyVec))\n",
    "print(\"BodyVec shape:\" +  str(BodyVec.shape))\n",
    "print()\n",
    "print(\"bodyVec_norm type: %s\" % type(BodyVec_norm))\n",
    "print(\"bodyVec_norm shape:\" + str(BodyVec_norm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body word2vec features of training set saved in train.body.word2vec.pkl\n"
     ]
    }
   ],
   "source": [
    "# save train dataset\n",
    "bodyVecTrain = BodyVec_norm\n",
    "outfilename_bvec_train = \"train.body.word2vec.pkl\"\n",
    "with open(outfilename_bvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(bodyVecTrain, outfile, -1)\n",
    "print ('body word2vec features of training set saved in %s' % outfilename_bvec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(75385, 1)\n",
      "simVec_w2v num dimensions:2\n",
      "[[0.53903601]\n",
      " [0.79412922]]\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity between headline/body word2vec features\n",
    "simVec_w2v = np.asarray(list(map(cosine_sim, headlineVec_norm, BodyVec_norm)))[:, np.newaxis]\n",
    "print(type(simVec_w2v))\n",
    "print(simVec_w2v.shape)\n",
    "print(\"simVec_w2v num dimensions:\" + str(simVec_w2v.ndim))\n",
    "print(simVec_w2v[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec sim. features of training set saved in train.sim.word2vec.pkl\n"
     ]
    }
   ],
   "source": [
    "simVecTrain = simVec_w2v\n",
    "outfilename_simvec_train = \"train.sim.word2vec.pkl\"\n",
    "with open(outfilename_simvec_train, \"wb\") as outfile:\n",
    "    pickle.dump(simVecTrain, outfile, -1)\n",
    "print ('word2vec sim. features of training set saved in %s' % outfilename_simvec_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sentiment Features\n",
    "\n",
    "- Use [NLTK Sentiment Analyzer](https://www.nltk.org/_modules/nltk/sentiment/vader.html) with [VADERSentiment](https://github.com/mgavish/vaderSentiment) to assign a sentiment polarity score to Headline and articelBody separately.\n",
    "- negative score means a negative opinion.\n",
    "- Do headline and articleBody have same sentiment?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/durdenjax/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate polarity score of each sentance in a Headline observation and return the average\n",
    "\n",
    "sid = SentimentIntensityAnalyzer() # https://www.nltk.org/howto/sentiment.html\n",
    "\n",
    "def compute_sentiment(sentences):\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence) # https://www.nltk.org/howto/sentiment.html\n",
    "        result.append(ss)\n",
    "    return pd.DataFrame(result).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Headline_unigram_vec</th>\n",
       "      <th>articleBody_unigram_vec</th>\n",
       "      <th>headline_sentmts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...   \n",
       "\n",
       "                                                                                       Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                    articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                   articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                       Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...   \n",
       "\n",
       "                                                                                    articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...   \n",
       "\n",
       "                                                                                      Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...   \n",
       "\n",
       "                                                                                   articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \\\n",
       "0                     9   \n",
       "\n",
       "                                                                                              all_text  \\\n",
       "0  polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...   \n",
       "\n",
       "                                                                                  Headline_unigram_vec  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                               articleBody_unigram_vec  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      headline_sentmts  \n",
       "0  [police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2['headline_sentmts'] = df_2['Headline'].apply(lambda x: sent_tokenize(x)) # nltk's method sent_tokenize()\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_2 = pd.concat([df_2, df_2['headline_sentmts'].apply(lambda x: compute_sentiment(x))], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Headline_unigram_vec</th>\n",
       "      <th>articleBody_unigram_vec</th>\n",
       "      <th>headline_sentmts</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...   \n",
       "\n",
       "                                                                                       Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                    articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                   articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                       Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...   \n",
       "\n",
       "                                                                                    articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...   \n",
       "\n",
       "                                                                                      Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...   \n",
       "\n",
       "                                                                                   articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \\\n",
       "0                     9   \n",
       "\n",
       "                                                                                              all_text  \\\n",
       "0  polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...   \n",
       "\n",
       "                                                                                  Headline_unigram_vec  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                               articleBody_unigram_vec  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      headline_sentmts  \\\n",
       "0  [police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...   \n",
       "\n",
       "     neg    neu  pos  compound  \n",
       "0  0.194  0.806  0.0   -0.4767  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Headline_unigram_vec</th>\n",
       "      <th>articleBody_unigram_vec</th>\n",
       "      <th>headline_sentmts</th>\n",
       "      <th>h_neg</th>\n",
       "      <th>h_neu</th>\n",
       "      <th>h_pos</th>\n",
       "      <th>h_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...   \n",
       "\n",
       "                                                                                       Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                    articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                   articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                       Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...   \n",
       "\n",
       "                                                                                    articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...   \n",
       "\n",
       "                                                                                      Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...   \n",
       "\n",
       "                                                                                   articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \\\n",
       "0                     9   \n",
       "\n",
       "                                                                                              all_text  \\\n",
       "0  polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...   \n",
       "\n",
       "                                                                                  Headline_unigram_vec  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                               articleBody_unigram_vec  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      headline_sentmts  \\\n",
       "0  [police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...   \n",
       "\n",
       "   h_neg  h_neu  h_pos  h_compound  \n",
       "0  0.194  0.806    0.0     -0.4767  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.rename(columns={'compound':'h_compound', 'neg':'h_neg', 'neu':'h_neu', 'pos':'h_pos'}, inplace=True)\n",
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlineSenti.shape:(75385, 4)\n"
     ]
    }
   ],
   "source": [
    "headlineSenti = df_2[['h_compound','h_neg','h_neu','h_pos']].values\n",
    "print ('headlineSenti.shape:' + str(headlineSenti.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headline sentiment features of training set saved in train.headline.senti.pkl\n"
     ]
    }
   ],
   "source": [
    "headlineSentiTrain = headlineSenti\n",
    "outfilename_hsenti_train = \"train.headline.senti.pkl\"\n",
    "with open(outfilename_hsenti_train, \"wb\") as outfile:\n",
    "    pickle.dump(headlineSentiTrain, outfile, -1)\n",
    "print ('headline sentiment features of training set saved in %s' % outfilename_hsenti_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodySenti.shape:(75385, 4)\n"
     ]
    }
   ],
   "source": [
    "df_2['body_sents'] = df_2['articleBody'].map(lambda x: sent_tokenize(x))\n",
    "df_2 = pd.concat([df_2, df_2['body_sents'].apply(lambda x: compute_sentiment(x))], axis=1)\n",
    "df_2.rename(columns={'compound':'b_compound', 'neg':'b_neg', 'neu':'b_neu', 'pos':'b_pos'}, inplace=True)\n",
    "bodySenti = df_2[['b_compound','b_neg','b_neu','b_pos']].values\n",
    "print ('bodySenti.shape:' + str(bodySenti.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.sort(*, key=None, reverse=False)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(df_2.columns)\n",
    "cols.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body sentiment features of training set saved in train.body.senti.pkl\n"
     ]
    }
   ],
   "source": [
    "bodySentiTrain = bodySenti\n",
    "outfilename_bsenti_train = \"train.body.senti.pkl\"\n",
    "with open(outfilename_bsenti_train, \"wb\") as outfile:\n",
    "    pickle.dump(bodySentiTrain, outfile, -1)\n",
    "print ('body sentiment features of training set saved in %s' % outfilename_bsenti_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body_ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline_tokens</th>\n",
       "      <th>articleBody_tokens</th>\n",
       "      <th>Headline_unigram</th>\n",
       "      <th>articleBody_unigram</th>\n",
       "      <th>Headline_bigram</th>\n",
       "      <th>articleBody_bigram</th>\n",
       "      <th>Headline_trigram</th>\n",
       "      <th>articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram</th>\n",
       "      <th>count_of_unique_Headline_unigram</th>\n",
       "      <th>ratio_of_unique_Headline_unigram</th>\n",
       "      <th>count_of_Headline_bigram</th>\n",
       "      <th>count_of_unique_Headline_bigram</th>\n",
       "      <th>ratio_of_unique_Headline_bigram</th>\n",
       "      <th>count_of_Headline_trigram</th>\n",
       "      <th>count_of_unique_Headline_trigram</th>\n",
       "      <th>ratio_of_unique_Headline_trigram</th>\n",
       "      <th>count_of_articleBody_unigram</th>\n",
       "      <th>count_of_unique_articleBody_unigram</th>\n",
       "      <th>ratio_of_unique_articleBody_unigram</th>\n",
       "      <th>count_of_articleBody_bigram</th>\n",
       "      <th>count_of_unique_articleBody_bigram</th>\n",
       "      <th>ratio_of_unique_articleBody_bigram</th>\n",
       "      <th>count_of_articleBody_trigram</th>\n",
       "      <th>count_of_unique_articleBody_trigram</th>\n",
       "      <th>ratio_of_unique_articleBody_trigram</th>\n",
       "      <th>count_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_unigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_bigram_in_articleBody</th>\n",
       "      <th>count_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>ratio_of_Headline_trigram_in_articleBody</th>\n",
       "      <th>len_sent_Headline</th>\n",
       "      <th>len_sent_articleBody</th>\n",
       "      <th>all_text</th>\n",
       "      <th>Headline_unigram_vec</th>\n",
       "      <th>articleBody_unigram_vec</th>\n",
       "      <th>headline_sentmts</th>\n",
       "      <th>h_neg</th>\n",
       "      <th>h_neu</th>\n",
       "      <th>h_pos</th>\n",
       "      <th>h_compound</th>\n",
       "      <th>body_sents</th>\n",
       "      <th>b_neg</th>\n",
       "      <th>b_neu</th>\n",
       "      <th>b_pos</th>\n",
       "      <th>b_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...</td>\n",
       "      <td>712</td>\n",
       "      <td>3</td>\n",
       "      <td>danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...</td>\n",
       "      <td>[danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...</td>\n",
       "      <td>[polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...</td>\n",
       "      <td>[danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>88</td>\n",
       "      <td>0.77193</td>\n",
       "      <td>113</td>\n",
       "      <td>108</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...</td>\n",
       "      <td>[polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...</td>\n",
       "      <td>[danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...</td>\n",
       "      <td>[police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>[danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-fou...</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.931444</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>0.226711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Headline  \\\n",
       "0  police find mass graves with at least '15 bodies' near mexico town where 43 students disappeared...   \n",
       "\n",
       "   Body_ID  Stance  \\\n",
       "0      712       3   \n",
       "\n",
       "                                                                                           articleBody  \\\n",
       "0  danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-foun...   \n",
       "\n",
       "                                                                                       Headline_tokens  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                    articleBody_tokens  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      Headline_unigram  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                                   articleBody_unigram  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                       Headline_bigram  \\\n",
       "0  [polic_find, find_mass, mass_grave, grave_least, least_15, 15_bodi, bodi_near, near_mexico, mexi...   \n",
       "\n",
       "                                                                                    articleBody_bigram  \\\n",
       "0  [danni_boyl, boyl_direct, direct_untitl, untitl_film, film_seth, seth_rogen, rogen_eye, eye_play...   \n",
       "\n",
       "                                                                                      Headline_trigram  \\\n",
       "0  [polic_find_mass, find_mass_grave, mass_grave_least, grave_least_15, least_15_bodi, 15_bodi_near...   \n",
       "\n",
       "                                                                                   articleBody_trigram  \\\n",
       "0  [danni_boyl_direct, boyl_direct_untitl, direct_untitl_film, untitl_film_seth, film_seth_rogen, s...   \n",
       "\n",
       "   count_of_Headline_unigram  count_of_unique_Headline_unigram  \\\n",
       "0                         15                                14   \n",
       "\n",
       "   ratio_of_unique_Headline_unigram  count_of_Headline_bigram  \\\n",
       "0                          0.933333                        14   \n",
       "\n",
       "   count_of_unique_Headline_bigram  ratio_of_unique_Headline_bigram  \\\n",
       "0                               14                              1.0   \n",
       "\n",
       "   count_of_Headline_trigram  count_of_unique_Headline_trigram  \\\n",
       "0                         13                                13   \n",
       "\n",
       "   ratio_of_unique_Headline_trigram  count_of_articleBody_unigram  \\\n",
       "0                               1.0                           114   \n",
       "\n",
       "   count_of_unique_articleBody_unigram  ratio_of_unique_articleBody_unigram  \\\n",
       "0                                   88                              0.77193   \n",
       "\n",
       "   count_of_articleBody_bigram  count_of_unique_articleBody_bigram  \\\n",
       "0                          113                                 108   \n",
       "\n",
       "   ratio_of_unique_articleBody_bigram  count_of_articleBody_trigram  \\\n",
       "0                            0.955752                           112   \n",
       "\n",
       "   count_of_unique_articleBody_trigram  ratio_of_unique_articleBody_trigram  \\\n",
       "0                                  109                             0.973214   \n",
       "\n",
       "   count_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_unigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   count_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   ratio_of_Headline_bigram_in_articleBody  \\\n",
       "0                                      0.0   \n",
       "\n",
       "   count_of_Headline_trigram_in_articleBody  \\\n",
       "0                                       0.0   \n",
       "\n",
       "   ratio_of_Headline_trigram_in_articleBody  len_sent_Headline  \\\n",
       "0                                       0.0                  1   \n",
       "\n",
       "   len_sent_articleBody  \\\n",
       "0                     9   \n",
       "\n",
       "                                                                                              all_text  \\\n",
       "0  polic find mass grave least 15 bodi near mexico town 43 student disappear polic clash danni boyl...   \n",
       "\n",
       "                                                                                  Headline_unigram_vec  \\\n",
       "0  [polic, find, mass, grave, least, 15, bodi, near, mexico, town, 43, student, disappear, polic, c...   \n",
       "\n",
       "                                                                               articleBody_unigram_vec  \\\n",
       "0  [danni, boyl, direct, untitl, film, seth, rogen, eye, play, appl, co, founder, steve, wozniak, s...   \n",
       "\n",
       "                                                                                      headline_sentmts  \\\n",
       "0  [police find mass graves with at least '15 bodies' near mexico town where 43 students disappeare...   \n",
       "\n",
       "   h_neg  h_neu  h_pos  h_compound  \\\n",
       "0  0.194  0.806    0.0     -0.4767   \n",
       "\n",
       "                                                                                            body_sents  \\\n",
       "0  [danny boyle is directing the untitled film\\r\\n\\r\\nseth rogen is being eyed to play apple co-fou...   \n",
       "\n",
       "      b_neg     b_neu     b_pos  b_compound  \n",
       "0  0.004444  0.931444  0.064222    0.226711  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save engineered features to one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15463387/pickle-putting-more-than-1-object-in-a-file/15463472\n",
    "\n",
    "# load features from pkl files\n",
    "\n",
    "# basic count features\n",
    "with open('train.basic.pkl', 'rb') as infile:\n",
    "    feat_names = pickle.load(infile)\n",
    "    xBasicCountsTrain = pickle.load(infile)\n",
    "\n",
    "# tfidf vectorized headline\n",
    "with open('train.headline.tfidf.pkl', 'rb') as tfidf_head:\n",
    "    headline_tfidf = pickle.load(tfidf_head)\n",
    "    headline_tfidf = headline_tfidf.toarray()\n",
    "    \n",
    "# tfidf vectorized body\n",
    "with open('train.body.tfidf.pkl', 'rb') as tfidf_in:\n",
    "    body_tfidf = pickle.load(tfidf_in)\n",
    "    body_tfidf = body_tfidf.toarray()\n",
    "\n",
    "# cosine similarity between tfidf headline and body\n",
    "with open('train.sim.tfidf.pkl', 'rb') as tfidf_sim:\n",
    "    sim_tfidf = pickle.load(tfidf_sim)\n",
    "\n",
    "# svd of headline\n",
    "with open('train.headline.svd.pkl', 'rb') as svd_head:\n",
    "    headline_svd = pickle.load(svd_head)\n",
    "    \n",
    "# svd of body\n",
    "with open('train.body.svd.pkl', 'rb') as svd_body:\n",
    "    body_svd = pickle.load(svd_body)\n",
    "\n",
    "# svd of tfidf cosine similarity\n",
    "with open('train.sim.svd.pkl', 'rb') as svd_sim:\n",
    "    sim_svd = pickle.load(svd_sim)\n",
    "    \n",
    "# w2v headline\n",
    "with open('train.headline.word2vec.pkl', 'rb') as w2v_head:\n",
    "    headline_w2v = pickle.load(w2v_head)\n",
    "    \n",
    "# w2v body\n",
    "with open('train.body.word2vec.pkl', 'rb') as w2v_body:\n",
    "    body_w2v = pickle.load(w2v_body)\n",
    "    \n",
    "# w2v sim\n",
    "with open('train.sim.word2vec.pkl', 'rb') as w2v_body:\n",
    "    sim_w2v = pickle.load(w2v_body)\n",
    "    \n",
    "# headlinen sentiment scores\n",
    "with open('train.headline.senti.pkl', 'rb') as senti_head:\n",
    "    headline_senti = pickle.load(senti_head)\n",
    "    \n",
    "# body sentiment scores\n",
    "with open('train.body.senti.pkl', 'rb') as senti_body:\n",
    "    body_senti = pickle.load(senti_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [xBasicCountsTrain,\n",
    "          sim_tfidf,\n",
    "          headline_svd, body_svd, sim_svd,\n",
    "          headline_w2v, body_w2v, sim_w2v,\n",
    "          headline_senti, body_senti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for vec in vectors:\n",
    "    print(type(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(75385, 26)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 100)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 300)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 300)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 4)\n",
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "(75385, 4)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for vec in vectors:\n",
    "    print(vec.ndim)\n",
    "    print(vec.shape)\n",
    "    print(type(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNC_data = np.hstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75385, 837)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FNC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"engineered_FNC_data.pkl\", \"wb\") as all_data:\n",
    "    pickle.dump(FNC_data, all_data, protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Future Work\n",
    "\n",
    "- use lematization instead of stemming; is there a meaningful impact on performance?\n",
    "- deep learning with convolutional layer\n",
    "- PMI - pointwise Mutual Information score on ngrams as another feature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
