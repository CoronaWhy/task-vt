{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Overview\n",
    "This script creates a toy dataset from INDRA covid19, hosted on emma.indra.bio \n",
    "\n",
    "Emma puts together this graph on daily basis via a cron job that pulls in literature, does NER,  train new ML model..\n",
    "It incorporates daily updates from CORD-19 and also searches the Internet, and runs about 6 text mining systems on those\n",
    "\n",
    "The script converts the graph to BEL format via pybel library. \n",
    "The pybel library can be used to further process the graph and generate toy dataset outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lani_lichtenstein/indra/', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/lani_lichtenstein/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/home/lani_lichtenstein/.local/lib/python3.6/site-packages/IPython/extensions', '/home/lani_lichtenstein/.ipython']\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,\"/home/lani_lichtenstein/indra/\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pykeen\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline\n",
    "import pybel\n",
    "import pybel_tools\n",
    "import indra\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Jul 17 2020, 12:50:27) \n",
      "[GCC 8.4.0]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 19 09:40:49 2020\n"
     ]
    }
   ],
   "source": [
    "print(time.asctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lani_lichtenstein\n"
     ]
    }
   ],
   "source": [
    "print(getpass.getuser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/lani_lichtenstein/indra/indra']\n"
     ]
    }
   ],
   "source": [
    "print(indra.__path__) # check using local installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.10-UNHASHED\n"
     ]
    }
   ],
   "source": [
    "print(pybel.get_version(with_git_hash=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#from indra.statements import stmts_from_json\n",
    "#from indra.tools import assemble_corpus as ac\n",
    "#from indra.assemblers.pybel import PybelAssembler\n",
    "#model_url = 'https://emmaa.s3.amazonaws.com/assembled/covid19/latest_statements_covid19.json'\n",
    "#stmts_json = requests.get(model_url).json()\n",
    "#stmts = stmts_from_json(stmts_json)\n",
    "#filtered_stmts = ac.filter_belief(stmts, 0.9)\n",
    "#pa = PybelAssembler(filtered_stmts)\n",
    "#pybel_graph = pa.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybel.io.emmaa import get_statements_from_emmaa\n",
    "from indra.tools import assemble_corpus as ac\n",
    "from indra.assemblers.pybel import PybelAssembler\n",
    "\n",
    "stmts = get_statements_from_emmaa('covid19')\n",
    "filtered_stmts = ac.filter_belief(stmts, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_stmts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indra.assemblers.html.assembler import _format_evidence_text\n",
    "from tqdm import tqdm\n",
    "from indra.statements import Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore filtered statements \n",
    "filtered_stmts_with_nlp_evidence=[]\n",
    "for st in tqdm(filtered_stmts):\n",
    "    \n",
    "    #print(type(st.evidence[0].text)) # each statement has a list of objects of type Evidence https://indra.readthedocs.io/en/latest/_modules/indra/statements/evidence.html\n",
    "\n",
    "            \n",
    "    statement_evidence =_format_evidence_text(st)\n",
    "    for i, statement_evidence_tmp in enumerate(statement_evidence):\n",
    "            #st.evidence[i][\"text_annotated\"]=statement_evidence_tmp\n",
    "        evjson=st.evidence[i].to_json()                \n",
    "        evjson['annotations']['text_nlp']=statement_evidence_tmp # create new dict element in annotations    \n",
    "        evobj=Evidence._from_json(evjson)\n",
    "        st.evidence[i] = evobj\n",
    "\n",
    "    filtered_stmts_with_nlp_evidence.append(st)        \n",
    "    # explore evidence\n",
    "    # evjson=st.evidence[0].to_json()\n",
    "    # evannotations=evjson['annotations']\n",
    "    # for key in evannotations.keys():\n",
    "     #   print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=filtered_stmts_with_nlp_evidence[0]\n",
    "x=st.evidence[0].to_json()\n",
    "for key in x.keys():\n",
    "    print(key)\n",
    "print(\"\\n\")\n",
    "y=x['annotations']\n",
    "for key in y.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in filtered_stmts_with_nlp_evidence:\n",
    "    x=st.evidence[0].to_json()['annotations']\n",
    "    for key in x.keys():\n",
    "        print(key)\n",
    "    #print(x[\"text_nlp\"])\n",
    "    \n",
    "        print(\"new \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = PybelAssembler(filtered_stmts_with_nlp_evidence)\n",
    "pybel_graph = pa.make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Indra graph to Pybel\n",
    "#https://emmaa.indra.bio/dashboard/covid19?tab=model\n",
    "\n",
    "#pybel_covid_graph=pybel.from_emmaa('covid19', date=\"2020-04-23-17-44-57\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybel_graph.summarize() # summarise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pybel_graph, open( \"pybel_graph.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach B - Generate Toy Dataset with Raw Text and Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use local repo cloned from github to access to_triple function\n",
    "# this is not yet in pypi version, so need to access local cloned location\n",
    "#sys.path.insert(0,\"/home/username/pybel/src/\") # If you are using a local version of the file\n",
    "\n",
    "#from pybel.io.triples import api\n",
    "# not working - IGNORE\n",
    "#import imp\n",
    "#imp.find_module(\"pybel\")\n",
    "#triples_api = imp.load_source('api', \"/home/lani_lichtenstein/pybel/src/pybel/io/triples/api.py\")\n",
    "#import importlib\n",
    "#importlib.reload(pybel)\n",
    "pybel.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pybel_graph=pickle.load(open( \"pybel_graph.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pybel.dsl import BaseConcept\n",
    "from tqdm import tqdm\n",
    "uniq_key_list_annotations=[]\n",
    "\n",
    "for u,v,data in tqdm(pybel_graph.edges(data=True)):\n",
    "            \n",
    "    if 'annotations' in data.keys():\n",
    "        #print(\"Explore relation \\n\")\n",
    "        #print(data['relation'])\n",
    "        #print(\"\\n\")\n",
    "        annotations=data['annotations']\n",
    "        #print(annotations)\n",
    "        \n",
    "        for key in annotations.keys():\n",
    "            if key not in uniq_key_list_annotations:\n",
    "                uniq_key_list_annotations.append(key)\n",
    "                \n",
    "        #for key,val in annotations.items():\n",
    "        #    print(key)\n",
    "        #    print(type(annotations[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_key_list_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pybel.dsl import BaseConcept\n",
    "from tqdm import tqdm\n",
    "#from pybel.io.triples import api\n",
    "\n",
    "uniq_key_list_annotations=[]\n",
    "column_list=[\"Source\", \"Target\", \"Relation\", \"Evidence\", \"Citation\", \"Text_NLP\"]\n",
    "indra_df=pd.DataFrame(columns=column_list)\n",
    "\n",
    "for u,v,data in tqdm(pybel_graph.edges(data=True)):\n",
    "\n",
    "    source='NaN'\n",
    "    target='NaN'\n",
    "    evidence='NaN'\n",
    "    relation='NaN'\n",
    "    annotations='NaN'\n",
    "    text_nlp='NaN'\n",
    "    \n",
    "    #h,r,t=to_triple(u,v,data) https://github.com/pybel/pybel/blob/master/src/pybel/io/triples/api.py\n",
    "    \n",
    "    if isinstance(u, BaseConcept):\n",
    "        source=u.name\n",
    "        #source_obo=u.obo\n",
    "        #print(entity)\n",
    "        #print(u.name)\n",
    "        #print(u.obo)\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    if isinstance(v, BaseConcept):\n",
    "        target=v.name\n",
    "        \n",
    "    if 'evidence' in data.keys():  # look also at pybel.has_edge_evidence() \n",
    "        #print(\"Explore evidence \\n\")\n",
    "        #print(data['evidence'])\n",
    "        evidence=data[\"evidence\"]\n",
    "        #print(type(evidence))\n",
    "    \n",
    "    if 'relation' in data.keys():\n",
    "        #print(\"Explore relation \\n\")\n",
    "        #print(data['relation'])\n",
    "        #print(\"\\n\")\n",
    "        relation=data['relation']\n",
    "        \n",
    "    if 'annotations' in data.keys():\n",
    "        #print(\"Explore relation \\n\")\n",
    "        #print(data['relation'])\n",
    "        #print(\"\\n\")\n",
    "        annotations=data['annotations']\n",
    "        #print(annotations)\n",
    "        \n",
    "        for key in annotations.keys():\n",
    "            if key not in uniq_key_list_annotations:\n",
    "                uniq_key_list_annotations.append(key)\n",
    "        \n",
    "        if 'evidence_annotations' in annotations.keys():\n",
    "            evidence_annotations=annotations['evidence_annotations']\n",
    "            if 'text_nlp' in evidence_annotations.keys():\n",
    "                text_nlp=evidence_annotations['text_nlp']\n",
    "            \n",
    "    if 'citation' in data.keys():\n",
    "        #print(\"Explore relation \\n\")\n",
    "        #print(data['relation'])\n",
    "        #print(\"\\n\")\n",
    "        citation=data['citation']\n",
    "        \n",
    "    tmp=pd.Series([source, target, relation, evidence, citation, text_nlp], index=column_list)\n",
    "    indra_df=indra_df.append(tmp, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(indra_df, open( \"indra_df.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "indra_df=pickle.load(open( \"indra_df.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Citation</th>\n",
       "      <th>Text_NLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '30340996'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'Other', 'db_id': 'reach:Unknown'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '30340996'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS</td>\n",
       "      <td>MUC3B</td>\n",
       "      <td>increases</td>\n",
       "      <td>MS analysis of peptides recovered from an in-g...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '23118947'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '32214933', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS</td>\n",
       "      <td>MUC3B</td>\n",
       "      <td>increases</td>\n",
       "      <td>MALDI MS allows the identification of higher m...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '32214933'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '32214933', 't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source  Target           Relation  \\\n",
       "0     MS  Mucins  directlyIncreases   \n",
       "1     MS  Mucins  directlyIncreases   \n",
       "2     MS  Mucins  directlyIncreases   \n",
       "3     MS   MUC3B          increases   \n",
       "4     MS   MUC3B          increases   \n",
       "\n",
       "                                            Evidence  \\\n",
       "0  In the current study, we investigated the NanS...   \n",
       "1  In the current study, we investigated the NanS...   \n",
       "2  In the current study, we investigated the NanS...   \n",
       "3  MS analysis of peptides recovered from an in-g...   \n",
       "4  MALDI MS allows the identification of higher m...   \n",
       "\n",
       "                                    Citation  \\\n",
       "0      {'db': 'PubMed', 'db_id': '30340996'}   \n",
       "1  {'db': 'Other', 'db_id': 'reach:Unknown'}   \n",
       "2      {'db': 'PubMed', 'db_id': '30340996'}   \n",
       "3      {'db': 'PubMed', 'db_id': '23118947'}   \n",
       "4      {'db': 'PubMed', 'db_id': '32214933'}   \n",
       "\n",
       "                                            Text_NLP  \n",
       "0  {'source_api': 'reach', 'pmid': '30340996', 't...  \n",
       "1  {'source_api': 'reach', 'pmid': '30340996', 't...  \n",
       "2  {'source_api': 'reach', 'pmid': '30340996', 't...  \n",
       "3  {'source_api': 'reach', 'pmid': '32214933', 't...  \n",
       "4  {'source_api': 'reach', 'pmid': '32214933', 't...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore\n",
    "indra_df.shape\n",
    "indra_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_md-0.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "import en_core_sci_md\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correct code\n",
    "# nlp=spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "# problem with not recognising path\n",
    "nlp=spacy.load(\"/home/lani_lichtenstein/.local/lib/python3.6/site-packages/en_core_sci_md/en_core_sci_md-0.2.5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133401, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indra_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_position(txt_nlp, position_flag):\n",
    "    ''' Function get cell position for source, target start and stop'''\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "    nlp_doc = txt_nlp\n",
    "    new_doc = txt_nlp.text\n",
    "    new_doc_nlp=[]\n",
    "\n",
    "    names = []\n",
    "    \n",
    "    if position_flag== 0: # source start\n",
    "        pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"span\"}, {\"TEXT\": \"class=\\\"badge\"}, {\"TEXT\": \"badge-subject\\\"\"},{\"TEXT\": {\"REGEX\": \">\"}}]\n",
    "    elif position_flag == 1:\n",
    "        pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"/span\"}, {\"TEXT\": \">\"}]\n",
    "    elif position_flag == 2:\n",
    "        pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"span\"}, {\"TEXT\": \"class=\\\"badge\"}, {\"TEXT\": \"badge-object\\\"\"},{\"TEXT\": {\"REGEX\": \">\"}}]\n",
    "    elif position_flag == 3:\n",
    "        pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"/span\"}, {\"TEXT\": \">\"}]\n",
    "\n",
    "    matcher.add('source_start', None, pattern) \n",
    "    matches = matcher(nlp_doc) \n",
    "\n",
    "    for match_id, start, end in matches[0:1]: # just use first match \n",
    "        span = nlp_doc[start:end] \n",
    "        names.append(span.text) \n",
    "\n",
    "    for name in names: \n",
    "        new_doc = new_doc.replace(name,'',1) # replace first instance only\n",
    "        new_doc = new_doc.replace('  ',' ') # replace double whitespace with one whitespace\n",
    "        new_doc_nlp=nlp(new_doc)\n",
    "    \n",
    "    return(matches,new_doc_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "indra_df_new=indra_df.copy()\n",
    "\n",
    "indra_df_new['source_start']=np.NaN\n",
    "indra_df_new['source_end']=np.NaN\n",
    "indra_df_new['target_start']=np.NaN\n",
    "indra_df_new['target_end']=np.NaN\n",
    "indra_df_new['annotation_text']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source', 'Target', 'Relation', 'Evidence', 'Citation', 'Text_NLP',\n",
       "       'source_start', 'source_end', 'target_start', 'target_end',\n",
       "       'annotation_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indra_df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/133401 [00:00<4:32:43,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[(10708931376594966169, 23, 26), (10708931376594966169, 50, 53)]\n",
      "22\n",
      "[(10708931376594966169, 23, 26), (10708931376594966169, 50, 53)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/133401 [00:00<4:31:43,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[(10708931376594966169, 23, 26), (10708931376594966169, 50, 53)]\n",
      "[(10708931376594966169, 10, 13)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/133401 [00:00<4:34:21,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 10, 13)]\n",
      "[(10708931376594966169, 10, 13)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/133401 [00:00<4:35:36,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 8, 11)]\n",
      "[(10708931376594966169, 8, 11)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/133401 [00:01<4:38:02,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 8, 11)]\n",
      "[(10708931376594966169, 8, 11)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/133401 [00:01<4:28:09,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 8, 11)]\n",
      "[(10708931376594966169, 27, 30)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/133401 [00:01<4:13:02,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 27, 30)]\n",
      "[(10708931376594966169, 27, 30)]\n",
      "[(10708931376594966169, 5, 8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 16/133401 [00:01<3:53:12,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 5, 8)]\n",
      "[(10708931376594966169, 5, 8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 19/133401 [00:02<3:31:21, 10.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 25, 28)]\n",
      "[(10708931376594966169, 25, 28)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/133401 [00:02<4:18:56,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 25, 28)]\n",
      "[(10708931376594966169, 25, 28)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/133401 [00:02<4:30:55,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 7, 10), (10708931376594966169, 24, 27), (10708931376594966169, 36, 39)]\n",
      "[(10708931376594966169, 7, 10), (10708931376594966169, 24, 27), (10708931376594966169, 36, 39)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/133401 [00:02<4:21:51,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10708931376594966169, 7, 10), (10708931376594966169, 24, 27), (10708931376594966169, 36, 39)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-94ebbceff84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_doc_nlp\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_doc_nlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_text_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_nlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_doc_nlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mposition_flag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0msource_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b5495e84659f>\u001b[0m in \u001b[0;36mget_text_position\u001b[0;34m(txt_nlp, position_flag)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnew_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# replace first instance only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnew_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# replace double whitespace with one whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mnew_doc_nlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_doc_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in tqdm(range(indra_df.shape[0])): \n",
    "    #print(i)\n",
    "\n",
    "    if indra_df.Text_NLP[i]==\"NaN\" or 'text' not in indra_df.Text_NLP[i].keys():\n",
    "        continue\n",
    "        \n",
    "    txt=indra_df.Text_NLP[i]['text']\n",
    "            \n",
    "    source_start=np.NaN\n",
    "    source_end=np.NaN\n",
    "    target_start=np.NaN\n",
    "    target_end=np.NaN\n",
    "\n",
    "    if txt == None:\n",
    "        continue    \n",
    "    txt=txt.replace(\"</span\", \" </span\") # so tokenizer can split the makrker of the source/target span into separate token\n",
    "    txt_nlp=nlp(txt)\n",
    "    evidence_nlp=nlp(indra_df.Evidence[i])\n",
    "\n",
    "    \n",
    "    # Two cases - source appears first, target appears first. \n",
    "    matches,new_doc_nlp=get_text_position(txt_nlp=txt_nlp,position_flag=0) # source start\n",
    "    if len(matches) > 0:\n",
    "        source_start=matches[0][1]\n",
    "    matches,new_doc_nlp=get_text_position(txt_nlp=txt_nlp,position_flag=2) # source start\n",
    "    if len(matches) > 0:\n",
    "        target_start=matches[0][1]\n",
    "\n",
    "    if source_start < target_start:\n",
    "        option_start=\"A\"\n",
    "    else:\n",
    "        option_start=\"B\"\n",
    "    \n",
    "    if option_start == \"A\":\n",
    "        \n",
    "        # get source start\n",
    "        if txt_nlp==[]: # empty list\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=txt_nlp,position_flag=0)\n",
    "        if len(matches) > 0:\n",
    "            source_start=matches[0][1]\n",
    "\n",
    "        # get source end \n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=1)\n",
    "        if len(matches) > 0:\n",
    "            source_end=matches[0][1] \n",
    "\n",
    "        # get target start\n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=2) # get target start\n",
    "        if len(matches) > 0:\n",
    "            target_start=matches[0][1]\n",
    "\n",
    "        # get target end\n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=3)\n",
    "        if len(matches) > 0:\n",
    "            print(matches)\n",
    "            target_end=matches[0][1]\n",
    "\n",
    "    if option_start == \"B\": # target appears before source in text\n",
    "\n",
    "        # get target start\n",
    "        if txt_nlp==[]: # empty list\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=txt_nlp,position_flag=2) # pick up target pattern\n",
    "        if len(matches) > 0:\n",
    "            target_start=matches[0][1]\n",
    "\n",
    "        # get target end \n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=3) # pick up '</span>' to end target\n",
    "        if len(matches) > 0:\n",
    "            print(target_start)\n",
    "            print(matches)\n",
    "            target_end=matches[0][1]\n",
    "\n",
    "        # get source start\n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=0) # get source start\n",
    "        if len(matches) > 0:\n",
    "            source_start=matches[0][1]\n",
    "\n",
    "        # get source end\n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp,position_flag=1)  # pick up '</span>' to end source\n",
    "        if len(matches) > 0:\n",
    "            source_end=matches[0][1]\n",
    "\n",
    "    indra_df_new.loc[i,\"annotation_text\"] = new_doc_nlp.text\n",
    "    indra_df_new.loc[i,\"source_start\"] = source_start\n",
    "    indra_df_new.loc[i,\"source_end\"] = source_end\n",
    "    indra_df_new.loc[i,\"target_start\"] = target_start\n",
    "    indra_df_new.loc[i,\"target_end\"] = target_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source', 'Target', 'Relation', 'Evidence', 'Citation', 'Text_NLP',\n",
       "       'source_start', 'source_end', 'target_start', 'target_end',\n",
       "       'annotation_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indra_df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Citation</th>\n",
       "      <th>Text_NLP</th>\n",
       "      <th>source_start</th>\n",
       "      <th>source_end</th>\n",
       "      <th>target_start</th>\n",
       "      <th>target_end</th>\n",
       "      <th>annotation_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '30340996'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'Other', 'db_id': 'reach:Unknown'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS</td>\n",
       "      <td>Mucins</td>\n",
       "      <td>directlyIncreases</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '30340996'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '30340996', 't...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>In the current study, we investigated the NanS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS</td>\n",
       "      <td>MUC3B</td>\n",
       "      <td>increases</td>\n",
       "      <td>MS analysis of peptides recovered from an in-g...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '23118947'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '32214933', 't...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>MALDI MS allows the identification of higher m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS</td>\n",
       "      <td>MUC3B</td>\n",
       "      <td>increases</td>\n",
       "      <td>MALDI MS allows the identification of higher m...</td>\n",
       "      <td>{'db': 'PubMed', 'db_id': '32214933'}</td>\n",
       "      <td>{'source_api': 'reach', 'pmid': '32214933', 't...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>MALDI MS allows the identification of higher m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source  Target           Relation  \\\n",
       "0     MS  Mucins  directlyIncreases   \n",
       "1     MS  Mucins  directlyIncreases   \n",
       "2     MS  Mucins  directlyIncreases   \n",
       "3     MS   MUC3B          increases   \n",
       "4     MS   MUC3B          increases   \n",
       "\n",
       "                                            Evidence  \\\n",
       "0  In the current study, we investigated the NanS...   \n",
       "1  In the current study, we investigated the NanS...   \n",
       "2  In the current study, we investigated the NanS...   \n",
       "3  MS analysis of peptides recovered from an in-g...   \n",
       "4  MALDI MS allows the identification of higher m...   \n",
       "\n",
       "                                    Citation  \\\n",
       "0      {'db': 'PubMed', 'db_id': '30340996'}   \n",
       "1  {'db': 'Other', 'db_id': 'reach:Unknown'}   \n",
       "2      {'db': 'PubMed', 'db_id': '30340996'}   \n",
       "3      {'db': 'PubMed', 'db_id': '23118947'}   \n",
       "4      {'db': 'PubMed', 'db_id': '32214933'}   \n",
       "\n",
       "                                            Text_NLP  source_start  \\\n",
       "0  {'source_api': 'reach', 'pmid': '30340996', 't...          41.0   \n",
       "1  {'source_api': 'reach', 'pmid': '30340996', 't...          41.0   \n",
       "2  {'source_api': 'reach', 'pmid': '30340996', 't...          41.0   \n",
       "3  {'source_api': 'reach', 'pmid': '32214933', 't...           1.0   \n",
       "4  {'source_api': 'reach', 'pmid': '32214933', 't...           1.0   \n",
       "\n",
       "   source_end  target_start  target_end  \\\n",
       "0        42.0          22.0        23.0   \n",
       "1        42.0          22.0        23.0   \n",
       "2        42.0          22.0        23.0   \n",
       "3         2.0           9.0        10.0   \n",
       "4         2.0           9.0        10.0   \n",
       "\n",
       "                                     annotation_text  \n",
       "0  In the current study, we investigated the NanS...  \n",
       "1  In the current study, we investigated the NanS...  \n",
       "2  In the current study, we investigated the NanS...  \n",
       "3  MALDI MS allows the identification of higher m...  \n",
       "4  MALDI MS allows the identification of higher m...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indra_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mucin"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check example - at row 22 \n",
    "#nlp.tokenizer.explain(indra_df_new.annotation_text[3])\n",
    "\n",
    "#nlp(indra_df_new.Evidence[22])\n",
    "start=indra_df_new.source_start[22]\n",
    "end=indra_df_new.source_end[22]\n",
    "nlp(indra_df_new['annotation_text'][22])[start:end]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "biofilm formation"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#nlp(indra_df_new.Evidence[22])\n",
    "start=indra_df_new.target_start[22]\n",
    "end=indra_df_new.target_end[22]\n",
    "nlp(indra_df_new['annotation_text'][22])[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our finding that <span class=\"badge badge-subject\">mucin</span> decreases <span class=\"badge badge-object\">biofilm formation</span> is consistent with studies, which showed that <span class=\"badge badge-subject\">mucin</span> significantly decreased <span class=\"badge badge-object\">biofilm formation</span> by Streptococcus mutans and P. aeruginosa (Haley et al., 2014, Frenkel and Ribbeck, 2015).'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indra_df_new['Text_NLP'][22]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "indra_df.to_csv(\"indra_covid_toy_dataset_raw_evidence_high_belief.csv\",index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach A - Generate Triples\n",
    "\n",
    "One approach to generating a toy dataset is to generate triples. \n",
    "Triples can be used to generate knowledge graph embeddings. \n",
    "They also contain grounded source and target identifiers, as well as details relation descriptions. \n",
    "\n",
    "This is not obtained using Approach B - Generate Raw Data with Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pybel_graph=pickle.load(open( \"pybel_graph.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybel.io.tsv.api\n",
    "\n",
    "triples=pybel.io.tsv.api.get_triples(pybel_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "triples = np.array(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df=pd.DataFrame(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_df.to_csv(\"indra_covid_toy_dataset_triples.csv\",index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches # position is matches[1] (token start)\n",
    "source_start=matches[0][1]\n",
    "source_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc_nlp[5] # position of matches\n",
    "#nlp.tokenizer.explain(new_doc_nlp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now going to find END of source\n",
    "def get_source_end(txt_nlp):\n",
    "    \n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "    nlp_doc = new_doc_nlp \n",
    "    new_doc = new_doc_nlp.text\n",
    "\n",
    "    names = []\n",
    "\n",
    "    #pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}] \n",
    "    pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"/span\"}, {\"TEXT\": \">\"}]\n",
    "    #pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}]\n",
    "    #{\"TEXT\": {\"REGEX\": \"<\"}\n",
    "\n",
    "    matcher.add('end_source', None, pattern) \n",
    "    matches = matcher(nlp_doc) \n",
    "\n",
    "    for match_id, start, end in matches[0:1]: # just replacing first \"</span>\". Not replacing the end of the target for now. \n",
    "        span = nlp_doc[start:end] \n",
    "        names.append(span.text) \n",
    "\n",
    "    for name in names: \n",
    "        new_doc = new_doc.replace(name,'',1) # only replace first occurence\n",
    "        new_doc = new_doc.replace('  ',' ') # replace double whitespace with one whitespace\n",
    "        new_doc_nlp=nlp(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_end=matches[0][1] # assume only one element in matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidence_nlp[source_start:source_end] # good pi\n",
    "#nlp.tokenizer.explain(new_doc_nlp.text)  # there is a \"blank token in here that doesnt get shown\"\n",
    "\n",
    "new_doc_nlp[3]\n",
    "#evidence_nlp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now for target \n",
    "#nlp.tokenizer.explain(new_doc_nlp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "nlp_doc = new_doc_nlp\n",
    "new_doc = new_doc_nlp.text\n",
    "\n",
    "names = []\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"span\"}, {\"TEXT\": \"class=\\\"badge\"}, {\"TEXT\": \"badge-object\\\"\"},{\"TEXT\": {\"REGEX\": \">\"}}]\n",
    "\n",
    "matcher.add('target_start', None, pattern) \n",
    "matches = matcher(nlp_doc) \n",
    "\n",
    "for match_id, start, end in matches: \n",
    "    span = nlp_doc[start:end] \n",
    "    names.append(span.text) \n",
    "\n",
    "for name in names: \n",
    "    new_doc = new_doc.replace(name,'')\n",
    "    new_doc_nlp=nlp(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_start=matches[0][1]  # subtract 1, because first match of \"<\" is treated as prefix, not a token\n",
    "target_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now going to find END of target\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "nlp_doc = new_doc_nlp \n",
    "new_doc = new_doc_nlp.text\n",
    "\n",
    "names = []\n",
    "\n",
    "#pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}] \n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}, {\"TEXT\": \"/span\"}, {\"TEXT\": \">\"}]\n",
    "#pattern = [{\"TEXT\": {\"REGEX\": \"<\"}}]\n",
    "#{\"TEXT\": {\"REGEX\": \"<\"}\n",
    "\n",
    "matcher.add('end_source', None, pattern) \n",
    "matches = matcher(nlp_doc) \n",
    "\n",
    "for match_id, start, end in matches[0:1]: # just replacing first \"</span>\". Not replacing the end of the target for now. \n",
    "    span = nlp_doc[start:end] \n",
    "    names.append(span.text) \n",
    "\n",
    "for name in names: \n",
    "    new_doc = new_doc.replace(name,'')\n",
    "    new_doc = new_doc.replace('  ',' ')\n",
    "    new_doc_nlp=nlp(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_end=matches[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_nlp[target_start:target_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "#pattern = [{\"LOWER\": \"\\<span class=\\\"badge badge-subject\\\"\\>\"}]\n",
    "pattern = [{\"LOWER\": \"\\<\"}, {\"LOWER\": \"span\"}, {\"LOWER\": \"\\\"badge\"}]\n",
    "pattern = [{\"LOWER\": \"\\<\"}]\n",
    "\n",
    "matcher.add(\"HelloWorld\", None, pattern)\n",
    "doc = nlp(\"hello world!\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", None, pattern)\n",
    "doc = nlp(\"hello world!\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OLD CODE\n",
    "# get source end \n",
    "        if new_doc_nlp==[]:\n",
    "            continue\n",
    "            \n",
    "        #  Sometimes object (target) appears before subject (source\n",
    "        # To ensure we match the right '</span>' marking end of the source (not the target)\n",
    "        # just pass in text from source start until the end of the text. \n",
    "        # If src before trgt, then first '</span>' is replaced withut a problem\n",
    "        # If trgt before src, then we cut out this first part of the sentnce, so wont impact the target end '</span>' marker\n",
    "        new_doc_nlp_end=new_doc_nlp[source_start:len(new_doc_nlp.text)]\n",
    "        matches,new_doc_nlp_end=get_text_position(txt_nlp=new_doc_nlp_end,position_flag=1)\n",
    "\n",
    "        # now, need to piece together original doc, + returned doc..\n",
    "        if new_doc_nlp_end==[]:\n",
    "            continue\n",
    "\n",
    "        new_doc_nlp1=nlp(new_doc_nlp[0:source_start].text + new_doc_nlp_end.text)\n",
    "\n",
    "        if len(matches) > 0:\n",
    "            source_end=source_start+matches[0][1] # need to offset the matches position by location of source start\n",
    "\n",
    "        # get target start\n",
    "        #print(\"before target start\")\n",
    "        #print(new_doc_nlp)\n",
    "        if new_doc_nlp1==[]:\n",
    "            continue\n",
    "        matches,new_doc_nlp=get_text_position(txt_nlp=new_doc_nlp1,position_flag=2) # get target start\n",
    "        if len(matches) > 0:\n",
    "            target_start=matches[0][1]\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add triples - read in api.py module and use to_triple\n",
    "#https://github.com/pybel/pybel/blob/master/src/pybel/io/triples/api.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
